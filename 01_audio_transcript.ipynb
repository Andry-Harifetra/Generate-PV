{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13136823,"sourceType":"datasetVersion","datasetId":8302666}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transcription CA\n\nCe notebook applique le **pipeline complet** :\n- **Prétraitement** audio (FFmpeg + noisereduce)\n- **Transcription** faster-whisper (réglages anti-hallucinations)\n- **Chunks longs** pour une meilleure cohérence (3–5 min)\n- **Diarisation** (pyannote → fallback whisperx)\n- **Post-traitement** (dédup + normalisation chiffres/unités)\n- **Nettoyage LLM** par morceaux (1000 caractères) avec borne de correction\n- **Sauvegarde JSON** des sorties (raw, diarized, cleaned, llm_cleaned)","metadata":{}},{"cell_type":"markdown","source":"# **Installation des packages nécessaires**","metadata":{}},{"cell_type":"code","source":"# %%capture\n# # Installation silencieuse des dépendances avec gestion des conflits\n\n# # 1. Mise à jour pip pour éviter les problèmes\n# #!pip install --upgrade pip -q\n\n# # 2. Installation FFmpeg (système)\n# !apt-get update -qq\n# !apt-get install -qq ffmpeg sox\n\n# # 3. Nettoyage et verrouillage de la stack NumPy/Numba/Scipy\n# #!pip uninstall -y numpy numba >/dev/null 2>&1 || true\n# !pip install -q numpy==1.26.4 scipy==1.11.4\n# !pip install -q numba==0.58.1\n# !pip install -q torch==2.1.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n\n# # 4. Installation des packages de transcription\n# #!pip install -q openai-whisper==20231117\n# !pip install -q faster-whisper==1.0.3\n\n# # 5. Packages de débruitage audio\n# !pip install -q librosa==0.10.1\n# !pip install -q soundfile==0.12.1\n# !pip install -q noisereduce==3.0.0\n# !pip install -q pydub==0.25.1\n\n# # 6. Diarization\n# !pip install -q \"pyannote.audio>=3.1\"\n# !pip install -q whisperx\n\n# !pip install -q regex==2023.12.25 unidecode==1.3.8\n\n# # 7. Packages documents\n# !pip install -q python-docx==1.2.0\n# !pip install -q python-pptx==1.0.2\n\n# # 8. Packages LLM et NLP\n# !pip install -q openai==1.91.0\n# !pip install -q assemblyai==0.44.3\n# !pip install -q tiktoken==0.9.0\n\n# # 9. LangChain\n# #!pip install -q langchain==0.3.27 langchain-community==0.3.29 langchain-core==0.3.30\n\n# # 10. Packages utilitaires\n# !pip install -q pandas==2.1.4 matplotlib==3.8.2 seaborn==0.13.2\n\n# # 11. Installation FAISS pour le RAG\n# #!pip install -q faiss-cpu==1.7.4\n\n# print(\"✅ Installation terminée!\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n# Installation minimale des dépendances nécessaires sans perturber l'environnement Kaggle\nimport importlib\nimport os\nimport shutil\nimport subprocess\nimport sys\n\ndef ensure_packages(requirements):\n    missing = []\n    for module_name, package_spec in requirements:\n        try:\n            importlib.import_module(module_name)\n        except Exception:\n            missing.append(package_spec)\n    if missing:\n        cmd = [sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '-q'] + missing\n        subprocess.check_call(cmd)\n\ncore_requirements = [\n    ('faster_whisper', 'faster-whisper==1.0.3'),\n    ('librosa', 'librosa==0.10.1'),\n    ('soundfile', 'soundfile==0.12.1'),\n    ('noisereduce', 'noisereduce==3.0.0'),\n    ('pydub', 'pydub==0.25.1'),\n    ('docx', 'python-docx==1.2.0'),\n    ('pptx', 'python-pptx==1.0.2'),\n    ('openai', 'openai==1.91.0'),\n    ('assemblyai', 'assemblyai==0.44.3'),\n    ('tiktoken', 'tiktoken==0.9.0'),\n]\n\nensure_packages(core_requirements)\n\nif os.environ.get('INSTALL_LANGCHAIN', '0') == '1':\n    optional_requirements = [\n        ('langchain', 'langchain==0.3.27'),\n        ('langchain_community', 'langchain-community==0.3.29'),\n        ('faiss', 'faiss-cpu==1.7.4'),\n    ]\n    try:\n        ensure_packages(optional_requirements)\n    except subprocess.CalledProcessError:\n        pass\n\nif not shutil.which('ffmpeg'):\n    subprocess.check_call(['apt-get', 'update', '-qq'])\n    subprocess.check_call(['apt-get', 'install', '-qq', 'ffmpeg'])\n\nprint('✅ Vérification des dépendances terminée')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:52:46.653453Z","iopub.execute_input":"2025-09-25T11:52:46.653989Z","iopub.status.idle":"2025-09-25T11:52:57.847103Z","shell.execute_reply.started":"2025-09-25T11:52:46.653932Z","shell.execute_reply":"2025-09-25T11:52:57.846451Z"}},"outputs":[{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 37.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 253.0/253.0 kB 360.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 472.8/472.8 kB 143.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.2/50.2 kB 283.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 214.8 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.6/38.6 MB 154.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 36.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.3/175.3 kB 32.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 275.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 55.4 MB/s eta 0:00:00\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Vérification que tout est installé correctement\nimport importlib\n\npackages_to_check = [\n    ('numpy', 'numpy'),\n    ('scipy', 'scipy'),\n    ('numba', 'numba'),\n    ('whisper', 'openai-whisper'),\n    ('faster_whisper', 'faster-whisper'),\n    ('librosa', 'librosa'),\n    ('soundfile', 'soundfile'),\n    ('noisereduce', 'noisereduce'),\n    ('pydub', 'pydub'),\n    ('docx', 'python-docx'),\n    ('pptx', 'python-pptx'),\n    ('openai', 'openai'),\n    ('langchain', 'langchain'),\n    ('langchain_community', 'langchain-community'),\n    ('faiss', 'faiss-cpu'),\n    ('assemblyai', 'assemblyai'),\n    ('tiktoken', 'tiktoken')\n]\n\nprint(\"🔍 Vérification des packages installés:\")\nprint(\"-\" * 50)\n\nall_ok = True\nfor import_name, package_name in packages_to_check:\n    try:\n        module = importlib.import_module(import_name)\n        version = getattr(module, '__version__', 'N/A')\n        print(f\"✅ {package_name:20} : {version}\")\n    except ImportError:\n        print(f\"❌ {package_name:20} : Non installé\")\n        all_ok = False\n    except Exception as exc:\n        print(f\"⚠️ {package_name:20} : Erreur lors de l'import ({type(exc).__name__}: {exc})\")\n        all_ok = False\n\nif all_ok:\n    print(\"✨ Tous les packages sont installés correctement!\")\nelse:\n    print(\"⚠️ Certains packages nécessitent une attention. Consultez les messages ci-dessus.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:54:38.645614Z","iopub.execute_input":"2025-09-25T11:54:38.646140Z","iopub.status.idle":"2025-09-25T11:54:49.409266Z","shell.execute_reply.started":"2025-09-25T11:54:38.646114Z","shell.execute_reply":"2025-09-25T11:54:49.408666Z"}},"outputs":[{"name":"stdout","text":"🔍 Vérification des packages installés:\n--------------------------------------------------\n✅ numpy                : 1.26.4\n✅ scipy                : 1.15.3\n✅ numba                : 0.60.0\n❌ openai-whisper       : Non installé\n✅ faster-whisper       : 1.0.3\n✅ librosa              : 0.11.0\n✅ soundfile            : 0.13.1\n✅ noisereduce          : N/A\n✅ pydub                : N/A\n✅ python-docx          : 1.2.0\n✅ python-pptx          : 1.0.2\n✅ openai               : 1.91.0\n✅ langchain            : 0.3.26\n❌ langchain-community  : Non installé\n❌ faiss-cpu            : Non installé\n✅ assemblyai           : 0.44.3\n✅ tiktoken             : 0.9.0\n⚠️ Certains packages nécessitent une attention. Consultez les messages ci-dessus.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Imports et configuration GPU**","metadata":{}},{"cell_type":"code","source":"# Imports standards\nimport os, sys, json, math, re, shutil, subprocess\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datetime import datetime, timezone\nimport time\ntry:\n    from zoneinfo import ZoneInfo\nexcept Exception:\n    ZoneInfo = None\n\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass, field\nimport gc  # Garbage collector\n\nimport numpy as np\nimport pandas as pd\n\n# Imports audio et débruitage\nimport librosa\nimport soundfile as sf\nimport noisereduce as nr\nfrom scipy.signal import butter, filtfilt, medfilt\nfrom pydub import AudioSegment\n\n# Imports pour la transcription\n#import whisper\nfrom faster_whisper import WhisperModel\n\n# Imports pour les documents\nfrom docx import Document\nfrom pptx import Presentation\n\n# Imports pour le NLP et LLM\nimport openai\ntry:\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from langchain_community.vectorstores import FAISS\n    from langchain_community.embeddings import OpenAIEmbeddings\n    langchain_available = True\nexcept ImportError:\n    print(\"⚠️ LangChain non disponible\")\n    langchain_available = False\n\nimport torch\nprint(f\"🔧 PyTorch: {torch.__version__}\")\nprint(f\"🎮 CUDA disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Mémoire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:55:12.624163Z","iopub.execute_input":"2025-09-25T11:55:12.624984Z","iopub.status.idle":"2025-09-25T11:55:13.091049Z","shell.execute_reply.started":"2025-09-25T11:55:12.624944Z","shell.execute_reply":"2025-09-25T11:55:13.090271Z"}},"outputs":[{"name":"stdout","text":"⚠️ LangChain non disponible\n🔧 PyTorch: 2.6.0+cu124\n🎮 CUDA disponible: True\n   GPU: Tesla T4\n   Mémoire: 15.83 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Configuration des clés API**","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\nASSEMBLYAI_API_KEY = user_secrets.get_secret(\"ASSEMBLYAI_API_KEY\")\nHUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:55:13.092453Z","iopub.execute_input":"2025-09-25T11:55:13.092869Z","iopub.status.idle":"2025-09-25T11:55:13.315253Z","shell.execute_reply.started":"2025-09-25T11:55:13.092849Z","shell.execute_reply":"2025-09-25T11:55:13.314514Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# **Configuration des chemins**","metadata":{}},{"cell_type":"code","source":"UPLOAD_PATH = \"/kaggle/input/meeting-audio/\" # Chemin des fichiers uploadés \nOUTPUT_PATH = \"/kaggle/working\" # Chemin de sortie","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:55:13.316016Z","iopub.execute_input":"2025-09-25T11:55:13.316373Z","iopub.status.idle":"2025-09-25T11:55:13.319629Z","shell.execute_reply.started":"2025-09-25T11:55:13.316348Z","shell.execute_reply":"2025-09-25T11:55:13.319117Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"FULL_DIR = Path(OUTPUT_PATH) / \"temp_chunks\"\nFULL_DIR.mkdir(parents=True, exist_ok=True)\n\ndef cleanup_temp_files():\n    \"\"\"Nettoyer les fichiers temporaires\"\"\"\n    if FULL_DIR.exists():\n        shutil.rmtree(FULL_DIR)\n    FULL_DIR.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:55:13.321092Z","iopub.execute_input":"2025-09-25T11:55:13.321267Z","iopub.status.idle":"2025-09-25T11:55:13.331341Z","shell.execute_reply.started":"2025-09-25T11:55:13.321252Z","shell.execute_reply":"2025-09-25T11:55:13.330837Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **Utilitaires de commande système**","metadata":{}},{"cell_type":"code","source":"def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True) #Vérification création de dossier\ndef run(cmd): # Lancement commande\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate(); return p.returncode, out.decode(), err.decode()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:55:13.332052Z","iopub.execute_input":"2025-09-25T11:55:13.332268Z","iopub.status.idle":"2025-09-25T11:55:13.341016Z","shell.execute_reply.started":"2025-09-25T11:55:13.332253Z","shell.execute_reply":"2025-09-25T11:55:13.340420Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**Optimiser la mémoire**","metadata":{}},{"cell_type":"code","source":"def check_gpu_memory():\n    if torch.cuda.is_available():\n        free, total = torch.cuda.mem_get_info()\n        print(f\"📊 GPU: {free/1e9:.2f}GB libres / {total/1e9:.2f}GB total\")\n        if free < 4e9:  # Moins de 4GB libres\n            print(\"⚠️ Mémoire GPU faible, utilisation de 'base' recommandée\")\n            return \"base\"\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:55:13.341842Z","iopub.execute_input":"2025-09-25T11:55:13.342513Z","iopub.status.idle":"2025-09-25T11:55:13.351869Z","shell.execute_reply.started":"2025-09-25T11:55:13.342476Z","shell.execute_reply":"2025-09-25T11:55:13.351237Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Monitoring et debug**","metadata":{}},{"cell_type":"code","source":"def print_memory_usage():\n    \"\"\"Afficher l'utilisation mémoire GPU/RAM\"\"\"\n    if torch.cuda.is_available():\n        print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB / {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n    \n    import psutil\n    process = psutil.Process()\n    print(f\"RAM Usage: {process.memory_info().rss / 1e9:.2f}GB\")\n\n# Appeler après chaque étape majeure\nprint_memory_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:55:13.352540Z","iopub.execute_input":"2025-09-25T11:55:13.352736Z","iopub.status.idle":"2025-09-25T11:55:13.363700Z","shell.execute_reply.started":"2025-09-25T11:55:13.352719Z","shell.execute_reply":"2025-09-25T11:55:13.363004Z"}},"outputs":[{"name":"stdout","text":"GPU Memory: 0.00GB / 0.00GB\nRAM Usage: 0.85GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# **Configuration du pipeline**","metadata":{}},{"cell_type":"code","source":" @dataclass \nclass Config: \n    \"\"\"Configuration centralisée pour Kaggle\"\"\" \n    \n    timezone: str = \"Indian/Antananarivo\"\n    # Clés API \n    openai_key: str = OPENAI_API_KEY \n    assemblyai_key: str = ASSEMBLYAI_API_KEY\n    \n    # Whisper\n    whisper_model: str = \"large-v3\" # 'tiny', 'base', 'small', 'medium', 'large', \"large-v3\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    compute_type: str = \"float16\" if torch.cuda.is_available() else \"int8\"\n    # Audio\n    sample_rate: int = 16000\n    # Decoding / anti-hallucination\n    beam_size: int = 5\n    best_of: int = 2\n    patience: float = 1.0\n    temperature: float = 0.0\n    compression_ratio_threshold: float = 2.8\n    log_prob_threshold: float = -0.50\n    no_speech_threshold: float = 0.80\n    max_initial_timestamp: float = 1.0\n    suppress_blank: bool = True\n    suppress_tokens: list[int] = field(default_factory=lambda: [-1])\n    # VAD\n    use_vad: bool = True\n    vad_threshold: float = 0.45\n    vad_min_speech_duration_ms: int = 500\n    vad_max_speech_duration_s: float = 60.0\n    vad_min_silence_duration_ms: int = 1000\n    vad_speech_pad_ms: int = 400\n    # Chunks longs pour cohérence (3–5 min)\n    chunk_length_s: int = 300\n    chunk_overlap_s: int = 30\n    # Post-traitement\n    max_repetitions: int = 3\n    # Prompt spécialisé\n    initial_prompt: str = (\n        \"Conseil d'administration Madagascar. Vocabulaire financier: Ariary, millions, \"\n        \"budget, rapport financier, résolution, délibération. \"\n        \"Termes spécifiques: Fihariana, SON'INVEST, UNIMA, AQUALMA. \"\n        \"Intervenants: Président, Directeur Général, Commissaire aux Comptes. \"\n        \"Format: discours naturel sans répétitions.\"\n    )\n    # LLM (activé par défaut en production)\n    enable_llm: bool = True\n    openai_model: str = \"gpt-4o-mini\" # \"gpt-3.5-turbo\" : Plus économique que GPT-4\n    max_correction_rate: float = 0.18\n    chunk_size_chars: int = 1000\n    chunk_overlap_chars: int = 200\n\nconfig = Config() \nprint(f\"✅ FullRun — Whisper: {config.whisper_model} | device: {config.device} | compute: {config.compute_type}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.562144Z","iopub.execute_input":"2025-09-25T12:09:25.562892Z","iopub.status.idle":"2025-09-25T12:09:25.572694Z","shell.execute_reply.started":"2025-09-25T12:09:25.562864Z","shell.execute_reply":"2025-09-25T12:09:25.572023Z"}},"outputs":[{"name":"stdout","text":"✅ FullRun — Whisper: large-v3 | device: cuda | compute: float16\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"***Comment régler les paramètres selon les cas***\n\nCas A — Audio propre (dictaphones, salle calme)\n*  beam_size=3, best_of=1–2 (plus rapide)\n* no_speech_threshold=0.6 (ok)\n* temperature=0.0\n* VAD : min_silence_duration_ms=1500\n\nCas B — Audio bruité (portes, brouhaha)\n* beam_size=5, best_of=5 (qualité)\n* baisser no_speech_threshold à 0.5 si coupures\n* VAD : threshold=0.4–0.5, min_speech_duration_ms=200, min_silence_duration_ms=1800–2200\n* Garde-fous : garder compression_ratio_threshold=2.4\n\nCas C — CPU-only (pas de GPU Kaggle)\n* compute_type=\"int8\", modèle tiny ou base\n* beam_size=3, best_of=1\n* Threads : cpu_threads=2, num_workers=1\n* Attends un RTF ≈ 2–5 (selon longueur)","metadata":{}},{"cell_type":"markdown","source":"# **Préparation de l'audio**","metadata":{}},{"cell_type":"markdown","source":"**Extrait**","metadata":{}},{"cell_type":"code","source":"def slice_audio(input_path: str, output_path: str, start: float = 0.0, duration: Optional[int] = None) -> str:\n    args = [\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\"-ss\",str(start),\"-i\",input_path,\"-ac\",\"1\",\"-ar\",str(config.sample_rate)]\n    if duration and duration > 0:\n        args += [\"-t\",str(duration)]\n    args += [output_path]\n    ensure_dir(str(Path(output_path).parent))\n    code, _, err = run(args)\n    if code!=0:\n        raise RuntimeError(\"FFmpeg slice failed: \" + err)\n    return output_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.573734Z","iopub.execute_input":"2025-09-25T12:09:25.573944Z","iopub.status.idle":"2025-09-25T12:09:25.597088Z","shell.execute_reply.started":"2025-09-25T12:09:25.573928Z","shell.execute_reply":"2025-09-25T12:09:25.596381Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# **Préprocessing et Débruitage Audio**\n**Classe de débruitage audio avancé**\n","metadata":{}},{"cell_type":"code","source":"def prepare_audio_file(audio_path: str) -> Dict:\n    \"\"\"Prépare et valide le fichier audio pour la transcription\"\"\"\n    import wave\n    import contextlib\n    \n    file_info = {\n        \"path\": audio_path,\n        \"exists\": os.path.exists(audio_path),\n        \"size_mb\": 0,\n        \"duration_seconds\": 0,\n        \"format\": audio_path.split('.')[-1],\n        \"sample_rate\": 0,\n        \"channels\": 0\n    }\n    \n    if file_info[\"exists\"]:\n        file_info[\"size_mb\"] = os.path.getsize(audio_path) / (1024 * 1024)\n        \n        try:\n            # Charger avec librosa pour info\n            y, sr = librosa.load(audio_path, sr=None, duration=10)\n            file_info[\"sample_rate\"] = sr\n            \n            # Durée totale\n            duration = librosa.get_duration(path=audio_path)\n            file_info[\"duration_seconds\"] = duration\n            \n        except Exception as e:\n            print(f\"⚠️ Erreur lecture audio: {e}\")\n    \n    return file_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.597767Z","iopub.execute_input":"2025-09-25T12:09:25.597998Z","iopub.status.idle":"2025-09-25T12:09:25.609429Z","shell.execute_reply.started":"2025-09-25T12:09:25.597975Z","shell.execute_reply":"2025-09-25T12:09:25.608687Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class AudioPreprocessor:\n    def __init__(self, sample_rate: int):\n        self.sr = sample_rate\n    def ffmpeg_enhance(self, src: str, dst: str) -> str:\n        chain = \"highpass=f=100,lowpass=f=7500,adeclip,afftdn=nf=-25,compand=attacks=0.005:decays=0.05:points=-80/-90|-20/-20|0/-10:gain=5\"\n        code,_,err = run([\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\"-i\",src,\"-ac\",\"1\",\"-ar\",str(self.sr),\"-af\",chain,dst])\n        if code!=0: raise RuntimeError(\"FFmpeg failed: \"+err)\n        return dst\n    def reduce_noise(self, src: str, dst: str) -> str:\n        y, sr = librosa.load(src, sr=self.sr)\n        y = nr.reduce_noise(y=y, sr=sr)\n        sf.write(dst, y, sr); return dst\n    def process(self, src: str, outdir: str) -> str:\n        ensure_dir(outdir)\n        ff = str(Path(outdir)/f\"{Path(src).stem}_ffmpeg.wav\")\n        dn = str(Path(outdir)/f\"{Path(src).stem}_denoise.wav\")\n        self.ffmpeg_enhance(src, ff)\n        return self.reduce_noise(ff, dn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.610750Z","iopub.execute_input":"2025-09-25T12:09:25.611175Z","iopub.status.idle":"2025-09-25T12:09:25.621553Z","shell.execute_reply.started":"2025-09-25T12:09:25.611158Z","shell.execute_reply":"2025-09-25T12:09:25.620836Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# **Transcription Audio**\n**Service de transcription avec audio nettoyé**","metadata":{}},{"cell_type":"code","source":"class Transcriber:\n    def __init__(self, cfg: Config):\n        self.cfg = cfg; self.model=None\n\n    def load(self):\n        if self.model is None:\n            # Nettoyer le GPU avant chargement\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            self.model = WhisperModel(\n                self.cfg.whisper_model, \n                device=self.cfg.device, \n                compute_type=self.cfg.compute_type,\n                cpu_threads=4,  # Ajouter pour limiter les threads\n                num_workers=1   # Limiter les workers\n            )\n        return self.model\n    \n    def unload(self):\n        \"\"\"Libérer le modèle de la mémoire\"\"\"\n        if self.model is not None:\n            del self.model\n            self.model = None\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    def _decode(self, wav_path: str):\n        m = self.load()\n        kwargs = dict(\n            language=\"fr\",\n            beam_size=self.cfg.beam_size,\n            best_of=self.cfg.best_of,\n            patience=self.cfg.patience,\n            temperature=self.cfg.temperature,\n            compression_ratio_threshold=self.cfg.compression_ratio_threshold,\n            log_prob_threshold=self.cfg.log_prob_threshold,\n            no_speech_threshold=self.cfg.no_speech_threshold,\n            condition_on_previous_text=False,\n            initial_prompt=self.cfg.initial_prompt,\n            word_timestamps=True,\n            suppress_tokens=self.cfg.suppress_tokens,\n            suppress_blank=self.cfg.suppress_blank,\n            max_initial_timestamp=self.cfg.max_initial_timestamp,\n            vad_filter=self.cfg.use_vad,\n            vad_parameters={\n                \"threshold\": self.cfg.vad_threshold,\n                \"min_speech_duration_ms\": self.cfg.vad_min_speech_duration_ms,\n                \"max_speech_duration_s\": self.cfg.vad_max_speech_duration_s,\n                \"min_silence_duration_ms\": self.cfg.vad_min_silence_duration_ms,\n                \"speech_pad_ms\": self.cfg.vad_speech_pad_ms,\n            } if self.cfg.use_vad else None\n        )\n        return m.transcribe(wav_path, **kwargs)\n\n    def transcribe_long_audio(self, audio_path: str) -> Dict[str, Any]:\n        # Découpage manuel en gros segments (3–5 min) pour robustesse mémoire et cohérence\n        # Charger l'audio par petits morceaux\n        y, sr = librosa.load(audio_path, sr=self.cfg.sample_rate, mono=True)\n        total = len(y) / sr\n        \n        # Libérer l'audio complet après avoir la durée\n        del y\n        gc.collect()\n        \n        L = self.cfg.chunk_length_s\n        O = self.cfg.chunk_overlap_s\n        segs_all = []\n        text_parts = []\n        start = 0.0\n        idx = 0\n\n        while start < total:\n            end = min(start + L, total)\n\n            # Traiter un chunk\n            clip_path = str(FULL_DIR / f\"chunk_{idx:04d}.wav\")\n            slice_audio(audio_path, clip_path, start=start, duration=int(end-start))\n\n            # Transcription\n            segs, info = self._decode(clip_path)\n\n            # Supprimer le chunk immédiatement après traitement\n            Path(clip_path).unlink(missing_ok=True)\n            \n            for s in segs:\n                segs_all.append({\n                    \"start\": float(s.start + start),\n                    \"end\": float(s.end + start),\n                    \"text\": s.text.strip(),\n                    \"no_speech_prob\": float(getattr(s, \"no_speech_prob\", 0.0))\n                })\n                \n            text_parts.append(\" \".join(s.text.strip() for s in segs if float(getattr(s,\"no_speech_prob\",0.0))<=0.9))\n            \n            if end >= total: \n                break\n            \n            start = end - O\n            idx += 1\n\n        transcription = \" \".join(text_parts).strip()\n        return {\"status\":\"success\",\"duration\": float(total), \"segments\": segs_all, \"transcription\": transcription}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.665976Z","iopub.execute_input":"2025-09-25T12:09:25.666177Z","iopub.status.idle":"2025-09-25T12:09:25.677353Z","shell.execute_reply.started":"2025-09-25T12:09:25.666162Z","shell.execute_reply":"2025-09-25T12:09:25.676709Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Exemple d'utilisation\n#result = transcription_service.transcribe_audio(audio_file)\n#print(f\"Transcription: {result['transcription'][:500]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.678892Z","iopub.execute_input":"2025-09-25T12:09:25.679294Z","iopub.status.idle":"2025-09-25T12:09:25.693708Z","shell.execute_reply.started":"2025-09-25T12:09:25.679277Z","shell.execute_reply":"2025-09-25T12:09:25.693150Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# **Diarization**","metadata":{}},{"cell_type":"code","source":"def diarize(transcript: Dict[str,Any], audio_path: str, hf_token: Optional[str]):\n    if transcript.get(\"status\")!=\"success\" or not transcript.get(\"segments\"):\n        return transcript\n    try:\n        if not hf_token:\n            print(\"ℹ️ Pas de HUGGINGFACE_TOKEN — diarisation ignorée.\")\n            return transcript\n        import whisperx\n        align_model, meta = whisperx.load_align_model(language_code='fr', device=config.device)\n        aligned = whisperx.align(\n            [{\"text\":s[\"text\"],\"start\":s[\"start\"],\"end\":s[\"end\"]} for s in transcript[\"segments\"]],\n            align_model, meta, audio_path, device=config.device\n        )\n        diar = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=config.device)\n        dsegs = diar(audio_path)\n        result = whisperx.assign_word_speakers(dsegs, aligned)\n        transcript[\"diarized_segments\"] = result.get(\"segments\", [])\n        return transcript\n    except Exception as e:\n        print(\"⚠️ Diarisation échouée:\", e)\n        return transcript\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.694546Z","iopub.execute_input":"2025-09-25T12:09:25.694800Z","iopub.status.idle":"2025-09-25T12:09:25.706233Z","shell.execute_reply.started":"2025-09-25T12:09:25.694775Z","shell.execute_reply":"2025-09-25T12:09:25.705381Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# **Post-traitement du texte**","metadata":{}},{"cell_type":"code","source":"def normalize_compound_numbers(text: str) -> str:\n    pattern = re.compile(r\"(\\\\d+[\\\\s ]*)milliards?\\\\s+(\\\\d+[\\\\s ]*)millions?\", re.IGNORECASE)\n    def repl(m):\n        b = int(m.group(1).replace(\" \",\"\"))\n        M = int(m.group(2).replace(\" \",\"\"))\n        total = b*1_000_000_000 + M*1_000_000\n        return f\"{total:,}\".replace(\",\", \" \")\n    return pattern.sub(repl, text)\n\ndef normalize_units(text: str) -> str:\n    pattern = re.compile(r\"(\\\\d+[\\\\d\\\\s,.]*)\\\\s*(millions?|milliards?)\", re.IGNORECASE)\n    def repl(m):\n        raw = m.group(1); unit = m.group(2).lower()\n        try:\n            val = float(raw.replace(\" \",\"\").replace(\",\", \".\"))\n        except ValueError:\n            return m.group(0)\n        factor = 1_000_000 if \"million\" in unit else 1_000_000_000\n        total = val*factor\n        return f\"{total:,.0f}\".replace(\",\", \" \")\n    return pattern.sub(repl, text)\n\ndef deduplicate(text: str) -> str:\n    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\\\s+\", text) if s.strip()]\n    seen=set(); out=[]\n    for s in sents:\n        k=s.lower()\n        if k in seen: continue\n        seen.add(k); out.append(s)\n    return \". \".join(out)\n\ndef postprocess_text(text: str) -> str:\n    text = normalize_compound_numbers(text)\n    text = normalize_units(text)\n    text = deduplicate(text)\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.707066Z","iopub.execute_input":"2025-09-25T12:09:25.707340Z","iopub.status.idle":"2025-09-25T12:09:25.722122Z","shell.execute_reply.started":"2025-09-25T12:09:25.707314Z","shell.execute_reply":"2025-09-25T12:09:25.721384Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# **Nettoyage LLM**","metadata":{}},{"cell_type":"code","source":"class LLMPostEditor:\n    def __init__(self, api_key: Optional[str], model: str, max_rate: float, size: int, overlap: int):\n        self.api_key = api_key; self.model = model\n        self.max_rate = max_rate; self.size=size; self.overlap=overlap\n        self.client = None\n        if api_key:\n            from openai import OpenAI\n            self.client = OpenAI(api_key=api_key)\n\n    def chunks(self, text: str) -> List[str]:\n        if not text: return []\n        step = max(1, self.size - self.overlap)\n        return [text[i:i+self.size] for i in range(0, len(text), step)]\n\n    def clean(self, text: str) -> Tuple[str, float]:\n        if not self.client or not text: return text, 0.0\n        cleaned=[]; delta=0\n        for i, chunk in enumerate(self.chunks(text), 1):\n            messages=[\n                {\"role\":\"system\",\"content\":(\n                    \"Tu corriges une transcription FR: orthographe, grammaire, ponctuation, noms propres. \"\n                    \"NE JAMAIS ajouter d'information non présente dans la transcription\"\n                    \"Ne change pas le sens. Applique un style réunion formel.\"\n                )},\n                {\"role\":\"user\",\"content\":chunk}\n            ]\n            try:\n                resp = self.client.chat.completions.create(model=self.model, messages=messages, temperature=0.2, max_tokens=1400)\n                ct = resp.choices[0].message.content.strip()\n            except Exception as e:\n                print(f\"⚠️ LLM chunk {i} échoué:\", e); ct = chunk\n            cleaned.append(ct); delta += abs(len(ct)-len(chunk))\n        merged = \" \".join(cleaned)\n        rate = delta/max(len(text),1)\n        if rate > self.max_rate:\n            print(f\"⚠️ LLM correction rate {rate:.1%} > seuil {self.max_rate:.0%}. On garde le texte post-traité.\")\n            return text, rate\n        return merged, rate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.723882Z","iopub.execute_input":"2025-09-25T12:09:25.724083Z","iopub.status.idle":"2025-09-25T12:09:25.737101Z","shell.execute_reply.started":"2025-09-25T12:09:25.724069Z","shell.execute_reply":"2025-09-25T12:09:25.736498Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"# **Fallback AssemblyAI (si échec Whisper)**","metadata":{}},{"cell_type":"code","source":"class AssemblyAIFallback:\n    \"\"\"Service de fallback avec AssemblyAI\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        \n    def transcribe_with_assemblyai(self, audio_path: str) -> Dict:\n        \"\"\"\n        Transcription de secours via AssemblyAI\n        \n        Args:\n            audio_path: Chemin du fichier audio\n            \n        Returns:\n            Dict avec la transcription\n        \"\"\"\n        if not self.api_key:\n            return {\n                \"status\": \"error\",\n                \"error\": \"Clé API AssemblyAI non configurée\"\n            }\n        \n        try:\n            import assemblyai as aai\n            \n            print(\"🔄 Utilisation du fallback AssemblyAI...\")\n            \n            aai.settings.api_key = self.api_key\n            transcriber = aai.Transcriber()\n            \n            # Upload et transcription\n            config_lang = aai.TranscriptionConfig(\n                language_code=\"fr\",\n                punctuate=True,\n                format_text=True,\n                disfluencies=True,\n                speaker_labels=True\n            )\n            transcript = transcriber.transcribe(audio_path, config=config_lang)\n            \n            if transcript.status == aai.TranscriptStatus.error:\n                raise Exception(f\"Erreur AssemblyAI: {transcript.error}\")\n            \n            # Attente de la transcription\n            while transcript.status not in [aai.TranscriptStatus.completed, aai.TranscriptStatus.error]:\n                time.sleep(5)\n                transcript = transcriber.get_transcript(transcript.id)\n            \n            return {\n                \"status\": \"success\",\n                \"method\": \"assemblyai\",\n                \"transcription\": transcript.text,\n                \"confidence\": transcript.confidence if hasattr(transcript, 'confidence') else 0.85,\n                \"words\": transcript.words if hasattr(transcript, 'words') else []\n            }\n            \n        except Exception as e:\n            print(f\"❌ Erreur AssemblyAI: {str(e)}\")\n            return {\n                \"status\": \"error\",\n                \"error\": str(e),\n                \"method\": \"assemblyai\"\n            }\n\n# Service de fallback\nfallback_service = AssemblyAIFallback(config.assemblyai_key)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-09-25T12:09:25.737711Z","iopub.execute_input":"2025-09-25T12:09:25.737986Z","iopub.status.idle":"2025-09-25T12:09:25.749444Z","shell.execute_reply.started":"2025-09-25T12:09:25.737942Z","shell.execute_reply":"2025-09-25T12:09:25.748820Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"1. Par défaut, la langue est auto. Pour ton cas, force français :\n        config = aai.TranscriptionConfig(language_code=\"fr\")\n2. Diarisation (orateurs)\n        config = aai.TranscriptionConfig(speaker_labels=True)\n\nExemple :\n    config = aai.TranscriptionConfig(language_code=\"fr\", speaker_labels=True)\n    transcript = transcriber.transcribe(audio_path, config=config)\n\nAppel :\n    Si TranscriptionService.transcribe_audio renvoie status=\"error\" ou un real_time_factor >> 5 (trop lent) ou trop de segments sous ton confidence_threshold, alors :\n        > result = fallback_service.transcribe_with_assemblyai(audio_path)","metadata":{}},{"cell_type":"markdown","source":"# **Pipeline de transcription avec gestion automatique du fallback**","metadata":{}},{"cell_type":"code","source":"def transcribe_audio_pipeline(audio_path: str, cfg: Config, save_json: bool=True) -> Dict[str, Any]:\n    try:\n        # Initialisation\n        cleanup_temp_files()\n        \n        # [1] Prétraitement\n        print(\"[1/5] Prétraitement audio...\")\n        preprocessor = AudioPreprocessor(cfg.sample_rate)\n        clean_path = preprocessor.process(audio_path, str(FULL_DIR))\n        \n        # Libérer mémoire après prétraitement\n        print_memory_usage()\n        del preprocessor\n        gc.collect()\n        \n        # [2] Transcription avec gestion mémoire\n        print(\"[2/5] Transcription...\")\n        suggested_model = check_gpu_memory()\n        if suggested_model and suggested_model != cfg.whisper_model:\n            print(f\"🔁 Ajustement Whisper: {cfg.whisper_model} -> {suggested_model}\")\n            cfg.whisper_model = suggested_model\n        if torch.cuda.is_available():\n            free_mem, total_mem = torch.cuda.mem_get_info()\n            print(f\"📊 GPU libre avant transcription: {free_mem/1e9:.2f}GB / {total_mem/1e9:.2f}GB\")\n            if free_mem < 6e9:\n                previous_compute = cfg.compute_type\n                if free_mem < 4e9 and previous_compute != \"int8\":\n                    cfg.compute_type = \"int8\"\n                elif previous_compute not in {\"int8\", \"int8_float16\"}:\n                    cfg.compute_type = \"int8_float16\"\n                if cfg.compute_type != previous_compute:\n                    print(f\"⚙️ Compute type ajusté: {previous_compute} -> {cfg.compute_type}\")\n                else:\n                    print(f\"⚙️ Compute type conservé: {cfg.compute_type}\")\n                    \n        transcriber = Transcriber(cfg)\n        asr = transcriber.transcribe_long_audio(clean_path)\n        print_memory_usage()\n        \n        # Libérer le modèle après transcription\n        print_memory_usage()\n        transcriber.unload()\n        \n        # Diarisation\n        print(\"[3/5] Diarisation (pyannote -> whisperx)\")\n        asr = diarize(asr, clean_path, HUGGINGFACE_TOKEN)\n        print_memory_usage()\n    \n        # Post-traitement règles\n        print(\"[4/5] Post-traitement (dédup + chiffres)\")\n        post_text = postprocess_text(asr.get(\"transcription\",\"\"))\n        asr[\"transcription_postprocessed\"] = post_text\n        print_memory_usage()\n    \n        # LLM (si activé)\n        print(\"[5/5] Nettoyage LLM (chunks 1000 caractères)\")\n        final_text = post_text; rate = 0.0\n        if cfg.enable_llm and OPENAI_API_KEY:\n            editor = LLMPostEditor(OPENAI_API_KEY, cfg.openai_model, cfg.max_correction_rate, cfg.chunk_size_chars, cfg.chunk_overlap_chars)\n            final_text, rate = editor.clean(post_text)\n        else:\n            print(\"ℹ️ LLM non activé ou clé absente — on garde le post-traitement règles.\")\n    \n        asr[\"transcription_llm\"] = final_text\n        asr[\"llm_correction_rate\"] = rate\n\n        print_memory_usage()\n    \n        # Sauvegarde JSONs\n        if save_json:\n            base = f\"full_{int(time.time())}\"\n            out_path = str(Path(OUTPUT_PATH)/f\"{base}.json\")\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(asr, f, ensure_ascii=False, indent=2)\n            print(\"💾 Sauvegardé:\", out_path)\n        \n        return asr\n        \n    finally:\n        # Nettoyage final\n        cleanup_temp_files()\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.750074Z","iopub.execute_input":"2025-09-25T12:09:25.750298Z","iopub.status.idle":"2025-09-25T12:09:25.765651Z","shell.execute_reply.started":"2025-09-25T12:09:25.750278Z","shell.execute_reply":"2025-09-25T12:09:25.765091Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Test avec votre fichier audio\n#audio_file = f\"{UPLOAD_PATH}atelier.mp3\"\n#audio_file = f\"{UPLOAD_PATH}test_1h.wav\"\naudio_file = f\"{UPLOAD_PATH}test_30mn.mp3\"\n#audio_info = prepare_audio_file(audio_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.766342Z","iopub.execute_input":"2025-09-25T12:09:25.766567Z","iopub.status.idle":"2025-09-25T12:09:25.778871Z","shell.execute_reply.started":"2025-09-25T12:09:25.766548Z","shell.execute_reply":"2025-09-25T12:09:25.778165Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"result = transcribe_audio_pipeline(audio_file, config, save_json=True)\nprint(result.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:09:25.779616Z","iopub.execute_input":"2025-09-25T12:09:25.779921Z","iopub.status.idle":"2025-09-25T12:14:31.947496Z","shell.execute_reply.started":"2025-09-25T12:09:25.779900Z","shell.execute_reply":"2025-09-25T12:14:31.946699Z"}},"outputs":[{"name":"stdout","text":"[1/5] Prétraitement audio...\nGPU Memory: 0.00GB / 0.00GB\nRAM Usage: 1.86GB\n[2/5] Transcription...\n📊 GPU: 15.71GB libres / 15.83GB total\n📊 GPU libre avant transcription: 15.71GB / 15.83GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6de3455c9e9941fb95f16f4262dcd9b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f89776df94b4b0db264e4e81426f512"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocabulary.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f03bc5e3f234015b477594cd48831d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9270e380409478d9b95891082cf80ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb55370c5f8c427091f1dd74d9850138"}},"metadata":{}},{"name":"stdout","text":"GPU Memory: 0.00GB / 0.00GB\nRAM Usage: 2.35GB\nGPU Memory: 0.00GB / 0.00GB\nRAM Usage: 2.35GB\n[3/5] Diarisation (pyannote -> whisperx)\n⚠️ Diarisation échouée: No module named 'whisperx'\nGPU Memory: 0.00GB / 0.00GB\nRAM Usage: 2.35GB\n[4/5] Post-traitement (dédup + chiffres)\nGPU Memory: 0.00GB / 0.00GB\nRAM Usage: 2.35GB\n[5/5] Nettoyage LLM (chunks 1000 caractères)\nGPU Memory: 0.00GB / 0.00GB\nRAM Usage: 2.35GB\n💾 Sauvegardé: /kaggle/working/full_1758802471.json\ndict_keys(['status', 'duration', 'segments', 'transcription', 'transcription_postprocessed', 'transcription_llm', 'llm_correction_rate'])\n","output_type":"stream"}],"execution_count":48}]}