{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13136823,"sourceType":"datasetVersion","datasetId":8302666}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transcription CA\n\nCe notebook applique le **pipeline complet** :\n- **Pr√©traitement** audio (FFmpeg + noisereduce)\n- **Transcription** faster-whisper (r√©glages anti-hallucinations)\n- **Chunks longs** pour une meilleure coh√©rence (3‚Äì5 min)\n- **Diarisation** (pyannote ‚Üí fallback whisperx)\n- **Post-traitement** (d√©dup + normalisation chiffres/unit√©s)\n- **Nettoyage LLM** par morceaux (1000 caract√®res) avec borne de correction\n- **Sauvegarde JSON** des sorties (raw, diarized, cleaned, llm_cleaned)","metadata":{}},{"cell_type":"markdown","source":"# **Installation des packages n√©cessaires**","metadata":{}},{"cell_type":"code","source":"# %%capture\n# # Installation silencieuse des d√©pendances avec gestion des conflits\n\n# # 1. Mise √† jour pip pour √©viter les probl√®mes\n# #!pip install --upgrade pip -q\n\n# # 2. Installation FFmpeg (syst√®me)\n# !apt-get update -qq\n# !apt-get install -qq ffmpeg sox\n\n# # 3. Nettoyage et verrouillage de la stack NumPy/Numba/Scipy\n# #!pip uninstall -y numpy numba >/dev/null 2>&1 || true\n# !pip install -q numpy==1.26.4 scipy==1.11.4\n# !pip install -q numba==0.58.1\n# !pip install -q torch==2.1.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n\n# # 4. Installation des packages de transcription\n# #!pip install -q openai-whisper==20231117\n# !pip install -q faster-whisper==1.0.3\n\n# # 5. Packages de d√©bruitage audio\n# !pip install -q librosa==0.10.1\n# !pip install -q soundfile==0.12.1\n# !pip install -q noisereduce==3.0.0\n# !pip install -q pydub==0.25.1\n\n# # 6. Diarization\n# !pip install -q \"pyannote.audio>=3.1\"\n# !pip install -q whisperx\n\n# !pip install -q regex==2023.12.25 unidecode==1.3.8\n\n# # 7. Packages documents\n# !pip install -q python-docx==1.2.0\n# !pip install -q python-pptx==1.0.2\n\n# # 8. Packages LLM et NLP\n# !pip install -q openai==1.91.0\n# !pip install -q assemblyai==0.44.3\n# !pip install -q tiktoken==0.9.0\n\n# # 9. LangChain\n# #!pip install -q langchain==0.3.27 langchain-community==0.3.29 langchain-core==0.3.30\n\n# # 10. Packages utilitaires\n# !pip install -q pandas==2.1.4 matplotlib==3.8.2 seaborn==0.13.2\n\n# # 11. Installation FAISS pour le RAG\n# #!pip install -q faiss-cpu==1.7.4\n\n# print(\"‚úÖ Installation termin√©e!\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-09-29T07:38:00.038612Z","iopub.execute_input":"2025-09-29T07:38:00.038826Z","iopub.status.idle":"2025-09-29T07:38:00.043874Z","shell.execute_reply.started":"2025-09-29T07:38:00.038806Z","shell.execute_reply":"2025-09-29T07:38:00.043252Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n# Installation minimale des d√©pendances n√©cessaires sans perturber l'environnement Kaggle\nimport importlib\nimport os\nimport shutil\nimport subprocess\nimport sys\n\ndef ensure_packages(requirements):\n    missing = []\n    for module_name, package_spec in requirements:\n        try:\n            importlib.import_module(module_name)\n        except Exception:\n            missing.append(package_spec)\n    if missing:\n        cmd = [sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '-q'] + missing\n        subprocess.check_call(cmd)\n\ncore_requirements = [\n    ('faster_whisper', 'faster-whisper==1.0.3'),\n    ('librosa', 'librosa==0.10.1'),\n    ('soundfile', 'soundfile==0.12.1'),\n    ('noisereduce', 'noisereduce==3.0.0'),\n    ('pydub', 'pydub==0.25.1'),\n    ('docx', 'python-docx==1.2.0'),\n    ('pptx', 'python-pptx==1.0.2'),\n    ('openai', 'openai==1.91.0'),\n    ('assemblyai', 'assemblyai==0.44.3'),\n    ('tiktoken', 'tiktoken==0.9.0'),\n]\n\nensure_packages(core_requirements)\n\n# if os.environ.get('INSTALL_LANGCHAIN', '0') == '1':\n#     optional_requirements = [\n#         ('langchain', 'langchain==0.3.27'),\n#         ('langchain_community', 'langchain-community==0.3.29'),\n#         ('faiss', 'faiss-cpu==1.7.4'),\n#     ]\n#     try:\n#         ensure_packages(optional_requirements)\n#     except subprocess.CalledProcessError:\n#         pass\n\nif not shutil.which('ffmpeg'):\n    subprocess.check_call(['apt-get', 'update', '-qq'])\n    subprocess.check_call(['apt-get', 'install', '-qq', 'ffmpeg'])\n\nprint('‚úÖ V√©rification des d√©pendances termin√©e')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:38:00.045008Z","iopub.execute_input":"2025-09-29T07:38:00.045223Z","iopub.status.idle":"2025-09-29T07:38:05.164470Z","shell.execute_reply.started":"2025-09-29T07:38:00.045207Z","shell.execute_reply":"2025-09-29T07:38:05.163646Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# V√©rification que tout est install√© correctement\nimport importlib\n\npackages_to_check = [\n    ('numpy', 'numpy'),\n    ('scipy', 'scipy'),\n    ('numba', 'numba'),\n    ('whisper', 'openai-whisper'),\n    ('faster_whisper', 'faster-whisper'),\n    ('librosa', 'librosa'),\n    ('soundfile', 'soundfile'),\n    ('noisereduce', 'noisereduce'),\n    ('pydub', 'pydub'),\n    ('docx', 'python-docx'),\n    ('pptx', 'python-pptx'),\n    ('openai', 'openai'),\n    ('langchain', 'langchain'),\n    ('langchain_community', 'langchain-community'),\n    ('faiss', 'faiss-cpu'),\n    ('assemblyai', 'assemblyai'),\n    ('tiktoken', 'tiktoken')\n]\n\nprint(\"üîç V√©rification des packages install√©s:\")\nprint(\"-\" * 50)\n\nall_ok = True\nfor import_name, package_name in packages_to_check:\n    try:\n        module = importlib.import_module(import_name)\n        version = getattr(module, '__version__', 'N/A')\n        print(f\"‚úÖ {package_name:20} : {version}\")\n    except ImportError:\n        print(f\"‚ùå {package_name:20} : Non install√©\")\n        all_ok = False\n    except Exception as exc:\n        print(f\"‚ö†Ô∏è {package_name:20} : Erreur lors de l'import ({type(exc).__name__}: {exc})\")\n        all_ok = False\n\nif all_ok:\n    print(\"‚ú® Tous les packages sont install√©s correctement!\")\nelse:\n    print(\"‚ö†Ô∏è Certains packages n√©cessitent une attention. Consultez les messages ci-dessus.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:53.927372Z","iopub.execute_input":"2025-09-29T07:51:53.928201Z","iopub.status.idle":"2025-09-29T07:51:53.937201Z","shell.execute_reply.started":"2025-09-29T07:51:53.928171Z","shell.execute_reply":"2025-09-29T07:51:53.936366Z"}},"outputs":[{"name":"stdout","text":"üîç V√©rification des packages install√©s:\n--------------------------------------------------\n‚úÖ numpy                : 1.26.4\n‚úÖ scipy                : 1.15.3\n‚úÖ numba                : 0.60.0\n‚ùå openai-whisper       : Non install√©\n‚úÖ faster-whisper       : 1.0.3\n‚úÖ librosa              : 0.11.0\n‚úÖ soundfile            : 0.13.1\n‚úÖ noisereduce          : N/A\n‚úÖ pydub                : N/A\n‚úÖ python-docx          : 1.2.0\n‚úÖ python-pptx          : 1.0.2\n‚úÖ openai               : 1.91.0\n‚úÖ langchain            : 0.3.26\n‚ùå langchain-community  : Non install√©\n‚ùå faiss-cpu            : Non install√©\n‚úÖ assemblyai           : 0.44.3\n‚úÖ tiktoken             : 0.9.0\n‚ö†Ô∏è Certains packages n√©cessitent une attention. Consultez les messages ci-dessus.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# **Imports et configuration GPU**","metadata":{}},{"cell_type":"code","source":"# Imports standards\nimport os, sys, json, re, shutil, subprocess, tempfile\nfrom math import ceil\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datetime import datetime, timezone\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass, field\nimport gc  # Garbage collector\n\nimport numpy as np\nimport pandas as pd\n\n# Imports audio et d√©bruitage\nimport librosa\nimport soundfile as sf\nimport noisereduce as nr\nfrom scipy.signal import butter, filtfilt, medfilt\nfrom pydub import AudioSegment\n\n# Imports pour la transcription\n#import whisper\nfrom faster_whisper import WhisperModel\n\n# Imports pour les documents\nfrom docx import Document\nfrom pptx import Presentation\n\n# Imports pour le NLP et LLM\nimport openai\ntry:\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from langchain_community.vectorstores import FAISS\n    from langchain_community.embeddings import OpenAIEmbeddings\n    langchain_available = True\nexcept ImportError:\n    print(\"‚ö†Ô∏è LangChain non disponible\")\n    langchain_available = False\n\nimport torch\nprint(f\"üîß PyTorch: {torch.__version__}\")\nprint(f\"üéÆ CUDA disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:53.938571Z","iopub.execute_input":"2025-09-29T07:51:53.938781Z","iopub.status.idle":"2025-09-29T07:51:53.956952Z","shell.execute_reply.started":"2025-09-29T07:51:53.938765Z","shell.execute_reply":"2025-09-29T07:51:53.956261Z"}},"outputs":[{"name":"stdout","text":"‚ö†Ô∏è LangChain non disponible\nüîß PyTorch: 2.6.0+cu124\nüéÆ CUDA disponible: True\n   GPU: Tesla T4\n   M√©moire: 15.83 GB\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# **Configuration des cl√©s API**","metadata":{}},{"cell_type":"code","source":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    OPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\n    ASSEMBLYAI_API_KEY = user_secrets.get_secret(\"ASSEMBLYAI_API_KEY\")\n    HUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n    GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\nexcept:\n    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n    ASSEMBLYAI_API_KEY = os.environ.get(\"ASSEMBLYAI_API_KEY\", \"\")\n    HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", \"\")\n    GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:53.957719Z","iopub.execute_input":"2025-09-29T07:51:53.957967Z","iopub.status.idle":"2025-09-29T07:51:54.317508Z","shell.execute_reply.started":"2025-09-29T07:51:53.957944Z","shell.execute_reply":"2025-09-29T07:51:54.316903Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# **Configuration des chemins**","metadata":{}},{"cell_type":"code","source":"UPLOAD_PATH = \"/kaggle/input/meeting-audio/\" # Chemin des fichiers upload√©s \nOUTPUT_PATH = \"/kaggle/working\" # Chemin de sortie","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:54.318936Z","iopub.execute_input":"2025-09-29T07:51:54.319191Z","iopub.status.idle":"2025-09-29T07:51:54.322701Z","shell.execute_reply.started":"2025-09-29T07:51:54.319173Z","shell.execute_reply":"2025-09-29T07:51:54.321967Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"DEBUG_PATH = Path(OUTPUT_PATH) / \"debug\"\nDEBUG_PATH.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:54.323345Z","iopub.execute_input":"2025-09-29T07:51:54.323521Z","iopub.status.idle":"2025-09-29T07:51:54.342851Z","shell.execute_reply.started":"2025-09-29T07:51:54.323507Z","shell.execute_reply":"2025-09-29T07:51:54.342162Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"TEMP_DIR = Path(OUTPUT_PATH) / \"temp_chunks\"\nTEMP_DIR.mkdir(parents=True, exist_ok=True)\n\ndef cleanup_temp_files():\n    \"\"\"Nettoyer les fichiers temporaires\"\"\"\n    if TEMP_DIR.exists():\n        shutil.rmtree(TEMP_DIR)\n    TEMP_DIR.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:54.343539Z","iopub.execute_input":"2025-09-29T07:51:54.343748Z","iopub.status.idle":"2025-09-29T07:51:54.359503Z","shell.execute_reply.started":"2025-09-29T07:51:54.343733Z","shell.execute_reply":"2025-09-29T07:51:54.358970Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# **Utilitaires de commande syst√®me**","metadata":{}},{"cell_type":"code","source":"def ensure_dir(p): \n    Path(p).mkdir(parents=True, exist_ok=True) #V√©rification cr√©ation de dossier\n    \ndef run(cmd): # Lancement commande\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate()\n    return p.returncode, out.decode(), err.decode()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:54.360231Z","iopub.execute_input":"2025-09-29T07:51:54.360502Z","iopub.status.idle":"2025-09-29T07:51:54.376182Z","shell.execute_reply.started":"2025-09-29T07:51:54.360487Z","shell.execute_reply":"2025-09-29T07:51:54.375479Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def check_gpu_memory():\n    if torch.cuda.is_available():\n        free, total = torch.cuda.mem_get_info()\n        print(f\"üìä GPU: {free/1e9:.2f}GB libres / {total/1e9:.2f}GB total\")\n        if free < 4e9:  # Moins de 4GB libres\n            print(\"‚ö†Ô∏è M√©moire GPU faible, utilisation de 'base' recommand√©e\")\n            return \"medium\"\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:54.376898Z","iopub.execute_input":"2025-09-29T07:51:54.377106Z","iopub.status.idle":"2025-09-29T07:51:54.390959Z","shell.execute_reply.started":"2025-09-29T07:51:54.377083Z","shell.execute_reply":"2025-09-29T07:51:54.390443Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def save_debug_json(data: Dict, step_name: str, timestamp: Optional[str] = None) -> str:\n    \"\"\"Sauvegarde JSON de debug pour chaque √©tape\"\"\"\n    if not config.save_intermediate_json:\n        return \"\"\n    \n    if timestamp is None:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    filename = f\"{step_name}_{timestamp}.json\"\n    filepath = DEBUG_PATH / filename\n    \n    # Cr√©er un r√©sum√© pour les donn√©es volumineuses\n    debug_data = {\n        \"step\": step_name,\n        \"timestamp\": timestamp,\n        \"status\": data.get(\"status\", \"unknown\"),\n        \"summary\": {}\n    }\n    \n    if \"segments\" in data and isinstance(data[\"segments\"], list):\n        debug_data[\"summary\"][\"total_segments\"] = len(data[\"segments\"])\n        debug_data[\"summary\"][\"sample_segments\"] = data[\"segments\"][:3] if data[\"segments\"] else []\n        debug_data[\"segments_count\"] = len(data[\"segments\"])\n    \n    if \"transcription\" in data:\n        debug_data[\"summary\"][\"text_length\"] = len(data[\"transcription\"])\n        debug_data[\"summary\"][\"text_preview\"] = data[\"transcription\"][:500] + \"...\" if len(data[\"transcription\"]) > 500 else data[\"transcription\"]\n    \n    if \"transcription_postprocessed\" in data:\n        debug_data[\"summary\"][\"postprocessed_length\"] = len(data[\"transcription_postprocessed\"])\n        debug_data[\"summary\"][\"postprocessed_preview\"] = data[\"transcription_postprocessed\"][:500] + \"...\"\n    \n    if \"transcription_llm\" in data:\n        debug_data[\"summary\"][\"llm_length\"] = len(data[\"transcription_llm\"])\n        debug_data[\"summary\"][\"llm_preview\"] = data[\"transcription_llm\"][:500] + \"...\"\n        debug_data[\"llm_correction_rate\"] = data.get(\"llm_correction_rate\", 0)\n    \n    # Ajouter les m√©tadonn√©es compl√®tes\n    debug_data[\"full_data_keys\"] = list(data.keys())\n    \n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(debug_data, f, ensure_ascii=False, indent=2)\n    \n    print(f\"üìÅ Debug JSON sauv√©: {filepath}\")\n    return str(filepath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:54.391775Z","iopub.execute_input":"2025-09-29T07:51:54.391989Z","iopub.status.idle":"2025-09-29T07:51:54.408893Z","shell.execute_reply.started":"2025-09-29T07:51:54.391970Z","shell.execute_reply":"2025-09-29T07:51:54.408405Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"**Monitoring et debug**","metadata":{}},{"cell_type":"code","source":"def print_memory_usage(step_name: str = \"\"):\n    \"\"\"Afficher l'utilisation m√©moire\"\"\"\n    prefix = f\"[{step_name}] \" if step_name else \"\"\n    if torch.cuda.is_available():\n        print(f\"{prefix}GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB / {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n    import psutil\n    process = psutil.Process()\n    print(f\"{prefix}RAM Usage: {process.memory_info().rss / 1e9:.2f}GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:54.411462Z","iopub.execute_input":"2025-09-29T07:51:54.411990Z","iopub.status.idle":"2025-09-29T07:51:54.429084Z","shell.execute_reply.started":"2025-09-29T07:51:54.411974Z","shell.execute_reply":"2025-09-29T07:51:54.428407Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# **Configuration du pipeline**","metadata":{}},{"cell_type":"code","source":"def get_optimal_model_size() -> str:\n    \"\"\"\n    D√©termine automatiquement la taille du mod√®le Whisper selon les ressources.\n    Adapt√© du projet SIIS pour une meilleure gestion m√©moire.\n    \"\"\"\n    if torch.cuda.is_available():\n        try:\n            total_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n        except:\n            total_mem = 0\n        \n        if total_mem >= 12:\n            return \"large-v3\"\n        elif total_mem >= 8:\n            return \"medium\"\n        elif total_mem >= 4:\n            return \"small\"\n        else:\n            return \"base\"\n    \n    # Mode CPU\n    if psutil is not None:\n        try:\n            ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n        except:\n            ram_gb = 0\n    else:\n        ram_gb = 8  # D√©faut conservateur\n    \n    if ram_gb >= 16:\n        return \"small\"\n    elif ram_gb >= 8:\n        return \"base\"\n    else:\n        return \"tiny\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T07:51:54.429680Z","iopub.execute_input":"2025-09-29T07:51:54.429853Z","iopub.status.idle":"2025-09-29T07:51:54.453867Z","shell.execute_reply.started":"2025-09-29T07:51:54.429839Z","shell.execute_reply":"2025-09-29T07:51:54.453369Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"@dataclass \nclass Config: \n    \"\"\"Configuration centralis√©e pour Kaggle\"\"\" \n    \n    timezone: str = \"Indian/Antananarivo\"\n\n    # Debug\n    debug_mode: bool = True\n    save_intermediate_json: bool = True\n    \n    # Cl√©s API \n    openai_key: str = OPENAI_API_KEY \n    assemblyai_key: str = ASSEMBLYAI_API_KEY\n    huggingface_token: str = HUGGINGFACE_TOKEN\n    \n    # Whisper\n    whisper_model: str = get_optimal_model_size() # 'tiny', 'base', 'small', 'medium', 'large', \"large-v3\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    compute_type: str = \"float16\" if torch.cuda.is_available() else \"int8\"\n    num_workers: int = 4\n    \n    # Audio\n    sample_rate: int = 16000\n    \n    # Decoding / anti-hallucination\n    beam_size: int = 5\n    best_of: int = 5\n    patience: float = 1.0\n    temperature: float = 0.0\n    compression_ratio_threshold: float = 2.4\n    log_prob_threshold: float = -1.0\n    no_speech_threshold: float = 0.6\n    condition_on_previous_text: bool = False\n    suppress_blank: bool = True\n    suppress_tokens: list[int] = field(default_factory=lambda: [-1])\n    max_initial_timestamp: float = 1.0\n    \n    # VAD\n    use_vad: bool = True\n    vad_threshold: float = 0.5\n    vad_min_speech_duration_ms: int = 250\n    vad_max_speech_duration_s: float = float('inf')\n    vad_min_silence_duration_ms: int = 2000\n    vad_speech_pad_ms: int = 400\n    \n    # Chunks longs pour coh√©rence (3‚Äì5 min)\n    chunk_length_s: int = 900\n    chunk_overlap_s: int = 30\n    \n    # Post-traitement\n    max_repetitions: int = 3\n    \n    # Prompt sp√©cialis√©\n    initial_prompt: str = (\n        \"Transcription d'une r√©union du conseil d'administration √† Madagascar. \"\n        \"Vocabulaire: conseil d'administration, proc√®s-verbal, quorum, \"\n        \"r√©solution, d√©lib√©ration, vote, ordre du jour, budget, \"\n        \"millions d'Ariary, rapport financier. \"\n        \"Termes sp√©cifiques: Fihariana, SON'INVEST, UNIMA, AQUALMA. \"\n        \"Format: discours naturel sans r√©p√©titions ni hallucinations.\"\n    )\n    \n    # LLM (activ√© par d√©faut en production)\n    enable_llm: bool = True\n    use_groq: bool = True\n    groq_model: str = \"llama-3.3-70b-versatile\"  # ou \"llama-3.3-70b-versatile\"\n    openai_model: str = \"gpt-4o-mini\" # \"gpt-3.5-turbo\" : Plus √©conomique que GPT-4 # Fallback\n    max_correction_rate: float = 0.18\n    chunk_size_chars: int = 1000\n    chunk_overlap_chars: int = 200\n\nconfig = Config() \n\nprint(f\"‚úÖ Configuration: Whisper {config.whisper_model} | Device: {config.device}\")\nprint(f\"   Chunking: {config.chunk_length_s}s | Condition on previous: {config.condition_on_previous_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.568985Z","iopub.execute_input":"2025-09-29T08:25:53.569568Z","iopub.status.idle":"2025-09-29T08:25:53.579901Z","shell.execute_reply.started":"2025-09-29T08:25:53.569540Z","shell.execute_reply":"2025-09-29T08:25:53.579104Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Configuration: Whisper large-v3 | Device: cuda\n   Chunking: 900s | Condition on previous: False\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"***Comment r√©gler les param√®tres selon les cas***\n\nCas A ‚Äî Audio propre (dictaphones, salle calme)\n*  beam_size=3, best_of=1‚Äì2 (plus rapide)\n* no_speech_threshold=0.6 (ok)\n* temperature=0.0\n* VAD : min_silence_duration_ms=1500\n\nCas B ‚Äî Audio bruit√© (portes, brouhaha)\n* beam_size=5, best_of=5 (qualit√©)\n* baisser no_speech_threshold √† 0.5 si coupures\n* VAD : threshold=0.4‚Äì0.5, min_speech_duration_ms=200, min_silence_duration_ms=1800‚Äì2200\n* Garde-fous : garder compression_ratio_threshold=2.4\n\nCas C ‚Äî CPU-only (pas de GPU Kaggle)\n* compute_type=\"int8\", mod√®le tiny ou base\n* beam_size=3, best_of=1\n* Threads : cpu_threads=2, num_workers=1\n* Attends un RTF ‚âà 2‚Äì5 (selon longueur)","metadata":{}},{"cell_type":"markdown","source":"# **Pr√©paration de l'audio**","metadata":{}},{"cell_type":"code","source":"def slice_audio(input_path: str, output_path: str, start: float = 0.0, duration: Optional[int] = None) -> str:\n    args = [\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\"-ss\",str(start),\"-i\",input_path,\"-ac\",\"1\",\"-ar\",str(config.sample_rate)]\n    if duration and duration > 0:\n        args += [\"-t\",str(duration)]\n    args += [output_path]\n    ensure_dir(str(Path(output_path).parent))\n    code, _, err = run(args)\n    if code!=0:\n        raise RuntimeError(\"FFmpeg slice failed: \" + err)\n    return output_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.583641Z","iopub.execute_input":"2025-09-29T08:25:53.583860Z","iopub.status.idle":"2025-09-29T08:25:53.613087Z","shell.execute_reply.started":"2025-09-29T08:25:53.583844Z","shell.execute_reply":"2025-09-29T08:25:53.612357Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":"# **Pr√©processing et D√©bruitage Audio**","metadata":{}},{"cell_type":"code","source":"class AudioPreprocessor:\n    \"\"\"Pr√©traitement audio avec FFmpeg et r√©duction de bruit\"\"\"\n    \n    def __init__(self, sample_rate: int = 16000):\n        self.sample_rate = sample_rate\n    \n    def ffmpeg_enhance(self, input_path: str, output_path: str):\n        \"\"\"Am√©liore l'audio avec FFmpeg\"\"\"\n        cmd = [\n            \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n            \"-i\", input_path,\n            \"-ac\", \"1\",  # Mono\n            \"-ar\", str(self.sample_rate),  # 16kHz\n            \"-af\", \"highpass=f=200,lowpass=f=3000,afftdn=nf=-20\",  # Filtres\n            output_path\n        ]\n        subprocess.run(cmd, check=True)\n    \n    def reduce_noise(self, input_path: str, output_path: str):\n        \"\"\"R√©duit le bruit avec noisereduce\"\"\"\n        y, sr = librosa.load(input_path, sr=self.sample_rate)\n        y_clean = nr.reduce_noise(y=y, sr=sr, stationary=True, prop_decrease=0.8)\n        sf.write(output_path, y_clean, sr)\n        return output_path\n    \n    def process(self, input_path: str, output_dir: str) -> str:\n        \"\"\"Pipeline complet de pr√©traitement\"\"\"\n        base_name = Path(input_path).stem\n        ffmpeg_path = str(Path(output_dir) / f\"{base_name}_ffmpeg.wav\")\n        denoise_path = str(Path(output_dir) / f\"{base_name}_clean.wav\")\n        \n        self.ffmpeg_enhance(input_path, ffmpeg_path)\n        self.reduce_noise(ffmpeg_path, denoise_path)\n        \n        # Nettoyage fichier interm√©diaire\n        if Path(ffmpeg_path).exists():\n            Path(ffmpeg_path).unlink()\n        \n        return denoise_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.614365Z","iopub.execute_input":"2025-09-29T08:25:53.614577Z","iopub.status.idle":"2025-09-29T08:25:53.635381Z","shell.execute_reply.started":"2025-09-29T08:25:53.614563Z","shell.execute_reply":"2025-09-29T08:25:53.634621Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"def prepare_audio_file(audio_path: str) -> Dict:\n    \"\"\"Pr√©pare et valide le fichier audio pour la transcription\"\"\"\n    file_info = {\n        \"path\": audio_path,\n        \"exists\": os.path.exists(audio_path),\n        \"size_mb\": 0,\n        \"duration_seconds\": 0,\n        \"format\": audio_path.split('.')[-1],\n        \"sample_rate\": 0,\n        \"channels\": 0\n    }\n    \n    if file_info[\"exists\"]:\n        file_info[\"size_mb\"] = os.path.getsize(audio_path) / (1024 * 1024)\n        \n        try:\n            y, sr = librosa.load(audio_path, sr=None, duration=10)\n            file_info[\"sample_rate\"] = sr\n            duration = librosa.get_duration(path=audio_path)\n            file_info[\"duration_seconds\"] = duration\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Erreur lecture audio: {e}\")\n    \n    return file_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.635978Z","iopub.execute_input":"2025-09-29T08:25:53.636203Z","iopub.status.idle":"2025-09-29T08:25:53.656921Z","shell.execute_reply.started":"2025-09-29T08:25:53.636189Z","shell.execute_reply":"2025-09-29T08:25:53.656420Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"# **Transcription Audio**\n**Service de transcription avec audio nettoy√©**","metadata":{}},{"cell_type":"code","source":"class TranscriptionService:\n    \"\"\"Service de transcription avec configurations SIIS optimis√©es\"\"\"\n    \n    def __init__(self, cfg: Config):\n        self.cfg = cfg\n        self.model = None\n    \n    def load_model(self):\n        \"\"\"Charge le mod√®le Whisper avec gestion m√©moire\"\"\"\n        if self.model is None:\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            self.model = WhisperModel(\n                self.cfg.whisper_model,\n                device=self.cfg.device,\n                compute_type=self.cfg.compute_type,\n                num_workers=self.cfg.num_workers,  # 4 au lieu de 1\n                cpu_threads=4 if self.cfg.device == \"cpu\" else 0\n            )\n        return self.model\n    \n    def unload_model(self):\n        \"\"\"Lib√®re le mod√®le de la m√©moire\"\"\"\n        if self.model is not None:\n            del self.model\n            self.model = None\n            torch.cuda.empty_cache()\n            gc.collect()\n    \n    def transcribe_chunk(self, audio_path: str) -> Tuple[List, Dict]:\n        \"\"\"Transcrit un chunk audio\"\"\"\n        model = self.load_model()\n        \n        segments, info = model.transcribe(\n            audio_path,\n            language=\"fr\",\n            beam_size=self.cfg.beam_size,\n            best_of=self.cfg.best_of,\n            patience=self.cfg.patience,\n            temperature=self.cfg.temperature,\n            compression_ratio_threshold=self.cfg.compression_ratio_threshold,\n            log_prob_threshold=self.cfg.log_prob_threshold,\n            no_speech_threshold=self.cfg.no_speech_threshold,\n            condition_on_previous_text=self.cfg.condition_on_previous_text,  # TRUE!\n            initial_prompt=self.cfg.initial_prompt,\n            word_timestamps=True,\n            suppress_tokens=self.cfg.suppress_tokens,\n            suppress_blank=self.cfg.suppress_blank,\n            max_initial_timestamp=self.cfg.max_initial_timestamp,\n            vad_filter=self.cfg.use_vad,\n            vad_parameters={\n                \"threshold\": self.cfg.vad_threshold,\n                \"min_speech_duration_ms\": self.cfg.vad_min_speech_duration_ms,\n                \"max_speech_duration_s\": self.cfg.vad_max_speech_duration_s,\n                \"min_silence_duration_ms\": self.cfg.vad_min_silence_duration_ms,\n                \"speech_pad_ms\": self.cfg.vad_speech_pad_ms,\n            } if self.cfg.use_vad else None\n        )\n        \n        return list(segments), info\n    \n    def transcribe_long_audio(self, audio_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Transcrit un audio long avec chunking optimis√© SIIS\n        Chunks de 900s au lieu de 180-300s pour moins de d√©rive\n        \"\"\"\n        # Obtenir la dur√©e totale\n        y, sr = librosa.load(audio_path, sr=self.cfg.sample_rate, duration=1)\n        info = sf.info(audio_path)\n        total_duration = info.duration\n        \n        # Calcul des chunks\n        chunk_length = self.cfg.chunk_length_s\n        chunk_overlap = self.cfg.chunk_overlap_s\n        num_chunks = max(1, ceil(total_duration / chunk_length))\n        \n        print(f\"üìä Audio: {total_duration:.1f}s | {num_chunks} chunks de {chunk_length}s\")\n        \n        all_segments = []\n        all_text = []\n        \n        for i in range(num_chunks):\n            start_time = max(0, i * chunk_length - (chunk_overlap if i > 0 else 0))\n            duration = min(chunk_length + chunk_overlap, total_duration - start_time)\n            \n            # Extraire le chunk avec ffmpeg\n            chunk_path = str(TEMP_DIR / f\"chunk_{i:04d}.wav\")\n            cmd = [\n                \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n                \"-ss\", str(start_time),\n                \"-t\", str(duration),\n                \"-i\", audio_path,\n                \"-ac\", \"1\",\n                \"-ar\", str(self.cfg.sample_rate),\n                chunk_path\n            ]\n            subprocess.run(cmd, check=True)\n            \n            # Transcrire le chunk\n            print(f\"  Chunk {i+1}/{num_chunks}: {start_time:.1f}s - {start_time+duration:.1f}s\")\n            segments, chunk_info = self.transcribe_chunk(chunk_path)\n            \n            # Ajuster les timestamps\n            for seg in segments:\n                # Cr√©er un nouveau dictionnaire pour chaque segment\n                segment_dict = {\n                    \"start\": seg.start + start_time,\n                    \"end\": seg.end + start_time,\n                    \"text\": seg.text.strip(),\n                }\n                \n                # Ajouter les mots avec timestamps ajust√©s si disponibles\n                if hasattr(seg, 'words') and seg.words:\n                    segment_dict[\"words\"] = [\n                        {\n                            \"start\": w.start + start_time,\n                            \"end\": w.end + start_time,\n                            \"word\": w.word,\n                            \"probability\": getattr(w, 'probability', 0.0)\n                        }\n                        for w in seg.words\n                    ]\n                \n                # Ajouter d'autres m√©tadonn√©es si disponibles\n                if hasattr(seg, 'no_speech_prob'):\n                    segment_dict[\"no_speech_prob\"] = seg.no_speech_prob\n                if hasattr(seg, 'avg_logprob'):\n                    segment_dict[\"avg_logprob\"] = seg.avg_logprob\n                if hasattr(seg, 'compression_ratio'):\n                    segment_dict[\"compression_ratio\"] = seg.compression_ratio\n                \n                all_segments.append(segment_dict)\n                all_text.append(seg.text.strip())\n            \n            # Nettoyer le chunk temporaire\n            Path(chunk_path).unlink()\n        \n        # Assembler le r√©sultat\n        result = {\n            \"status\": \"success\",\n            \"duration\": total_duration,\n            \"language\": \"fr\",\n            \"segments\": all_segments,\n            \"text\": \" \".join(all_text),\n            \"metadata\": {\n                \"model\": self.cfg.whisper_model,\n                \"chunks\": num_chunks,\n                \"chunk_length\": chunk_length,\n                \"condition_on_previous\": self.cfg.condition_on_previous_text\n            }\n        }\n        \n        return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.672177Z","iopub.execute_input":"2025-09-29T08:25:53.672365Z","iopub.status.idle":"2025-09-29T08:25:53.687459Z","shell.execute_reply.started":"2025-09-29T08:25:53.672351Z","shell.execute_reply":"2025-09-29T08:25:53.686920Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"# Exemple d'utilisation\n#result = transcription_service.transcribe_audio(audio_file)\n#print(f\"Transcription: {result['transcription'][:500]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.688667Z","iopub.execute_input":"2025-09-29T08:25:53.688850Z","iopub.status.idle":"2025-09-29T08:25:53.709202Z","shell.execute_reply.started":"2025-09-29T08:25:53.688836Z","shell.execute_reply":"2025-09-29T08:25:53.708509Z"}},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":"# **Diarization**","metadata":{}},{"cell_type":"code","source":"def diarize(transcription_data: Dict, audio_path: str, hf_token: str) -> Dict:\n    \"\"\"\n    Diarisation avec pyannote (ou fallback whisperx)\n    \"\"\"\n    try:\n        from pyannote.audio import Pipeline\n        \n        print(\"üéôÔ∏è Diarisation avec pyannote...\")\n        pipeline = Pipeline.from_pretrained(\n            \"pyannote/speaker-diarization-3.1\",\n            use_auth_token=hf_token\n        )\n        \n        diarization = pipeline(audio_path)\n        \n        # Mapper les segments aux locuteurs\n        segments_with_speakers = []\n        for seg in transcription_data.get(\"segments\", []):\n            start, end = seg[\"start\"], seg[\"end\"]\n            \n            # Trouver le locuteur majoritaire pour ce segment\n            speaker_times = {}\n            for turn, _, speaker in diarization.itertracks(yield_label=True):\n                overlap_start = max(start, turn.start)\n                overlap_end = min(end, turn.end)\n                if overlap_start < overlap_end:\n                    overlap_duration = overlap_end - overlap_start\n                    speaker_times[speaker] = speaker_times.get(speaker, 0) + overlap_duration\n            \n            # Assigner le locuteur avec le plus de temps de parole\n            if speaker_times:\n                main_speaker = max(speaker_times, key=speaker_times.get)\n                seg[\"speaker\"] = main_speaker\n            else:\n                seg[\"speaker\"] = \"Unknown\"\n            \n            segments_with_speakers.append(seg)\n        \n        transcription_data[\"segments_diarized\"] = segments_with_speakers\n        transcription_data[\"diarization_method\"] = \"pyannote\"\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Diarisation pyannote √©chou√©e: {e}\")\n        \n        # Fallback sur whisperx si disponible\n        try:\n            import whisperx\n            print(\"üîÑ Fallback sur whisperx...\")\n            \n            # Aligner avec whisperx\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            align_model, metadata = whisperx.load_align_model(\n                language_code=\"fr\",\n                device=device\n            )\n            \n            result_aligned = whisperx.align(\n                transcription_data[\"segments\"],\n                align_model,\n                metadata,\n                audio_path,\n                device\n            )\n            \n            # Diarisation\n            diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token)\n            diarize_segments = diarize_model(audio_path)\n            result_diarized = whisperx.assign_word_speakers(diarize_segments, result_aligned)\n            \n            transcription_data[\"segments_diarized\"] = result_diarized[\"segments\"]\n            transcription_data[\"diarization_method\"] = \"whisperx\"\n            \n        except Exception as e2:\n            print(f\"‚ö†Ô∏è Diarisation whisperx √©chou√©e: {e2}\")\n            transcription_data[\"diarization_method\"] = \"none\"\n    \n    return transcription_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.709900Z","iopub.execute_input":"2025-09-29T08:25:53.710127Z","iopub.status.idle":"2025-09-29T08:25:53.727777Z","shell.execute_reply.started":"2025-09-29T08:25:53.710108Z","shell.execute_reply":"2025-09-29T08:25:53.727102Z"}},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":"# **Post-traitement du texte**","metadata":{}},{"cell_type":"code","source":"def normalize_numbers_and_units(text: str) -> str:\n    \"\"\"Normalise les nombres et unit√©s mon√©taires\"\"\"\n    import re\n    \n    # Normaliser les millions\n    text = re.sub(r'(\\d+)\\s*,\\s*(\\d+)\\s*millions?', r'\\1.\\2 millions', text)\n    text = re.sub(r'(\\d+)\\s*virgule\\s*(\\d+)\\s*millions?', r'\\1.\\2 millions', text)\n    \n    # Ajouter Ariary si manquant apr√®s les montants\n    text = re.sub(r'(\\d+(?:\\.\\d+)?)\\s*millions?\\s*(?!d\\'?[Aa]riary)', r'\\1 millions d\\'Ariary', text)\n    \n    # Normaliser les pourcentages\n    text = re.sub(r'(\\d+)\\s*pour\\s*cent', r'\\1%', text)\n    \n    return text\n\ndef deduplicate_sentences(text: str) -> str:\n    \"\"\"Supprime les r√©p√©titions de phrases\"\"\"\n    import re\n    \n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    seen = set()\n    unique_sentences = []\n    \n    for sent in sentences:\n        sent_lower = sent.lower().strip()\n        if sent_lower and sent_lower not in seen:\n            seen.add(sent_lower)\n            unique_sentences.append(sent)\n    \n    return ' '.join(unique_sentences)\n\ndef postprocess_text(text: str) -> str:\n    # Nettoyer les espaces multiples\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    text = normalize_numbers_and_units(text)\n    text = deduplicate_sentences(text)\n    \n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.728749Z","iopub.execute_input":"2025-09-29T08:25:53.728998Z","iopub.status.idle":"2025-09-29T08:25:53.754508Z","shell.execute_reply.started":"2025-09-29T08:25:53.728978Z","shell.execute_reply":"2025-09-29T08:25:53.753986Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"# **Nettoyage LLM**","metadata":{}},{"cell_type":"code","source":"class LLMCleaner:\n    \"\"\"Nettoyage du texte avec LLM (Groq ou OpenAI)\"\"\"\n    \n    def __init__(self, cfg: Config):\n        self.cfg = cfg\n        self.client = None\n        \n        if cfg.use_groq and GROQ_API_KEY:\n            try:\n                from groq import Groq\n                self.client = Groq(api_key=GROQ_API_KEY)\n                self.provider = \"groq\"\n                print(\"‚úÖ Utilisation de Groq pour le nettoyage LLM\")\n            except ImportError:\n                print(\"‚ö†Ô∏è Package groq non install√©, installation...\")\n                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"groq\"])\n                from groq import Groq\n                self.client = Groq(api_key=GROQ_API_KEY)\n                self.provider = \"groq\"\n        elif OPENAI_API_KEY:\n            from openai import OpenAI\n            self.client = OpenAI(api_key=OPENAI_API_KEY)\n            self.provider = \"openai\"\n            print(\"‚úÖ Utilisation d'OpenAI pour le nettoyage LLM\")\n        else:\n            print(\"‚ö†Ô∏è Aucune cl√© API LLM disponible\")\n    \n    def create_chunks(self, text: str) -> List[str]:\n        \"\"\"D√©coupe le texte en chunks pour traitement LLM\"\"\"\n        if not text:\n            return []\n        \n        step = max(1, self.cfg.chunk_size_chars - self.cfg.chunk_overlap_chars)\n        chunks = []\n        for i in range(0, len(text), step):\n            chunks.append(text[i:i + self.cfg.chunk_size_chars])\n        \n        return chunks\n    \n    def clean_text(self, text: str) -> Tuple[str, float]:\n        \"\"\"Nettoie le texte avec le LLM\"\"\"\n        if not self.client or not text:\n            return text, 0.0\n        \n        chunks = self.create_chunks(text)\n        cleaned_chunks = []\n        total_delta = 0\n        \n        system_prompt = \"\"\"Tu es un assistant de correction de transcription.\n            Tu corriges UNIQUEMENT : orthographe, grammaire, ponctuation, noms propres malgaches.\n            R√àGLES STRICTES :\n            1. NE JAMAIS ajouter d'information non pr√©sente\n            2. NE PAS changer le sens des phrases\n            3. Conserver tous les chiffres et montants exacts\n            Contexte: R√©union du conseil d'administration √† Madagascar.\n            Termes valides: Fihariana, SON'INVEST, UNIMA, AQUALMA, Ariary.\"\"\"\n        \n        for i, chunk in enumerate(chunks, 1):\n            print(f\"  Nettoyage chunk {i}/{len(chunks)}...\")\n            \n            try:\n                if self.provider == \"groq\":\n                    response = self.client.chat.completions.create(\n                        model=self.cfg.groq_model,\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_prompt},\n                            {\"role\": \"user\", \"content\": f\"Corrige ce texte:\\n\\n{chunk}\"}\n                        ],\n                        temperature=0.2,\n                        max_tokens=1500\n                    )\n                    cleaned = response.choices[0].message.content.strip()\n                else:  # OpenAI\n                    response = self.client.chat.completions.create(\n                        model=self.cfg.openai_model,\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_prompt},\n                            {\"role\": \"user\", \"content\": chunk}\n                        ],\n                        temperature=0.2,\n                        max_tokens=1400\n                    )\n                    cleaned = response.choices[0].message.content.strip()\n                \n                cleaned_chunks.append(cleaned)\n                total_delta += abs(len(cleaned) - len(chunk))\n                \n            except Exception as e:\n                print(f\"    ‚ö†Ô∏è Erreur LLM chunk {i}: {e}\")\n                cleaned_chunks.append(chunk)  # Garder l'original si erreur\n        \n        # Assembler et calculer le taux de correction\n        merged_text = ' '.join(cleaned_chunks)\n        correction_rate = total_delta / max(len(text), 1)\n        \n        # V√©rifier le taux de correction\n        if correction_rate > self.cfg.max_correction_rate:\n            print(f\"‚ö†Ô∏è Taux de correction {correction_rate:.1%} > seuil {self.cfg.max_correction_rate:.0%}\")\n            print(\"   ‚Üí Conservation du texte post-trait√© sans LLM\")\n            return text, correction_rate\n        \n        return merged_text, correction_rate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.755944Z","iopub.execute_input":"2025-09-29T08:25:53.756180Z","iopub.status.idle":"2025-09-29T08:25:53.780410Z","shell.execute_reply.started":"2025-09-29T08:25:53.756165Z","shell.execute_reply":"2025-09-29T08:25:53.779818Z"}},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":"# **Fallback AssemblyAI (si √©chec Whisper)**","metadata":{}},{"cell_type":"code","source":"class AssemblyAIFallback:\n    \"\"\"Service de fallback avec AssemblyAI\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        \n    def transcribe_with_assemblyai(self, audio_path: str) -> Dict:\n        \"\"\"\n        Transcription de secours via AssemblyAI\n        \n        Args:\n            audio_path: Chemin du fichier audio\n            \n        Returns:\n            Dict avec la transcription\n        \"\"\"\n        if not self.api_key:\n            return {\n                \"status\": \"error\",\n                \"error\": \"Cl√© API AssemblyAI non configur√©e\"\n            }\n        \n        try:\n            import assemblyai as aai\n            \n            print(\"üîÑ Utilisation du fallback AssemblyAI...\")\n            \n            aai.settings.api_key = self.api_key\n            transcriber = aai.Transcriber()\n            \n            # Upload et transcription\n            config_lang = aai.TranscriptionConfig(\n                language_code=\"fr\",\n                punctuate=True,\n                format_text=True,\n                disfluencies=True,\n                speaker_labels=True\n            )\n            transcript = transcriber.transcribe(audio_path, config=config_lang)\n            \n            if transcript.status == aai.TranscriptStatus.error:\n                raise Exception(f\"Erreur AssemblyAI: {transcript.error}\")\n            \n            # Attente de la transcription\n            while transcript.status not in [aai.TranscriptStatus.completed, aai.TranscriptStatus.error]:\n                time.sleep(5)\n                transcript = transcriber.get_transcript(transcript.id)\n            \n            return {\n                \"status\": \"success\",\n                \"method\": \"assemblyai\",\n                \"transcription\": transcript.text,\n                \"confidence\": transcript.confidence if hasattr(transcript, 'confidence') else 0.85,\n                \"words\": transcript.words if hasattr(transcript, 'words') else []\n            }\n            \n        except Exception as e:\n            print(f\"‚ùå Erreur AssemblyAI: {str(e)}\")\n            return {\n                \"status\": \"error\",\n                \"error\": str(e),\n                \"method\": \"assemblyai\"\n            }\n\n# Service de fallback\nfallback_service = AssemblyAIFallback(config.assemblyai_key)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-09-29T08:25:53.781185Z","iopub.execute_input":"2025-09-29T08:25:53.781425Z","iopub.status.idle":"2025-09-29T08:25:53.802048Z","shell.execute_reply.started":"2025-09-29T08:25:53.781405Z","shell.execute_reply":"2025-09-29T08:25:53.801555Z"}},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":"1. Par d√©faut, la langue est auto. Pour ton cas, force fran√ßais :\n        config = aai.TranscriptionConfig(language_code=\"fr\")\n2. Diarisation (orateurs)\n        config = aai.TranscriptionConfig(speaker_labels=True)\n\nExemple :\n    config = aai.TranscriptionConfig(language_code=\"fr\", speaker_labels=True)\n    transcript = transcriber.transcribe(audio_path, config=config)\n\nAppel :\n    Si TranscriptionService.transcribe_audio renvoie status=\"error\" ou un real_time_factor >> 5 (trop lent) ou trop de segments sous ton confidence_threshold, alors :\n        > result = fallback_service.transcribe_with_assemblyai(audio_path)","metadata":{}},{"cell_type":"markdown","source":"# **Pipeline de transcription avec gestion automatique du fallback**","metadata":{}},{"cell_type":"code","source":"def transcribe_audio_pipeline(\n    audio_path: str,\n    cfg: Config,\n    save_json: bool = True\n) -> Dict[str, Any]:\n    \"\"\"\n    Pipeline complet optimis√© de transcription\n    \n    √âtapes:\n    1. Pr√©traitement audio (FFmpeg + d√©bruitage)\n    2. Transcription avec chunking long (900s)\n    3. Diarisation des locuteurs\n    4. Post-traitement (normalisation, d√©duplications)\n    5. Nettoyage LLM avec Groq\n    \n    Returns:\n        Dictionnaire avec toutes les versions de la transcription\n    \"\"\"\n    \n    try:\n        # Initialisation\n        cleanup_temp_files()\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        \n        # [1/5] Pr√©traitement\n        print(\"\\n[1/5] üîä Pr√©traitement audio...\")\n        preprocessor = AudioPreprocessor(cfg.sample_rate)\n        clean_audio_path = preprocessor.process(audio_path, str(TEMP_DIR))\n        \n        # Lib√©rer m√©moire\n        del preprocessor\n        gc.collect()\n        \n        # [2/5] Transcription\n        print(\"\\n[2/5] üìù Transcription avec Whisper...\")\n        service = TranscriptionService(cfg)\n        transcription_result = service.transcribe_long_audio(clean_audio_path)\n        \n        # Sauvegarder la transcription brute (02_transcription)\n        if save_json:\n            raw_output = {\n                \"timestamp\": timestamp,\n                \"status\": transcription_result[\"status\"],\n                \"duration\": transcription_result[\"duration\"],\n                \"text\": transcription_result[\"text\"],\n                \"segments\": transcription_result[\"segments\"],\n                \"metadata\": transcription_result[\"metadata\"]\n            }\n            raw_path = Path(OUTPUT_PATH) / f\"02_transcription_{timestamp}.json\"\n            with open(raw_path, 'w', encoding='utf-8') as f:\n                json.dump(raw_output, f, ensure_ascii=False, indent=2)\n            print(f\"  üíæ Sauvegard√©: {raw_path}\")\n        \n        # Lib√©rer le mod√®le\n        service.unload_model()\n        gc.collect()\n        \n        # [3/5] Diarisation\n        print(\"\\n[3/5] üéôÔ∏è Diarisation des locuteurs...\")\n        if HUGGINGFACE_TOKEN:\n            transcription_result = diarize(\n                transcription_result,\n                clean_audio_path,\n                HUGGINGFACE_TOKEN\n            )\n            \n            # Sauvegarder avec diarisation (03_diarization)\n            if save_json and transcription_result.get(\"segments_diarized\"):\n                diar_output = {\n                    \"timestamp\": timestamp,\n                    \"status\": \"success\",\n                    \"duration\": transcription_result[\"duration\"],\n                    \"diarization_method\": transcription_result.get(\"diarization_method\", \"none\"),\n                    \"segments\": transcription_result.get(\"segments_diarized\", transcription_result[\"segments\"]),\n                    \"text\": transcription_result[\"text\"]\n                }\n                diar_path = Path(OUTPUT_PATH) / f\"03_diarization_{timestamp}.json\"\n                with open(diar_path, 'w', encoding='utf-8') as f:\n                    json.dump(diar_output, f, ensure_ascii=False, indent=2)\n                print(f\"  üíæ Sauvegard√©: {diar_path}\")\n        else:\n            print(\"  ‚ö†Ô∏è Token HuggingFace manquant, diarisation ignor√©e\")\n        \n        # [4/5] Post-traitement\n        print(\"\\n[4/5] üîß Post-traitement du texte...\")\n        text_postprocessed = postprocess_text(transcription_result[\"text\"])\n        print(f\"  R√©duction: {len(transcription_result['text'])} ‚Üí {len(text_postprocessed)} caract√®res\")\n        \n        # [5/5] Nettoyage LLM\n        print(\"\\n[5/5] ‚ú® Nettoyage LLM...\")\n        text_final = text_postprocessed\n        correction_rate = 0.0\n        \n        if cfg.enable_llm:\n            cleaner = LLMCleaner(cfg)\n            text_final, correction_rate = cleaner.clean_text(text_postprocessed)\n            print(f\"  Taux de correction: {correction_rate:.1%}\")\n        else:\n            print(\"  ‚ÑπÔ∏è LLM d√©sactiv√©\")\n        \n        # R√©sultat final\n        final_result = {\n            \"timestamp\": timestamp,\n            \"status\": \"success\",\n            \"duration\": transcription_result[\"duration\"],\n            \"model\": cfg.whisper_model,\n            \"transcription_raw\": transcription_result[\"text\"],\n            \"transcription_postprocessed\": text_postprocessed,\n            \"transcription_final\": text_final,\n            \"llm_correction_rate\": correction_rate,\n            \"llm_provider\": getattr(cleaner, 'provider', 'none') if cfg.enable_llm else 'none',\n            \"segments\": transcription_result.get(\"segments_diarized\", transcription_result[\"segments\"]),\n            \"metadata\": {\n                \"pipeline_version\": \"2.0-optimized\",\n                \"chunk_length\": cfg.chunk_length_s,\n                \"condition_on_previous\": cfg.condition_on_previous_text,\n                \"vad_enabled\": cfg.use_vad,\n                \"diarization\": transcription_result.get(\"diarization_method\", \"none\")\n            }\n        }\n        \n        # Sauvegarder le r√©sultat final\n        if save_json:\n            final_path = Path(OUTPUT_PATH) / f\"transcription_complete_{timestamp}.json\"\n            with open(final_path, 'w', encoding='utf-8') as f:\n                json.dump(final_result, f, ensure_ascii=False, indent=2)\n            print(f\"\\n‚úÖ Pipeline termin√©! R√©sultats sauvegard√©s:\")\n            print(f\"   - {raw_path.name} (transcription brute)\")\n            #if HUGGINGFACE_TOKEN:\n                #print(f\"   - {diar_path.name} (avec diarisation)\")\n            print(f\"   - {final_path.name} (version finale)\")\n        \n        return final_result\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Erreur dans le pipeline: {e}\")\n        import traceback\n        traceback.print_exc()\n        return {\"status\": \"error\", \"error\": str(e)}\n    \n    finally:\n        cleanup_temp_files()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.802913Z","iopub.execute_input":"2025-09-29T08:25:53.803137Z","iopub.status.idle":"2025-09-29T08:25:53.834983Z","shell.execute_reply.started":"2025-09-29T08:25:53.803115Z","shell.execute_reply":"2025-09-29T08:25:53.834405Z"}},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"# **EX√âCUTION PRINCIPALE**","metadata":{}},{"cell_type":"code","source":"# Test avec votre fichier audio\n#audio_file = f\"{UPLOAD_PATH}atelier.mp3\"\n#audio_file = f\"{UPLOAD_PATH}test_1h.wav\"\naudio_file = f\"{UPLOAD_PATH}test_30mn.mp3\"\n#audio_info = prepare_audio_file(audio_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.835737Z","iopub.execute_input":"2025-09-29T08:25:53.835976Z","iopub.status.idle":"2025-09-29T08:25:53.852880Z","shell.execute_reply.started":"2025-09-29T08:25:53.835955Z","shell.execute_reply":"2025-09-29T08:25:53.852407Z"}},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":"#**Analyse des R√©sultats de Debug**\n\nLes fichiers JSON de debug sont sauvegard√©s dans `/kaggle/working/debug_json/` avec le format:\n- `02_transcription_[timestamp].json` : R√©sultat brut de Whisper\n- `03_diarization_[timestamp].json` : Apr√®s identification des locuteurs\n- `04_postprocessing_[timestamp].json` : Apr√®s normalisation et d√©duplication\n- `05_llm_cleaning_[timestamp].json` : Version finale nettoy√©e par LLM\n\nChaque fichier contient :\n- Un r√©sum√© (`summary`) avec aper√ßu du texte et statistiques\n- Les m√©tadonn√©es de l'√©tape (`status`, `timestamp`)\n- Les donn√©es compl√®tes peuvent √™tre consult√©es dans le fichier principal","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Rechercher le fichier audio\n    audio_files = list(Path(UPLOAD_PATH).glob(\"*.mp3\")) + \\\n                  list(Path(UPLOAD_PATH).glob(\"*.wav\")) + \\\n                  list(Path(UPLOAD_PATH).glob(\"*.m4a\"))\n    \n    if audio_files:\n        audio_file = audio_files[0]\n        print(f\"\\nüìÇ Fichier trouv√©: {audio_file.name}\")\n        print(\"=\" * 60)\n        \n        # Lancer le pipeline\n        result = transcribe_audio_pipeline(\n            str(audio_file),\n            config,\n            save_json=True\n        )\n        \n        # Afficher un r√©sum√©\n        if result[\"status\"] == \"success\":\n            print(\"\\n\" + \"=\" * 60)\n            print(\"üìä R√âSUM√â DE LA TRANSCRIPTION\")\n            print(\"=\" * 60)\n            print(f\"Dur√©e: {result['duration']:.1f} secondes\")\n            print(f\"Mod√®le: {result['model']}\")\n            print(f\"Provider LLM: {result.get('llm_provider', 'none')}\")\n            print(f\"Taux correction LLM: {result.get('llm_correction_rate', 0):.1%}\")\n            print(f\"\\nüìù Extrait (500 premiers caract√®res):\")\n            print(\"-\" * 40)\n            print(result['transcription_final'][:500] + \"...\")\n    else:\n        print(\"‚ö†Ô∏è Aucun fichier audio trouv√© dans\", UPLOAD_PATH)\n        print(\"   Formats support√©s: .mp3, .wav, .m4a\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T08:25:53.854330Z","iopub.execute_input":"2025-09-29T08:25:53.854516Z","iopub.status.idle":"2025-09-29T08:29:17.056378Z","shell.execute_reply.started":"2025-09-29T08:25:53.854502Z","shell.execute_reply":"2025-09-29T08:29:17.055463Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ Fichier trouv√©: test_30mn.mp3\n============================================================\n\n[1/5] üîä Pr√©traitement audio...\n\n[2/5] üìù Transcription avec Whisper...\nüìä Audio: 1928.0s | 3 chunks de 900s\n  Chunk 1/3: 0.0s - 930.0s\n  Chunk 2/3: 870.0s - 1800.0s\n  Chunk 3/3: 1770.0s - 1928.0s\n  üíæ Sauvegard√©: /kaggle/working/02_transcription_20250929_082553.json\n\n[3/5] üéôÔ∏è Diarisation des locuteurs...\n‚ö†Ô∏è Diarisation pyannote √©chou√©e: No module named 'pyannote'\n‚ö†Ô∏è Diarisation whisperx √©chou√©e: No module named 'whisperx'\n\n[4/5] üîß Post-traitement du texte...\n  R√©duction: 14897 ‚Üí 14543 caract√®res\n\n[5/5] ‚ú® Nettoyage LLM...\n‚úÖ Utilisation de Groq pour le nettoyage LLM\n  Nettoyage chunk 1/19...\n  Nettoyage chunk 2/19...\n  Nettoyage chunk 3/19...\n  Nettoyage chunk 4/19...\n  Nettoyage chunk 5/19...\n  Nettoyage chunk 6/19...\n  Nettoyage chunk 7/19...\n  Nettoyage chunk 8/19...\n  Nettoyage chunk 9/19...\n  Nettoyage chunk 10/19...\n  Nettoyage chunk 11/19...\n  Nettoyage chunk 12/19...\n  Nettoyage chunk 13/19...\n  Nettoyage chunk 14/19...\n  Nettoyage chunk 15/19...\n  Nettoyage chunk 16/19...\n  Nettoyage chunk 17/19...\n  Nettoyage chunk 18/19...\n  Nettoyage chunk 19/19...\n‚ö†Ô∏è Taux de correction 44.4% > seuil 18%\n   ‚Üí Conservation du texte post-trait√© sans LLM\n  Taux de correction: 44.4%\n\n‚úÖ Pipeline termin√©! R√©sultats sauvegard√©s:\n   - 02_transcription_20250929_082553.json (transcription brute)\n   - transcription_complete_20250929_082553.json (version finale)\n\n============================================================\nüìä R√âSUM√â DE LA TRANSCRIPTION\n============================================================\nDur√©e: 1928.0 secondes\nMod√®le: large-v3\nProvider LLM: groq\nTaux correction LLM: 44.4%\n\nüìù Extrait (500 premiers caract√®res):\n----------------------------------------\nPar exemple, tous les √©tudiants, les professeurs, tous les membres de la communaut√©, nous avons le premier ordre d'ordre de la revue √† NIPARCOURT et SINOTION, au printemps 12 ao√ªt 2015. C'est la situation, une situation o√π on se rend compte qu'il y a une situation qui est en train de se rendre compte, donc on peut prendre la parole √† l'interpr√®te, pour qu'il puisse vous expliquer ce qui se passe. Pour ce vid√©o que je vais vous pr√©senter, c'est dans le profil d'estimation pour 31-12, le contexte ...\n","output_type":"stream"}],"execution_count":77}]}