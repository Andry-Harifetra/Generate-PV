{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12de406",
   "metadata": {
    "papermill": {
     "duration": 0.00807,
     "end_time": "2025-10-28T12:26:27.334381",
     "exception": false,
     "start_time": "2025-10-28T12:26:27.326311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transcription CA\n",
    "\n",
    "Ce notebook applique le **pipeline complet** :\n",
    "- **PrÃ©traitement** audio (FFmpeg + noisereduce)\n",
    "- **Transcription** faster-whisper (rÃ©glages anti-hallucinations)\n",
    "- **Chunks longs** pour une meilleure cohÃ©rence (3â€“5 min)\n",
    "- **Diarisation** (pyannote â†’ fallback whisperx)\n",
    "- **Post-traitement** (dÃ©dup + normalisation chiffres/unitÃ©s)\n",
    "- **Nettoyage LLM** par morceaux (1000 caractÃ¨res) avec borne de correction\n",
    "- **Sauvegarde JSON** des sorties (raw, diarized, cleaned, llm_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e0ddc",
   "metadata": {
    "papermill": {
     "duration": 0.007518,
     "end_time": "2025-10-28T12:26:27.348842",
     "exception": false,
     "start_time": "2025-10-28T12:26:27.341324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Installation des packages nÃ©cessaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5020c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:27.363730Z",
     "iopub.status.busy": "2025-10-28T12:26:27.363521Z",
     "iopub.status.idle": "2025-10-28T12:26:27.367904Z",
     "shell.execute_reply": "2025-10-28T12:26:27.367262Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013245,
     "end_time": "2025-10-28T12:26:27.369117",
     "exception": false,
     "start_time": "2025-10-28T12:26:27.355872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Installation silencieuse des dÃ©pendances avec gestion des conflits\n",
    "\n",
    "# # 1. Mise Ã  jour pip pour Ã©viter les problÃ¨mes\n",
    "# #!pip install --upgrade pip -q\n",
    "\n",
    "# # 2. Installation FFmpeg (systÃ¨me)\n",
    "# !apt-get update -qq\n",
    "# !apt-get install -qq ffmpeg sox\n",
    "\n",
    "# # 3. Nettoyage et verrouillage de la stack NumPy/Numba/Scipy\n",
    "# #!pip uninstall -y numpy numba >/dev/null 2>&1 || true\n",
    "# !pip install -q numpy==1.26.4 scipy==1.11.4\n",
    "# !pip install -q numba==0.58.1\n",
    "# !pip install -q torch==2.1.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# # 4. Installation des packages de transcription\n",
    "# #!pip install -q openai-whisper==20231117\n",
    "# !pip install -q faster-whisper==1.0.3\n",
    "\n",
    "# # 5. Packages de dÃ©bruitage audio\n",
    "# !pip install -q librosa==0.10.1\n",
    "# !pip install -q soundfile==0.12.1\n",
    "# !pip install -q noisereduce==3.0.0\n",
    "# !pip install -q pydub==0.25.1\n",
    "\n",
    "# # 6. Diarization\n",
    "# !pip install -q \"pyannote.audio>=3.1\"\n",
    "# !pip install -q whisperx\n",
    "\n",
    "# !pip install -q regex==2023.12.25 unidecode==1.3.8\n",
    "\n",
    "# # 7. Packages documents\n",
    "# !pip install -q python-docx==1.2.0\n",
    "# !pip install -q python-pptx==1.0.2\n",
    "\n",
    "# # 8. Packages LLM et NLP\n",
    "# !pip install -q openai==1.91.0\n",
    "# !pip install -q assemblyai==0.44.3\n",
    "# !pip install -q tiktoken==0.9.0\n",
    "\n",
    "# # 9. LangChain\n",
    "# #!pip install -q langchain==0.3.27 langchain-community==0.3.29 langchain-core==0.3.30\n",
    "\n",
    "# # 10. Packages utilitaires\n",
    "# !pip install -q pandas==2.1.4 matplotlib==3.8.2 seaborn==0.13.2\n",
    "\n",
    "# # 11. Installation FAISS pour le RAG\n",
    "# #!pip install -q faiss-cpu==1.7.4\n",
    "\n",
    "# print(\"âœ… Installation terminÃ©e!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b67cfc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:27.383014Z",
     "iopub.status.busy": "2025-10-28T12:26:27.382813Z",
     "iopub.status.idle": "2025-10-28T12:26:36.247300Z",
     "shell.execute_reply": "2025-10-28T12:26:36.246462Z"
    },
    "papermill": {
     "duration": 8.872936,
     "end_time": "2025-10-28T12:26:36.248693",
     "exception": false,
     "start_time": "2025-10-28T12:26:27.375757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.0/2.0 MB 38.6 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 51.5/51.5 kB 268.2 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 135.8/135.8 kB 289.6 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 34.4/34.4 MB 324.4 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 38.6/38.6 MB 225.3 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17.4/17.4 MB 281.4 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46.0/46.0 kB 264.9 MB/s eta 0:00:00\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 86.8/86.8 kB 290.4 MB/s eta 0:00:00\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Installation minimale des dÃ©pendances nÃ©cessaires sans perturber l'environnement Kaggle\n",
    "import importlib\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def ensure_packages(requirements):\n",
    "    missing = []\n",
    "    for module_name, package_spec in requirements:\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "        except Exception:\n",
    "            missing.append(package_spec)\n",
    "    if missing:\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '-q'] + missing\n",
    "        subprocess.check_call(cmd)\n",
    "\n",
    "core_requirements = [\n",
    "    ('faster_whisper', 'faster-whisper==1.0.3'),\n",
    "    ('librosa', 'librosa==0.10.1'),\n",
    "    ('soundfile', 'soundfile==0.12.1'),\n",
    "    ('noisereduce', 'noisereduce==3.0.0'),\n",
    "    # ('pydub', 'pydub==0.25.1'),\n",
    "    # ('docx', 'python-docx==1.2.0'),\n",
    "    # ('pptx', 'python-pptx==1.0.2'),\n",
    "    # ('openai', 'openai==1.91.0'),\n",
    "    #('assemblyai', 'assemblyai==0.44.3'),\n",
    "    ('assemblyai', 'assemblyai==0.45.4'),\n",
    "    # ('tiktoken', 'tiktoken==0.9.0'),\n",
    "    ('groq', 'groq==0.33.0'),\n",
    "]\n",
    "\n",
    "ensure_packages(core_requirements)\n",
    "\n",
    "# if os.environ.get('INSTALL_LANGCHAIN', '0') == '1':\n",
    "#     optional_requirements = [\n",
    "#         ('langchain', 'langchain==0.3.27'),\n",
    "#         ('langchain_community', 'langchain-community==0.3.29'),\n",
    "#         ('faiss', 'faiss-cpu==1.7.4'),\n",
    "#     ]\n",
    "#     try:\n",
    "#         ensure_packages(optional_requirements)\n",
    "#     except subprocess.CalledProcessError:\n",
    "#         pass\n",
    "\n",
    "if not shutil.which('ffmpeg'):\n",
    "    subprocess.check_call(['apt-get', 'update', '-qq'])\n",
    "    subprocess.check_call(['apt-get', 'install', '-qq', 'ffmpeg'])\n",
    "\n",
    "print('âœ… VÃ©rification des dÃ©pendances terminÃ©e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c9d2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:36.264688Z",
     "iopub.status.busy": "2025-10-28T12:26:36.263991Z",
     "iopub.status.idle": "2025-10-28T12:26:49.917502Z",
     "shell.execute_reply": "2025-10-28T12:26:49.916831Z"
    },
    "papermill": {
     "duration": 13.662313,
     "end_time": "2025-10-28T12:26:49.918603",
     "exception": false,
     "start_time": "2025-10-28T12:26:36.256290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” VÃ©rification des packages installÃ©s:\n",
      "--------------------------------------------------\n",
      "âœ… numpy                : 1.26.4\n",
      "âœ… scipy                : 1.15.3\n",
      "âœ… numba                : 0.60.0\n",
      "âŒ openai-whisper       : Non installÃ©\n",
      "âœ… faster-whisper       : 1.0.3\n",
      "âœ… librosa              : 0.11.0\n",
      "âœ… soundfile            : 0.13.1\n",
      "âœ… noisereduce          : N/A\n",
      "âœ… pydub                : N/A\n",
      "âŒ python-docx          : Non installÃ©\n",
      "âŒ python-pptx          : Non installÃ©\n",
      "âœ… openai               : 1.91.0\n",
      "âœ… langchain            : 0.3.26\n",
      "âŒ langchain-community  : Non installÃ©\n",
      "âŒ faiss-cpu            : Non installÃ©\n",
      "âœ… assemblyai           : 0.45.4\n",
      "âœ… tiktoken             : 0.9.0\n",
      "âœ… groq                 : 0.33.0\n",
      "âš ï¸ Certains packages nÃ©cessitent une attention. Consultez les messages ci-dessus.\n"
     ]
    }
   ],
   "source": [
    "# VÃ©rification que tout est installÃ© correctement\n",
    "import importlib\n",
    "\n",
    "packages_to_check = [\n",
    "    ('numpy', 'numpy'),\n",
    "    ('scipy', 'scipy'),\n",
    "    ('numba', 'numba'),\n",
    "    ('whisper', 'openai-whisper'),\n",
    "    ('faster_whisper', 'faster-whisper'),\n",
    "    ('librosa', 'librosa'),\n",
    "    ('soundfile', 'soundfile'),\n",
    "    ('noisereduce', 'noisereduce'),\n",
    "    ('pydub', 'pydub'),\n",
    "    ('docx', 'python-docx'),\n",
    "    ('pptx', 'python-pptx'),\n",
    "    ('openai', 'openai'),\n",
    "    ('langchain', 'langchain'),\n",
    "    ('langchain_community', 'langchain-community'),\n",
    "    ('faiss', 'faiss-cpu'),\n",
    "    ('assemblyai', 'assemblyai'),\n",
    "    ('tiktoken', 'tiktoken'),\n",
    "    ('groq', 'groq')\n",
    "]\n",
    "\n",
    "print(\"ğŸ” VÃ©rification des packages installÃ©s:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_ok = True\n",
    "for import_name, package_name in packages_to_check:\n",
    "    try:\n",
    "        module = importlib.import_module(import_name)\n",
    "        version = getattr(module, '__version__', 'N/A')\n",
    "        print(f\"âœ… {package_name:20} : {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {package_name:20} : Non installÃ©\")\n",
    "        all_ok = False\n",
    "    except Exception as exc:\n",
    "        print(f\"âš ï¸ {package_name:20} : Erreur lors de l'import ({type(exc).__name__}: {exc})\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"âœ¨ Tous les packages sont installÃ©s correctement!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Certains packages nÃ©cessitent une attention. Consultez les messages ci-dessus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e733a",
   "metadata": {
    "papermill": {
     "duration": 0.007535,
     "end_time": "2025-10-28T12:26:49.934287",
     "exception": false,
     "start_time": "2025-10-28T12:26:49.926752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Imports et configuration GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0679b707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:49.950484Z",
     "iopub.status.busy": "2025-10-28T12:26:49.950144Z",
     "iopub.status.idle": "2025-10-28T12:26:50.075745Z",
     "shell.execute_reply": "2025-10-28T12:26:50.075015Z"
    },
    "papermill": {
     "duration": 0.134857,
     "end_time": "2025-10-28T12:26:50.076793",
     "exception": false,
     "start_time": "2025-10-28T12:26:49.941936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ PyTorch: 2.6.0+cu124\n",
      "ğŸ® CUDA disponible: True\n",
      "   GPU: Tesla T4\n",
      "   MÃ©moire: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# Imports standards\n",
    "import os, sys, json, re, shutil, subprocess, tempfile\n",
    "from math import ceil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import gc  # Garbage collector\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Imports audio et dÃ©bruitage\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "# from scipy.signal import butter, filtfilt, medfilt\n",
    "# from pydub import AudioSegment\n",
    "from groq import Groq\n",
    "\n",
    "# Imports pour la transcription\n",
    "import assemblyai as aai\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# # Imports pour les documents\n",
    "# from docx import Document\n",
    "# from pptx import Presentation\n",
    "\n",
    "# # Imports pour le NLP et LLM\n",
    "# import openai\n",
    "# try:\n",
    "#     from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#     from langchain_community.vectorstores import FAISS\n",
    "#     from langchain_community.embeddings import OpenAIEmbeddings\n",
    "#     langchain_available = True\n",
    "# except ImportError:\n",
    "#     print(\"âš ï¸ LangChain non disponible\")\n",
    "#     langchain_available = False\n",
    "\n",
    "import torch\n",
    "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ğŸ® CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   MÃ©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5450c1",
   "metadata": {
    "papermill": {
     "duration": 0.006817,
     "end_time": "2025-10-28T12:26:50.091133",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.084316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Configuration des clÃ©s API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc54e34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.106137Z",
     "iopub.status.busy": "2025-10-28T12:26:50.105908Z",
     "iopub.status.idle": "2025-10-28T12:26:50.386506Z",
     "shell.execute_reply": "2025-10-28T12:26:50.385992Z"
    },
    "papermill": {
     "duration": 0.289518,
     "end_time": "2025-10-28T12:26:50.387591",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.098073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    OPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\n",
    "    ASSEMBLYAI_API_KEY = user_secrets.get_secret(\"ASSEMBLYAI_API_KEY\")\n",
    "    HUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "except:\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "    ASSEMBLYAI_API_KEY = os.environ.get(\"ASSEMBLYAI_API_KEY\", \"\")\n",
    "    HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", \"\")\n",
    "    GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cad90f",
   "metadata": {
    "papermill": {
     "duration": 0.006823,
     "end_time": "2025-10-28T12:26:50.401550",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.394727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Configuration des chemins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e792d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.416691Z",
     "iopub.status.busy": "2025-10-28T12:26:50.416477Z",
     "iopub.status.idle": "2025-10-28T12:26:50.419405Z",
     "shell.execute_reply": "2025-10-28T12:26:50.418919Z"
    },
    "papermill": {
     "duration": 0.011797,
     "end_time": "2025-10-28T12:26:50.420426",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.408629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "UPLOAD_PATH = \"/kaggle/input/meeting-audio/\" # Chemin des fichiers uploadÃ©s \n",
    "OUTPUT_PATH = \"/kaggle/working\" # Chemin de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e56277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.435215Z",
     "iopub.status.busy": "2025-10-28T12:26:50.435005Z",
     "iopub.status.idle": "2025-10-28T12:26:50.438326Z",
     "shell.execute_reply": "2025-10-28T12:26:50.437788Z"
    },
    "papermill": {
     "duration": 0.011879,
     "end_time": "2025-10-28T12:26:50.439348",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.427469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG_PATH = Path(OUTPUT_PATH) / \"debug\"\n",
    "DEBUG_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124b5eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.455844Z",
     "iopub.status.busy": "2025-10-28T12:26:50.455425Z",
     "iopub.status.idle": "2025-10-28T12:26:50.458941Z",
     "shell.execute_reply": "2025-10-28T12:26:50.458474Z"
    },
    "papermill": {
     "duration": 0.013283,
     "end_time": "2025-10-28T12:26:50.460024",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.446741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMP_DIR = Path(OUTPUT_PATH) / \"temp_chunks\"\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def cleanup_temp_files():\n",
    "    \"\"\"Nettoyer les fichiers temporaires\"\"\"\n",
    "    if TEMP_DIR.exists():\n",
    "        shutil.rmtree(TEMP_DIR)\n",
    "    TEMP_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46d610",
   "metadata": {
    "papermill": {
     "duration": 0.007217,
     "end_time": "2025-10-28T12:26:50.474343",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.467126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Utilitaires de commande systÃ¨me**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe40d410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.489563Z",
     "iopub.status.busy": "2025-10-28T12:26:50.489179Z",
     "iopub.status.idle": "2025-10-28T12:26:50.492979Z",
     "shell.execute_reply": "2025-10-28T12:26:50.492323Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.012637,
     "end_time": "2025-10-28T12:26:50.494101",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.481464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensure_dir(p): \n",
    "    Path(p).mkdir(parents=True, exist_ok=True) #VÃ©rification crÃ©ation de dossier\n",
    "    \n",
    "def run(cmd): # Lancement commande\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "    return p.returncode, out.decode(), err.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6533e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.509351Z",
     "iopub.status.busy": "2025-10-28T12:26:50.508783Z",
     "iopub.status.idle": "2025-10-28T12:26:50.512430Z",
     "shell.execute_reply": "2025-10-28T12:26:50.511902Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.012272,
     "end_time": "2025-10-28T12:26:50.513492",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.501220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        free, total = torch.cuda.mem_get_info()\n",
    "        print(f\"ğŸ“Š GPU: {free/1e9:.2f}GB libres / {total/1e9:.2f}GB total\")\n",
    "        if free < 4e9:  # Moins de 4GB libres\n",
    "            print(\"âš ï¸ MÃ©moire GPU faible, utilisation de 'base' recommandÃ©e\")\n",
    "            return \"medium\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a3ed3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.528686Z",
     "iopub.status.busy": "2025-10-28T12:26:50.528284Z",
     "iopub.status.idle": "2025-10-28T12:26:50.535179Z",
     "shell.execute_reply": "2025-10-28T12:26:50.534677Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015557,
     "end_time": "2025-10-28T12:26:50.536154",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.520597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_debug_json(data: Dict, step_name: str, timestamp: Optional[str] = None) -> str:\n",
    "    \"\"\"Sauvegarde JSON de debug pour chaque Ã©tape\"\"\"\n",
    "    if not config.save_intermediate_json:\n",
    "        return \"\"\n",
    "    \n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    filename = f\"{step_name}_{timestamp}.json\"\n",
    "    filepath = DEBUG_PATH / filename\n",
    "    \n",
    "    # CrÃ©er un rÃ©sumÃ© pour les donnÃ©es volumineuses\n",
    "    debug_data = {\n",
    "        \"step\": step_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"status\": data.get(\"status\", \"unknown\"),\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    if \"segments\" in data and isinstance(data[\"segments\"], list):\n",
    "        debug_data[\"summary\"][\"total_segments\"] = len(data[\"segments\"])\n",
    "        debug_data[\"summary\"][\"sample_segments\"] = data[\"segments\"][:3] if data[\"segments\"] else []\n",
    "        debug_data[\"segments_count\"] = len(data[\"segments\"])\n",
    "    \n",
    "    if \"transcription\" in data:\n",
    "        debug_data[\"summary\"][\"text_length\"] = len(data[\"transcription\"])\n",
    "        debug_data[\"summary\"][\"text_preview\"] = data[\"transcription\"][:500] + \"...\" if len(data[\"transcription\"]) > 500 else data[\"transcription\"]\n",
    "    \n",
    "    if \"transcription_postprocessed\" in data:\n",
    "        debug_data[\"summary\"][\"postprocessed_length\"] = len(data[\"transcription_postprocessed\"])\n",
    "        debug_data[\"summary\"][\"postprocessed_preview\"] = data[\"transcription_postprocessed\"][:500] + \"...\"\n",
    "    \n",
    "    if \"transcription_llm\" in data:\n",
    "        debug_data[\"summary\"][\"llm_length\"] = len(data[\"transcription_llm\"])\n",
    "        debug_data[\"summary\"][\"llm_preview\"] = data[\"transcription_llm\"][:500] + \"...\"\n",
    "        debug_data[\"llm_correction_rate\"] = data.get(\"llm_correction_rate\", 0)\n",
    "    \n",
    "    # Ajouter les mÃ©tadonnÃ©es complÃ¨tes\n",
    "    debug_data[\"full_data_keys\"] = list(data.keys())\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(debug_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ“ Debug JSON sauvÃ©: {filepath}\")\n",
    "    return str(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cb688",
   "metadata": {
    "papermill": {
     "duration": 0.00702,
     "end_time": "2025-10-28T12:26:50.550268",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.543248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Monitoring et debug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e984f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.564975Z",
     "iopub.status.busy": "2025-10-28T12:26:50.564522Z",
     "iopub.status.idle": "2025-10-28T12:26:50.568357Z",
     "shell.execute_reply": "2025-10-28T12:26:50.567822Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.012259,
     "end_time": "2025-10-28T12:26:50.569397",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.557138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_memory_usage(step_name: str = \"\"):\n",
    "    \"\"\"Afficher l'utilisation mÃ©moire\"\"\"\n",
    "    prefix = f\"[{step_name}] \" if step_name else \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"{prefix}GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB / {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    print(f\"{prefix}RAM Usage: {process.memory_info().rss / 1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837698e2",
   "metadata": {
    "papermill": {
     "duration": 0.006761,
     "end_time": "2025-10-28T12:26:50.583181",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.576420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Configuration du pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc809eff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.639859Z",
     "iopub.status.busy": "2025-10-28T12:26:50.639261Z",
     "iopub.status.idle": "2025-10-28T12:26:50.644272Z",
     "shell.execute_reply": "2025-10-28T12:26:50.643703Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013761,
     "end_time": "2025-10-28T12:26:50.645280",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.631519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimal_model_size() -> str:\n",
    "    \"\"\"\n",
    "    DÃ©termine automatiquement la taille du modÃ¨le Whisper selon les ressources.\n",
    "    AdaptÃ© du projet SIIS pour une meilleure gestion mÃ©moire.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            total_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "        except:\n",
    "            total_mem = 0\n",
    "        \n",
    "        if total_mem >= 12:\n",
    "            return \"large-v3\"\n",
    "        elif total_mem >= 8:\n",
    "            return \"medium\"\n",
    "        elif total_mem >= 4:\n",
    "            return \"small\"\n",
    "        else:\n",
    "            return \"base\"\n",
    "    \n",
    "    # Mode CPU\n",
    "    if psutil is not None:\n",
    "        try:\n",
    "            ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "        except:\n",
    "            ram_gb = 0\n",
    "    else:\n",
    "        ram_gb = 8  # DÃ©faut conservateur\n",
    "    \n",
    "    if ram_gb >= 16:\n",
    "        return \"small\"\n",
    "    elif ram_gb >= 8:\n",
    "        return \"base\"\n",
    "    else:\n",
    "        return \"tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308885cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.660107Z",
     "iopub.status.busy": "2025-10-28T12:26:50.659910Z",
     "iopub.status.idle": "2025-10-28T12:26:50.666884Z",
     "shell.execute_reply": "2025-10-28T12:26:50.666121Z"
    },
    "papermill": {
     "duration": 0.015706,
     "end_time": "2025-10-28T12:26:50.667928",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.652222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration: AssemblyAI best | Device: cuda\n"
     ]
    }
   ],
   "source": [
    "@dataclass \n",
    "class Config: \n",
    "    \"\"\"Configuration centralisÃ©e pour Kaggle\"\"\" \n",
    "    \n",
    "    timezone: str = \"Indian/Antananarivo\"\n",
    "\n",
    "    # Debug\n",
    "    debug_mode: bool = True\n",
    "    save_intermediate_json: bool = True\n",
    "    \n",
    "    # ClÃ©s API \n",
    "    openai_key: str = OPENAI_API_KEY \n",
    "    assemblyai_key: str = ASSEMBLYAI_API_KEY\n",
    "    huggingface_token: str = HUGGINGFACE_TOKEN\n",
    "    \n",
    "    # # Whisper\n",
    "    # whisper_model: str = get_optimal_model_size() # 'tiny', 'base', 'small', 'medium', 'large', \"large-v3\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # compute_type: str = \"float16\" if torch.cuda.is_available() else \"int8\"\n",
    "    # num_workers: int = 4\n",
    "\n",
    "    # AssemblyAI\n",
    "    speech_model: str = \"best\"  # \"nano\" (gratuit) | \"best\" (production)\n",
    "    speakers_expected: int = 10  # Nombre de participants Ã  la rÃ©union\n",
    "    \n",
    "    # Audio\n",
    "    sample_rate: int = 16000\n",
    "\n",
    "    # Vocabulaire mÃ©tier\n",
    "    word_boost: List[str] = None #['Ariary','Fihariana','Unima','Aqualma','rapport financier','budget']\n",
    "    \n",
    "    # # Decoding / anti-hallucination\n",
    "    # beam_size: int = 5\n",
    "    # best_of: int = 5\n",
    "    # patience: float = 1.0\n",
    "    # temperature: float = 0.0\n",
    "    # compression_ratio_threshold: float = 2.4\n",
    "    # log_prob_threshold: float = -1.0\n",
    "    # no_speech_threshold: float = 0.6\n",
    "    # condition_on_previous_text: bool = False\n",
    "    # suppress_blank: bool = True\n",
    "    # suppress_tokens: list[int] = field(default_factory=lambda: [-1])\n",
    "    # max_initial_timestamp: float = 1.0\n",
    "    \n",
    "    # # VAD\n",
    "    # use_vad: bool = True\n",
    "    # vad_threshold: float = 0.5\n",
    "    # vad_min_speech_duration_ms: int = 250\n",
    "    # vad_max_speech_duration_s: float = float('inf')\n",
    "    # vad_min_silence_duration_ms: int = 2000\n",
    "    # vad_speech_pad_ms: int = 400\n",
    "    \n",
    "    # # Chunks longs pour cohÃ©rence (3â€“5 min)\n",
    "    # chunk_length_s: int = 900\n",
    "    # chunk_overlap_s: int = 30\n",
    "    \n",
    "    # # Post-traitement\n",
    "    # max_repetitions: int = 3\n",
    "    \n",
    "    # # Prompt spÃ©cialisÃ©\n",
    "    # initial_prompt: str = (\n",
    "    #     \"Transcription d'une rÃ©union du conseil d'administration Ã  Madagascar. \"\n",
    "    #     \"Vocabulaire: conseil d'administration, procÃ¨s-verbal, quorum, \"\n",
    "    #     \"rÃ©solution, dÃ©libÃ©ration, vote, ordre du jour, budget, \"\n",
    "    #     \"millions d'Ariary, rapport financier. \"\n",
    "    #     \"Termes spÃ©cifiques: Fihariana, SON'INVEST, UNIMA, AQUALMA. \"\n",
    "    #     \"Format: discours naturel sans rÃ©pÃ©titions ni hallucinations.\"\n",
    "    # )\n",
    "    \n",
    "    # LLM (activÃ© par dÃ©faut en production)\n",
    "    enable_llm: bool = True\n",
    "    use_groq: bool = True\n",
    "    groq_model: str = \"llama-3.3-70b-versatile\"  # ou \"llama-3.3-70b-versatile\"\n",
    "    #openai_model: str = \"gpt-4o-mini\" # \"gpt-3.5-turbo\" : Plus Ã©conomique que GPT-4 # Fallback\n",
    "    max_correction_rate: float = 0.20\n",
    "    chunk_size_chars: int = 1500\n",
    "    #chunk_overlap_chars: int = 200\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.word_boost is None:\n",
    "            self.word_boost = [\n",
    "                \"Fihariana\", \"SON'INVEST\", \"UNIMA\", \"AQUALMA\",\n",
    "                \"Ariary\", \"procÃ¨s-verbal\", \"quorum\", \"rÃ©solution\",\n",
    "                \"conseil d'administration\", \"ordre du jour\",\"rapport financier\",\"budget\"\n",
    "            ]\n",
    "\n",
    "config = Config() \n",
    "\n",
    "print(f\"âœ… Configuration: AssemblyAI {config.speech_model} | Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc036d",
   "metadata": {
    "papermill": {
     "duration": 0.006807,
     "end_time": "2025-10-28T12:26:50.681899",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.675092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Comment rÃ©gler les paramÃ¨tres selon les cas***\n",
    "\n",
    "Cas A â€” Audio propre (dictaphones, salle calme)\n",
    "*  beam_size=3, best_of=1â€“2 (plus rapide)\n",
    "* no_speech_threshold=0.6 (ok)\n",
    "* temperature=0.0\n",
    "* VAD : min_silence_duration_ms=1500\n",
    "\n",
    "Cas B â€” Audio bruitÃ© (portes, brouhaha)\n",
    "* beam_size=5, best_of=5 (qualitÃ©)\n",
    "* baisser no_speech_threshold Ã  0.5 si coupures\n",
    "* VAD : threshold=0.4â€“0.5, min_speech_duration_ms=200, min_silence_duration_ms=1800â€“2200\n",
    "* Garde-fous : garder compression_ratio_threshold=2.4\n",
    "\n",
    "Cas C â€” CPU-only (pas de GPU Kaggle)\n",
    "* compute_type=\"int8\", modÃ¨le tiny ou base\n",
    "* beam_size=3, best_of=1\n",
    "* Threads : cpu_threads=2, num_workers=1\n",
    "* Attends un RTF â‰ˆ 2â€“5 (selon longueur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b632e",
   "metadata": {
    "papermill": {
     "duration": 0.006892,
     "end_time": "2025-10-28T12:26:50.695822",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.688930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **PrÃ©paration de l'audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f499668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.710984Z",
     "iopub.status.busy": "2025-10-28T12:26:50.710624Z",
     "iopub.status.idle": "2025-10-28T12:26:50.715166Z",
     "shell.execute_reply": "2025-10-28T12:26:50.714541Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013183,
     "end_time": "2025-10-28T12:26:50.716150",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.702967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slice_audio(input_path: str, output_path: str, start: float = 0.0, duration: Optional[int] = None) -> str:\n",
    "    args = [\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\"-ss\",str(start),\"-i\",input_path,\"-ac\",\"1\",\"-ar\",str(config.sample_rate)]\n",
    "    if duration and duration > 0:\n",
    "        args += [\"-t\",str(duration)]\n",
    "    args += [output_path]\n",
    "    ensure_dir(str(Path(output_path).parent))\n",
    "    code, _, err = run(args)\n",
    "    if code!=0:\n",
    "        raise RuntimeError(\"FFmpeg slice failed: \" + err)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5a708d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.731510Z",
     "iopub.status.busy": "2025-10-28T12:26:50.731211Z",
     "iopub.status.idle": "2025-10-28T12:26:50.744678Z",
     "shell.execute_reply": "2025-10-28T12:26:50.744209Z"
    },
    "papermill": {
     "duration": 0.022309,
     "end_time": "2025-10-28T12:26:50.745668",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.723359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_audio_quality(audio_path: str) -> Dict:\n",
    "    \"\"\"Analyse qualitÃ© audio avant transcription\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ” ANALYSE QUALITÃ‰ AUDIO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    y, sr = librosa.load(audio_path, sr=None, duration=60)  # Premier minute\n",
    "    \n",
    "    # MÃ©triques\n",
    "    duration = librosa.get_duration(path=audio_path)\n",
    "    rms = librosa.feature.rms(y=y)[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "    \n",
    "    # Score qualitÃ© simplifiÃ©\n",
    "    avg_rms = rms.mean()\n",
    "    avg_zcr = zcr.mean()\n",
    "    \n",
    "    quality_score = 0\n",
    "    issues = []\n",
    "    \n",
    "    # VÃ©rification niveau sonore\n",
    "    if avg_rms < 0.01:\n",
    "        issues.append(\"Volume trÃ¨s faible (prÃ©amplification recommandÃ©e)\")\n",
    "    elif avg_rms > 0.5:\n",
    "        issues.append(\"Possible saturation audio\")\n",
    "    else:\n",
    "        quality_score += 30\n",
    "    \n",
    "    # VÃ©rification bruit\n",
    "    if avg_zcr > 0.15:\n",
    "        issues.append(\"Bruit de fond Ã©levÃ© (dÃ©bruitage recommandÃ©)\")\n",
    "    else:\n",
    "        quality_score += 30\n",
    "    \n",
    "    # DurÃ©e\n",
    "    if duration > 7200:  # >2h\n",
    "        issues.append(\"Fichier trÃ¨s long (>2h) - surveillance recommandÃ©e\")\n",
    "        quality_score += 20\n",
    "    else:\n",
    "        quality_score += 40\n",
    "    \n",
    "    result = {\n",
    "        \"duree_totale\": f\"{int(duration // 60)}min {int(duration % 60)}s\",\n",
    "        \"duree_secondes\": duration,\n",
    "        \"sample_rate\": sr,\n",
    "        \"niveau_sonore\": f\"{avg_rms:.3f}\",\n",
    "        \"taux_bruit\": f\"{avg_zcr:.3f}\",\n",
    "        \"score_qualite\": f\"{quality_score}/100\",\n",
    "        \"problemes\": issues,\n",
    "        \"recommandation\": \"âœ… QualitÃ© acceptable\" if quality_score >= 60 else \"âš ï¸ PrÃ©traitement fortement recommandÃ©\"\n",
    "    }\n",
    "    \n",
    "    print(f\"DurÃ©e          : {result['duree_totale']}\")\n",
    "    print(f\"Sample rate    : {sr} Hz\")\n",
    "    print(f\"Niveau sonore  : {result['niveau_sonore']}\")\n",
    "    print(f\"Score qualitÃ©  : {result['score_qualite']}\")\n",
    "    print(f\"\\n{result['recommandation']}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\nâš ï¸  ProblÃ¨mes dÃ©tectÃ©s :\")\n",
    "        for issue in issues:\n",
    "            print(f\"   - {issue}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def estimate_cost(duration_seconds: float, model: str = \"best\") -> Dict:\n",
    "    \"\"\"Estime le coÃ»t AssemblyAI\"\"\"\n",
    "    \n",
    "    # Tarifs AssemblyAI (2025)\n",
    "    rates = {\n",
    "        \"nano\": 0.0,  # Gratuit\n",
    "        \"best\": 0.00025  # $0.00025/seconde\n",
    "    }\n",
    "    \n",
    "    rate = rates.get(model, 0.00025)\n",
    "    cost_usd = duration_seconds * rate\n",
    "    cost_eur = cost_usd * 0.92  # Conversion approximative\n",
    "    \n",
    "    return {\n",
    "        \"duree\": f\"{int(duration_seconds // 60)}min {int(duration_seconds % 60)}s\",\n",
    "        \"modele\": model,\n",
    "        \"cout_usd\": f\"${cost_usd:.2f}\",\n",
    "        \"cout_eur\": f\"â‚¬{cost_eur:.2f}\",\n",
    "        \"gratuit\": model == \"nano\"\n",
    "    }\n",
    "\n",
    "def create_speaker_mapping(utterances: List[Dict]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Aide interactive pour renommer les locuteurs\n",
    "    SPEAKER_A â†’ \"PrÃ©sident\", etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    speakers = sorted(set(u[\"speaker\"] for u in utterances))\n",
    "    \n",
    "    print(\"\\nğŸ‘¥ IDENTIFICATION DES LOCUTEURS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"DÃ©tectÃ©s : {', '.join(speakers)}\")\n",
    "    print(\"\\nAperÃ§u des premiÃ¨res interventions :\\n\")\n",
    "    \n",
    "    # Afficher premiÃ¨res phrases de chaque locuteur\n",
    "    shown = set()\n",
    "    for u in utterances:\n",
    "        if u[\"speaker\"] not in shown:\n",
    "            print(f\"[{u['speaker']}] \\\"{u['text'][:100]}...\\\"\")\n",
    "            shown.add(u[\"speaker\"])\n",
    "            if len(shown) == len(speakers):\n",
    "                break\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Suggestions de mapping :\")\n",
    "    suggestions = {\n",
    "        \"SPEAKER_A\": \"PrÃ©sident\",\n",
    "        \"SPEAKER_B\": \"Directeur GÃ©nÃ©ral\",\n",
    "        \"SPEAKER_C\": \"Directeur Financier\",\n",
    "        \"SPEAKER_D\": \"Participant 4\",\n",
    "        \"SPEAKER_E\": \"Participant 5\"\n",
    "    }\n",
    "    \n",
    "    mapping = {}\n",
    "    for speaker in speakers:\n",
    "        default = suggestions.get(speaker, speaker)\n",
    "        print(f\"   {speaker} â†’ {default}\")\n",
    "        mapping[speaker] = default\n",
    "    \n",
    "    print(\"\\nğŸ“ Appliquer ce mapping ? (Modifier manuellement dans le JSON si nÃ©cessaire)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "def apply_speaker_mapping(data: Dict, mapping: Dict[str, str]) -> Dict:\n",
    "    \"\"\"Applique le renommage des locuteurs\"\"\"\n",
    "    \n",
    "    for utterance in data.get(\"utterances\", []):\n",
    "        original = utterance[\"speaker\"]\n",
    "        if original in mapping:\n",
    "            utterance[\"speaker\"] = mapping[original]\n",
    "    \n",
    "    for utterance in data.get(\"transcription_detaillee\", []):\n",
    "        original = utterance[\"speaker\"]\n",
    "        if original in mapping:\n",
    "            utterance[\"speaker\"] = mapping[original]\n",
    "    \n",
    "    # Mettre Ã  jour stats locuteurs\n",
    "    if \"statistiques_locuteurs\" in data:\n",
    "        new_stats = {}\n",
    "        for speaker, stats in data[\"statistiques_locuteurs\"].items():\n",
    "            new_name = mapping.get(speaker, speaker)\n",
    "            new_stats[new_name] = stats\n",
    "        data[\"statistiques_locuteurs\"] = new_stats\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d284b90",
   "metadata": {
    "papermill": {
     "duration": 0.007092,
     "end_time": "2025-10-28T12:26:50.759978",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.752886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **PrÃ©processing et DÃ©bruitage Audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f212a8c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.775285Z",
     "iopub.status.busy": "2025-10-28T12:26:50.775110Z",
     "iopub.status.idle": "2025-10-28T12:26:50.780930Z",
     "shell.execute_reply": "2025-10-28T12:26:50.780467Z"
    },
    "papermill": {
     "duration": 0.014739,
     "end_time": "2025-10-28T12:26:50.781916",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.767177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioPreprocessor:\n",
    "    \"\"\"PrÃ©traitement audio avec FFmpeg et rÃ©duction de bruit\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate: int = 16000):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def ffmpeg_enhance(self, input_path: str, output_path: str):\n",
    "        \"\"\"AmÃ©liore l'audio avec FFmpeg\"\"\"\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "            \"-i\", input_path,\n",
    "            \"-ac\", \"1\",  # Mono\n",
    "            \"-ar\", str(self.sample_rate),  # 16kHz\n",
    "            \"-af\", \"highpass=f=200,lowpass=f=3000,afftdn=nf=-20\",  # Filtres\n",
    "            output_path\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True)\n",
    "    \n",
    "    def reduce_noise(self, input_path: str, output_path: str):\n",
    "        \"\"\"RÃ©duit le bruit avec noisereduce\"\"\"\n",
    "        y, sr = librosa.load(input_path, sr=self.sample_rate)\n",
    "        y_clean = nr.reduce_noise(y=y, sr=sr, stationary=True, prop_decrease=0.8)\n",
    "        sf.write(output_path, y_clean, sr)\n",
    "        return output_path\n",
    "    \n",
    "    def process(self, input_path: str, output_dir: str) -> str:\n",
    "        \"\"\"Pipeline complet de prÃ©traitement\"\"\"\n",
    "        base_name = Path(input_path).stem\n",
    "        ffmpeg_path = str(Path(output_dir) / f\"{base_name}_ffmpeg.wav\")\n",
    "        clean_path = str(Path(output_dir) / f\"{base_name}_clean.wav\")\n",
    "\n",
    "        print(\"ğŸ”Š PrÃ©traitement audio...\")\n",
    "        self.ffmpeg_enhance(input_path, str(ffmpeg_path))\n",
    "        self.reduce_noise(str(ffmpeg_path), str(clean_path))\n",
    "        \n",
    "        # Nettoyage fichier intermÃ©diaire\n",
    "        # if Path(ffmpeg_path).exists():\n",
    "        #     Path(ffmpeg_path).unlink()\n",
    "        ffmpeg_path.unlink(missing_ok=True)\n",
    "        \n",
    "        return str(clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef1c3a25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.796888Z",
     "iopub.status.busy": "2025-10-28T12:26:50.796718Z",
     "iopub.status.idle": "2025-10-28T12:26:50.801432Z",
     "shell.execute_reply": "2025-10-28T12:26:50.800764Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013432,
     "end_time": "2025-10-28T12:26:50.802561",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.789129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_audio_file(audio_path: str) -> Dict:\n",
    "    \"\"\"PrÃ©pare et valide le fichier audio pour la transcription\"\"\"\n",
    "    file_info = {\n",
    "        \"path\": audio_path,\n",
    "        \"exists\": os.path.exists(audio_path),\n",
    "        \"size_mb\": 0,\n",
    "        \"duration_seconds\": 0,\n",
    "        \"format\": audio_path.split('.')[-1],\n",
    "        \"sample_rate\": 0,\n",
    "        \"channels\": 0\n",
    "    }\n",
    "    \n",
    "    if file_info[\"exists\"]:\n",
    "        file_info[\"size_mb\"] = os.path.getsize(audio_path) / (1024 * 1024)\n",
    "        \n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None, duration=10)\n",
    "            file_info[\"sample_rate\"] = sr\n",
    "            duration = librosa.get_duration(path=audio_path)\n",
    "            file_info[\"duration_seconds\"] = duration\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur lecture audio: {e}\")\n",
    "    \n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d3888",
   "metadata": {
    "papermill": {
     "duration": 0.007015,
     "end_time": "2025-10-28T12:26:50.816791",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.809776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Transcription Audio**\n",
    "**Service de transcription avec audio nettoyÃ©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061350ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.831994Z",
     "iopub.status.busy": "2025-10-28T12:26:50.831783Z",
     "iopub.status.idle": "2025-10-28T12:26:50.844703Z",
     "shell.execute_reply": "2025-10-28T12:26:50.844191Z"
    },
    "papermill": {
     "duration": 0.021682,
     "end_time": "2025-10-28T12:26:50.845618",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.823936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TranscriptionService:\n",
    "    \"\"\"Service de transcription avec configurations SIIS optimisÃ©es\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.model = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Charge le modÃ¨le Whisper avec gestion mÃ©moire\"\"\"\n",
    "        if self.model is None:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            self.model = WhisperModel(\n",
    "                self.cfg.whisper_model,\n",
    "                device=self.cfg.device,\n",
    "                compute_type=self.cfg.compute_type,\n",
    "                num_workers=self.cfg.num_workers,  # 4 au lieu de 1\n",
    "                cpu_threads=4 if self.cfg.device == \"cpu\" else 0\n",
    "            )\n",
    "        return self.model\n",
    "    \n",
    "    def unload_model(self):\n",
    "        \"\"\"LibÃ¨re le modÃ¨le de la mÃ©moire\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    def transcribe_chunk(self, audio_path: str) -> Tuple[List, Dict]:\n",
    "        \"\"\"Transcrit un chunk audio\"\"\"\n",
    "        model = self.load_model()\n",
    "        \n",
    "        segments, info = model.transcribe(\n",
    "            audio_path,\n",
    "            language=\"fr\",\n",
    "            beam_size=self.cfg.beam_size,\n",
    "            best_of=self.cfg.best_of,\n",
    "            patience=self.cfg.patience,\n",
    "            temperature=self.cfg.temperature,\n",
    "            compression_ratio_threshold=self.cfg.compression_ratio_threshold,\n",
    "            log_prob_threshold=self.cfg.log_prob_threshold,\n",
    "            no_speech_threshold=self.cfg.no_speech_threshold,\n",
    "            condition_on_previous_text=self.cfg.condition_on_previous_text,  # TRUE!\n",
    "            initial_prompt=self.cfg.initial_prompt,\n",
    "            word_timestamps=True,\n",
    "            suppress_tokens=self.cfg.suppress_tokens,\n",
    "            suppress_blank=self.cfg.suppress_blank,\n",
    "            max_initial_timestamp=self.cfg.max_initial_timestamp,\n",
    "            vad_filter=self.cfg.use_vad,\n",
    "            vad_parameters={\n",
    "                \"threshold\": self.cfg.vad_threshold,\n",
    "                \"min_speech_duration_ms\": self.cfg.vad_min_speech_duration_ms,\n",
    "                \"max_speech_duration_s\": self.cfg.vad_max_speech_duration_s,\n",
    "                \"min_silence_duration_ms\": self.cfg.vad_min_silence_duration_ms,\n",
    "                \"speech_pad_ms\": self.cfg.vad_speech_pad_ms,\n",
    "            } if self.cfg.use_vad else None\n",
    "        )\n",
    "        \n",
    "        return list(segments), info\n",
    "    \n",
    "    def transcribe_long_audio(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Transcrit un audio long avec chunking optimisÃ© SIIS\n",
    "        Chunks de 900s au lieu de 180-300s pour moins de dÃ©rive\n",
    "        \"\"\"\n",
    "        # Obtenir la durÃ©e totale\n",
    "        y, sr = librosa.load(audio_path, sr=self.cfg.sample_rate, duration=1)\n",
    "        info = sf.info(audio_path)\n",
    "        total_duration = info.duration\n",
    "        \n",
    "        # Calcul des chunks\n",
    "        chunk_length = self.cfg.chunk_length_s\n",
    "        chunk_overlap = self.cfg.chunk_overlap_s\n",
    "        num_chunks = max(1, ceil(total_duration / chunk_length))\n",
    "        \n",
    "        print(f\"ğŸ“Š Audio: {total_duration:.1f}s | {num_chunks} chunks de {chunk_length}s\")\n",
    "        \n",
    "        all_segments = []\n",
    "        all_text = []\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_time = max(0, i * chunk_length - (chunk_overlap if i > 0 else 0))\n",
    "            duration = min(chunk_length + chunk_overlap, total_duration - start_time)\n",
    "            \n",
    "            # Extraire le chunk avec ffmpeg\n",
    "            chunk_path = str(TEMP_DIR / f\"chunk_{i:04d}.wav\")\n",
    "            cmd = [\n",
    "                \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "                \"-ss\", str(start_time),\n",
    "                \"-t\", str(duration),\n",
    "                \"-i\", audio_path,\n",
    "                \"-ac\", \"1\",\n",
    "                \"-ar\", str(self.cfg.sample_rate),\n",
    "                chunk_path\n",
    "            ]\n",
    "            subprocess.run(cmd, check=True)\n",
    "            \n",
    "            # Transcrire le chunk\n",
    "            print(f\"  Chunk {i+1}/{num_chunks}: {start_time:.1f}s - {start_time+duration:.1f}s\")\n",
    "            segments, chunk_info = self.transcribe_chunk(chunk_path)\n",
    "            \n",
    "            # Ajuster les timestamps\n",
    "            for seg in segments:\n",
    "                # CrÃ©er un nouveau dictionnaire pour chaque segment\n",
    "                segment_dict = {\n",
    "                    \"start\": seg.start + start_time,\n",
    "                    \"end\": seg.end + start_time,\n",
    "                    \"text\": seg.text.strip(),\n",
    "                }\n",
    "                \n",
    "                # Ajouter les mots avec timestamps ajustÃ©s si disponibles\n",
    "                if hasattr(seg, 'words') and seg.words:\n",
    "                    segment_dict[\"words\"] = [\n",
    "                        {\n",
    "                            \"start\": w.start + start_time,\n",
    "                            \"end\": w.end + start_time,\n",
    "                            \"word\": w.word,\n",
    "                            \"probability\": getattr(w, 'probability', 0.0)\n",
    "                        }\n",
    "                        for w in seg.words\n",
    "                    ]\n",
    "                \n",
    "                # Ajouter d'autres mÃ©tadonnÃ©es si disponibles\n",
    "                if hasattr(seg, 'no_speech_prob'):\n",
    "                    segment_dict[\"no_speech_prob\"] = seg.no_speech_prob\n",
    "                if hasattr(seg, 'avg_logprob'):\n",
    "                    segment_dict[\"avg_logprob\"] = seg.avg_logprob\n",
    "                if hasattr(seg, 'compression_ratio'):\n",
    "                    segment_dict[\"compression_ratio\"] = seg.compression_ratio\n",
    "                \n",
    "                all_segments.append(segment_dict)\n",
    "                all_text.append(seg.text.strip())\n",
    "            \n",
    "            # Nettoyer le chunk temporaire\n",
    "            Path(chunk_path).unlink()\n",
    "        \n",
    "        # Assembler le rÃ©sultat\n",
    "        result = {\n",
    "            \"status\": \"success\",\n",
    "            \"duration\": total_duration,\n",
    "            \"language\": \"fr\",\n",
    "            \"segments\": all_segments,\n",
    "            \"text\": \" \".join(all_text),\n",
    "            \"metadata\": {\n",
    "                \"model\": self.cfg.whisper_model,\n",
    "                \"chunks\": num_chunks,\n",
    "                \"chunk_length\": chunk_length,\n",
    "                \"condition_on_previous\": self.cfg.condition_on_previous_text\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16711249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.860737Z",
     "iopub.status.busy": "2025-10-28T12:26:50.860364Z",
     "iopub.status.idle": "2025-10-28T12:26:50.863450Z",
     "shell.execute_reply": "2025-10-28T12:26:50.862781Z"
    },
    "papermill": {
     "duration": 0.011667,
     "end_time": "2025-10-28T12:26:50.864445",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.852778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "#result = transcription_service.transcribe_audio(audio_file)\n",
    "#print(f\"Transcription: {result['transcription'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938d2e3",
   "metadata": {
    "papermill": {
     "duration": 0.006956,
     "end_time": "2025-10-28T12:26:50.878466",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.871510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Diarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30599b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.893648Z",
     "iopub.status.busy": "2025-10-28T12:26:50.893452Z",
     "iopub.status.idle": "2025-10-28T12:26:50.900500Z",
     "shell.execute_reply": "2025-10-28T12:26:50.899858Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015813,
     "end_time": "2025-10-28T12:26:50.901471",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.885658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def diarize(transcription_data: Dict, audio_path: str, hf_token: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Diarisation avec pyannote (ou fallback whisperx)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pyannote.audio import Pipeline\n",
    "        \n",
    "        print(\"ğŸ™ï¸ Diarisation avec pyannote...\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        \n",
    "        diarization = pipeline(audio_path)\n",
    "        \n",
    "        # Mapper les segments aux locuteurs\n",
    "        segments_with_speakers = []\n",
    "        for seg in transcription_data.get(\"segments\", []):\n",
    "            start, end = seg[\"start\"], seg[\"end\"]\n",
    "            \n",
    "            # Trouver le locuteur majoritaire pour ce segment\n",
    "            speaker_times = {}\n",
    "            for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "                overlap_start = max(start, turn.start)\n",
    "                overlap_end = min(end, turn.end)\n",
    "                if overlap_start < overlap_end:\n",
    "                    overlap_duration = overlap_end - overlap_start\n",
    "                    speaker_times[speaker] = speaker_times.get(speaker, 0) + overlap_duration\n",
    "            \n",
    "            # Assigner le locuteur avec le plus de temps de parole\n",
    "            if speaker_times:\n",
    "                main_speaker = max(speaker_times, key=speaker_times.get)\n",
    "                seg[\"speaker\"] = main_speaker\n",
    "            else:\n",
    "                seg[\"speaker\"] = \"Unknown\"\n",
    "            \n",
    "            segments_with_speakers.append(seg)\n",
    "        \n",
    "        transcription_data[\"segments_diarized\"] = segments_with_speakers\n",
    "        transcription_data[\"diarization_method\"] = \"pyannote\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Diarisation pyannote Ã©chouÃ©e: {e}\")\n",
    "        \n",
    "        # Fallback sur whisperx si disponible\n",
    "        try:\n",
    "            import whisperx\n",
    "            print(\"ğŸ”„ Fallback sur whisperx...\")\n",
    "            \n",
    "            # Aligner avec whisperx\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            align_model, metadata = whisperx.load_align_model(\n",
    "                language_code=\"fr\",\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            result_aligned = whisperx.align(\n",
    "                transcription_data[\"segments\"],\n",
    "                align_model,\n",
    "                metadata,\n",
    "                audio_path,\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Diarisation\n",
    "            diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token)\n",
    "            diarize_segments = diarize_model(audio_path)\n",
    "            result_diarized = whisperx.assign_word_speakers(diarize_segments, result_aligned)\n",
    "            \n",
    "            transcription_data[\"segments_diarized\"] = result_diarized[\"segments\"]\n",
    "            transcription_data[\"diarization_method\"] = \"whisperx\"\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"âš ï¸ Diarisation whisperx Ã©chouÃ©e: {e2}\")\n",
    "            transcription_data[\"diarization_method\"] = \"none\"\n",
    "    \n",
    "    return transcription_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7b0dd",
   "metadata": {
    "papermill": {
     "duration": 0.007049,
     "end_time": "2025-10-28T12:26:50.915698",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.908649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Post-traitement du texte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e91343bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.930652Z",
     "iopub.status.busy": "2025-10-28T12:26:50.930464Z",
     "iopub.status.idle": "2025-10-28T12:26:50.935580Z",
     "shell.execute_reply": "2025-10-28T12:26:50.935101Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013628,
     "end_time": "2025-10-28T12:26:50.936546",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.922918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_numbers_and_units(text: str) -> str:\n",
    "    \"\"\"Normalise les nombres et unitÃ©s monÃ©taires\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Normaliser les millions\n",
    "    text = re.sub(r'(\\d+)\\s*,\\s*(\\d+)\\s*millions?', r'\\1.\\2 millions', text)\n",
    "    text = re.sub(r'(\\d+)\\s*virgule\\s*(\\d+)\\s*millions?', r'\\1.\\2 millions', text)\n",
    "    \n",
    "    # Ajouter Ariary si manquant aprÃ¨s les montants\n",
    "    text = re.sub(r'(\\d+(?:\\.\\d+)?)\\s*millions?\\s*(?!d\\'?[Aa]riary)', r'\\1 millions d\\'Ariary', text)\n",
    "    \n",
    "    # Normaliser les pourcentages\n",
    "    text = re.sub(r'(\\d+)\\s*pour\\s*cent', r'\\1%', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def deduplicate_sentences(text: str) -> str:\n",
    "    \"\"\"Supprime les rÃ©pÃ©titions de phrases\"\"\"\n",
    "    import re\n",
    "    \n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    seen = set()\n",
    "    unique_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent_lower = sent.lower().strip()\n",
    "        if sent_lower and sent_lower not in seen:\n",
    "            seen.add(sent_lower)\n",
    "            unique_sentences.append(sent)\n",
    "    \n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def postprocess_text(text: str) -> str:\n",
    "    # Nettoyer les espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = normalize_numbers_and_units(text)\n",
    "    text = deduplicate_sentences(text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a054a3c",
   "metadata": {
    "papermill": {
     "duration": 0.007124,
     "end_time": "2025-10-28T12:26:50.950988",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.943864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Transcription AssemblyAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaeb331f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.966522Z",
     "iopub.status.busy": "2025-10-28T12:26:50.966351Z",
     "iopub.status.idle": "2025-10-28T12:26:50.974194Z",
     "shell.execute_reply": "2025-10-28T12:26:50.973730Z"
    },
    "papermill": {
     "duration": 0.016978,
     "end_time": "2025-10-28T12:26:50.975204",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.958226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AssemblyAITranscriber:\n",
    "    \"\"\"Service de fallback avec AssemblyAI\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        aai.settings.api_key = cfg.assemblyai_key\n",
    "        \n",
    "        self.config = aai.TranscriptionConfig(\n",
    "            language_code=\"fr\",\n",
    "            speaker_labels=True,\n",
    "            speakers_expected=cfg.speakers_expected,\n",
    "            punctuate=True,\n",
    "            format_text=True,\n",
    "            disfluencies=True,\n",
    "            word_boost=cfg.word_boost,\n",
    "            speech_model=cfg.speech_model,\n",
    "            auto_chapters=True  # DÃ©coupage thÃ©matique\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def transcribe(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Transcription avec polling jusqu'Ã  completion\n",
    "        \n",
    "        Returns:\n",
    "            Dict avec text, utterances (locuteur + timestamps), metadata\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“ Transcription AssemblyAI...\")\n",
    "        print(f\"   ModÃ¨le: {self.cfg.speech_model}\")\n",
    "        \n",
    "        transcriber = aai.Transcriber(config=self.config)\n",
    "        transcript = transcriber.transcribe(audio_path)\n",
    "        \n",
    "        # Attente completion (polling toutes les 5s)\n",
    "        start_time = time.time()\n",
    "        while transcript.status not in [\n",
    "            aai.TranscriptStatus.completed,\n",
    "            aai.TranscriptStatus.error\n",
    "        ]:\n",
    "            elapsed = int(time.time() - start_time)\n",
    "            print(f\"   Traitement en cours... ({elapsed}s)\", end=\"\\r\")\n",
    "            time.sleep(5)\n",
    "            transcript = transcriber.get_transcript(transcript.id)\n",
    "        \n",
    "        if transcript.status == aai.TranscriptStatus.error:\n",
    "            raise Exception(f\"Erreur AssemblyAI: {transcript.error}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Transcription terminÃ©e ({int(time.time() - start_time)}s)\")\n",
    "        \n",
    "        # Structurer output\n",
    "        result = {\n",
    "            \"status\": \"success\",\n",
    "            \"text\": transcript.text,\n",
    "            \"duration\": transcript.audio_duration / 1000,  # ms â†’ secondes\n",
    "            \"confidence\": transcript.confidence,\n",
    "            \"utterances\": [\n",
    "                {\n",
    "                    \"speaker\": u.speaker,\n",
    "                    \"text\": u.text.strip(),\n",
    "                    \"start\": u.start / 1000,\n",
    "                    \"end\": u.end / 1000,\n",
    "                    \"confidence\": u.confidence,\n",
    "                    \"words\": [\n",
    "                        {\n",
    "                            \"word\": w.text,\n",
    "                            \"start\": w.start / 1000,\n",
    "                            \"end\": w.end / 1000,\n",
    "                            \"confidence\": w.confidence\n",
    "                        }\n",
    "                        for w in u.words\n",
    "                    ]\n",
    "                }\n",
    "                for u in transcript.utterances\n",
    "            ],\n",
    "            \"chapters\": [\n",
    "                {\n",
    "                    \"summary\": c.summary,\n",
    "                    \"headline\": c.headline,\n",
    "                    \"start\": c.start / 1000,\n",
    "                    \"end\": c.end / 1000\n",
    "                }\n",
    "                for c in (transcript.chapters or [])\n",
    "            ] if hasattr(transcript, 'chapters') else [],\n",
    "            \"metadata\": {\n",
    "                \"model\": self.cfg.speech_model,\n",
    "                \"speakers_detected\": len(set(u.speaker for u in transcript.utterances)),\n",
    "                \"provider\": \"AssemblyAI\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09dd54",
   "metadata": {
    "papermill": {
     "duration": 0.006922,
     "end_time": "2025-10-28T12:26:50.989646",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.982724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ** FALLBACK WHISPER **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea66cb6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.005247Z",
     "iopub.status.busy": "2025-10-28T12:26:51.004544Z",
     "iopub.status.idle": "2025-10-28T12:26:51.008498Z",
     "shell.execute_reply": "2025-10-28T12:26:51.007865Z"
    },
    "papermill": {
     "duration": 0.012691,
     "end_time": "2025-10-28T12:26:51.009532",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.996841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WhisperFallback:\n",
    "    \"\"\"Fallback si AssemblyAI Ã©choue (quota, timeout)\"\"\"\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Utilise faster-whisper large-v3 (code existant)\"\"\"\n",
    "        print(\"âš ï¸ Fallback sur Whisper (AssemblyAI indisponible)\")\n",
    "        \n",
    "        # Importer votre TranscriptionService existant\n",
    "        # return existing_transcription_service.transcribe_long_audio(audio_path)\n",
    "        \n",
    "        # Placeholder pour exemple\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": \"Fallback Whisper non implÃ©mentÃ© (conserver code existant)\",\n",
    "            \"text\": \"\",\n",
    "            \"utterances\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb5335",
   "metadata": {
    "papermill": {
     "duration": 0.00723,
     "end_time": "2025-10-28T12:26:51.024125",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.016895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Nettoyage LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe0d9357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.039503Z",
     "iopub.status.busy": "2025-10-28T12:26:51.039145Z",
     "iopub.status.idle": "2025-10-28T12:26:51.049396Z",
     "shell.execute_reply": "2025-10-28T12:26:51.048751Z"
    },
    "papermill": {
     "duration": 0.019343,
     "end_time": "2025-10-28T12:26:51.050542",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.031199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLMCleaner:\n",
    "    \"\"\"Nettoyage du texte avec LLM (Groq ou OpenAI)\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.client = None\n",
    "        \n",
    "        if cfg.use_groq and GROQ_API_KEY:\n",
    "            try:\n",
    "                from groq import Groq\n",
    "                self.client = Groq(api_key=GROQ_API_KEY)\n",
    "                self.provider = \"groq\"\n",
    "                print(\"âœ… Utilisation de Groq pour le nettoyage LLM\")\n",
    "            except ImportError:\n",
    "                print(\"âš ï¸ Package groq non installÃ©, installation...\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"groq\"])\n",
    "                from groq import Groq\n",
    "                self.client = Groq(api_key=GROQ_API_KEY)\n",
    "                self.provider = \"groq\"\n",
    "        elif OPENAI_API_KEY:\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "            self.provider = \"openai\"\n",
    "            print(\"âœ… Utilisation d'OpenAI pour le nettoyage LLM\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Aucune clÃ© API LLM disponible\")\n",
    "    \n",
    "    def _create_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"DÃ©coupe le texte en chunks pour traitement LLM avec overlap\"\"\"\n",
    "        chunks = []\n",
    "        chunk_size = self.cfg.chunk_size_chars\n",
    "        overlap = 200\n",
    "        \n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunks.append(text[i:i + chunk_size])\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def clean_text(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"Nettoie le texte avec le LLM\"\"\"\n",
    "        if not self.client or not text:\n",
    "            return text, 0.0\n",
    "        \n",
    "        chunks = self._create_chunks(text)\n",
    "        cleaned_chunks = []\n",
    "        total_delta = 0\n",
    "        \n",
    "        system_prompt = \"\"\"Tu es un assistant de correction de transcription.\n",
    "            Tu corriges UNIQUEMENT : orthographe, grammaire, ponctuation, noms propres malgaches.\n",
    "            RÃˆGLES STRICTES :\n",
    "            1. NE JAMAIS ajouter d'information non prÃ©sente\n",
    "            2. NE PAS changer le sens des phrases\n",
    "            3. Conserver tous les chiffres et montants exacts\n",
    "            Contexte: RÃ©union du conseil d'administration Ã  Madagascar.\n",
    "            Termes valides: Fihariana, SON'INVEST, UNIMA, AQUALMA, Ariary.\"\"\"\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"  Nettoyage chunk {i}/{len(chunks)}...\")\n",
    "            \n",
    "            try:\n",
    "                if self.provider == \"groq\":\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.cfg.groq_model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": f\"Corrige ce texte:\\n\\n{chunk}\"}\n",
    "                        ],\n",
    "                        temperature=0.2,\n",
    "                        max_tokens=2000\n",
    "                    )\n",
    "                    cleaned = response.choices[0].message.content.strip()\n",
    "                    cleaned_chunks.append(cleaned)\n",
    "                    \n",
    "                else:  # OpenAI\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.cfg.openai_model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": chunk}\n",
    "                        ],\n",
    "                        temperature=0.2,\n",
    "                        max_tokens=1400\n",
    "                    )\n",
    "                    cleaned = response.choices[0].message.content.strip()\n",
    "                \n",
    "                cleaned_chunks.append(cleaned)\n",
    "                total_delta += abs(len(cleaned) - len(chunk))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸ Erreur LLM chunk {i}: {e}\")\n",
    "                cleaned_chunks.append(chunk)  # Garder l'original si erreur\n",
    "        \n",
    "        # Assembler et calculer le taux de correction\n",
    "        merged_text = ' '.join(cleaned_chunks)\n",
    "        correction_rate = total_delta / max(len(text), 1)\n",
    "        \n",
    "        # VÃ©rifier le taux de correction\n",
    "        if correction_rate > self.cfg.max_correction_rate:\n",
    "            print(f\"âš ï¸ Taux de correction {correction_rate:.1%} > seuil {self.cfg.max_correction_rate:.0%}\")\n",
    "            print(\"   â†’ Conservation du texte post-traitÃ© sans LLM\")\n",
    "            return text#, correction_rate\n",
    "\n",
    "        print(f\"\\nâœ… Nettoyage terminÃ© (taux: {correction_rate:.1%})\")\n",
    "        return merged_text#, correction_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f0dd3",
   "metadata": {
    "papermill": {
     "duration": 0.007208,
     "end_time": "2025-10-28T12:26:51.066254",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.059046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Par dÃ©faut, la langue est auto. Pour ton cas, force franÃ§ais :\n",
    "        config = aai.TranscriptionConfig(language_code=\"fr\")\n",
    "2. Diarisation (orateurs)\n",
    "        config = aai.TranscriptionConfig(speaker_labels=True)\n",
    "\n",
    "Exemple :\n",
    "    config = aai.TranscriptionConfig(language_code=\"fr\", speaker_labels=True)\n",
    "    transcript = transcriber.transcribe(audio_path, config=config)\n",
    "\n",
    "Appel :\n",
    "    Si TranscriptionService.transcribe_audio renvoie status=\"error\" ou un real_time_factor >> 5 (trop lent) ou trop de segments sous ton confidence_threshold, alors :\n",
    "        > result = fallback_service.transcribe_with_assemblyai(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eba983",
   "metadata": {
    "papermill": {
     "duration": 0.007065,
     "end_time": "2025-10-28T12:26:51.080639",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.073574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **FORMATAGE SORTIE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17da9f71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.096014Z",
     "iopub.status.busy": "2025-10-28T12:26:51.095816Z",
     "iopub.status.idle": "2025-10-28T12:26:51.105392Z",
     "shell.execute_reply": "2025-10-28T12:26:51.104871Z"
    },
    "papermill": {
     "duration": 0.018482,
     "end_time": "2025-10-28T12:26:51.106393",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.087911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OutputFormatter:\n",
    "    \"\"\"GÃ©nÃ¨re JSON structurÃ© + TXT lisible pour comitÃ©\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_json(data: Dict, clean_text: str) -> Dict:\n",
    "        \"\"\"JSON enrichi pour archivage\"\"\"\n",
    "        \n",
    "        # Calculer statistiques locuteurs\n",
    "        speaker_stats = {}\n",
    "        for u in data.get(\"utterances\", []):\n",
    "            speaker = u[\"speaker\"]\n",
    "            duration = u[\"end\"] - u[\"start\"]\n",
    "            if speaker not in speaker_stats:\n",
    "                speaker_stats[speaker] = {\"temps_parole\": 0, \"interventions\": 0}\n",
    "            speaker_stats[speaker][\"temps_parole\"] += duration\n",
    "            speaker_stats[speaker][\"interventions\"] += 1\n",
    "        \n",
    "        # Formater temps de parole\n",
    "        for speaker, stats in speaker_stats.items():\n",
    "            minutes = int(stats[\"temps_parole\"] / 60)\n",
    "            secondes = int(stats[\"temps_parole\"] % 60)\n",
    "            stats[\"temps_parole_fmt\"] = f\"{minutes}min {secondes}s\"\n",
    "        \n",
    "        return {\n",
    "            \"metadata\": {\n",
    "                \"date\": time.strftime(\"%Y-%m-%d\"),\n",
    "                \"duree_totale\": f\"{int(data['duration'] // 60)}min {int(data['duration'] % 60)}s\",\n",
    "                \"participants\": list(speaker_stats.keys()),\n",
    "                \"modele\": data[\"metadata\"][\"model\"],\n",
    "                \"confiance_moyenne\": f\"{data.get('confidence', 0):.1%}\",\n",
    "                \"provider\": data[\"metadata\"][\"provider\"]\n",
    "            },\n",
    "            \"resume_executif\": {\n",
    "                \"chapitres\": data.get(\"chapters\", []),\n",
    "                \"decisions\": [],  # Ã€ extraire manuellement ou via NLP\n",
    "                \"actions\": []\n",
    "            },\n",
    "            \"transcription_complete\": clean_text,\n",
    "            \"transcription_detaillee\": data[\"utterances\"],\n",
    "            \"statistiques_locuteurs\": speaker_stats\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_txt(data: Dict, clean_text: str) -> str:\n",
    "        \"\"\"TXT formatÃ© pour lecture comitÃ©\"\"\"\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"â•\" * 70)\n",
    "        lines.append(\"  PROCÃˆS-VERBAL - CONSEIL D'ADMINISTRATION\")\n",
    "        lines.append(f\"  Date : {time.strftime('%d %B %Y')}\")\n",
    "        lines.append(f\"  DurÃ©e : {int(data['duration'] // 60)}min {int(data['duration'] % 60)}s\")\n",
    "        lines.append(\"â•\" * 70)\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Transcription avec locuteurs\n",
    "        lines.append(\"TRANSCRIPTION\")\n",
    "        lines.append(\"-\" * 70)\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        current_speaker = None\n",
    "        for u in data.get(\"utterances\", []):\n",
    "            timestamp = f\"[{int(u['start'] // 60):02d}:{int(u['start'] % 60):02d}]\"\n",
    "            speaker = u[\"speaker\"]\n",
    "            \n",
    "            if speaker != current_speaker:\n",
    "                lines.append(\"\")\n",
    "                lines.append(f\"{timestamp} {speaker}\")\n",
    "                current_speaker = speaker\n",
    "            \n",
    "            lines.append(u[\"text\"])\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        lines.append(\"=\" * 70)\n",
    "        \n",
    "        # Chapitres thÃ©matiques\n",
    "        if data.get(\"chapters\"):\n",
    "            lines.append(\"CHAPITRES THÃ‰MATIQUES\")\n",
    "            lines.append(\"-\" * 70)\n",
    "            for i, ch in enumerate(data[\"chapters\"], 1):\n",
    "                lines.append(f\"\\n{i}. {ch['headline']}\")\n",
    "                lines.append(f\"   [{int(ch['start'] // 60):02d}:{int(ch['start'] % 60):02d}] {ch['summary']}\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"=\" * 70)\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5439fd",
   "metadata": {
    "papermill": {
     "duration": 0.007031,
     "end_time": "2025-10-28T12:26:51.120712",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.113681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Pipeline de transcription avec gestion automatique du fallback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08de1e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.136377Z",
     "iopub.status.busy": "2025-10-28T12:26:51.136184Z",
     "iopub.status.idle": "2025-10-28T12:26:51.145814Z",
     "shell.execute_reply": "2025-10-28T12:26:51.145319Z"
    },
    "papermill": {
     "duration": 0.018708,
     "end_time": "2025-10-28T12:26:51.146844",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.128136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcribe_meeting(audio_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pipeline complet de transcription\n",
    "    \n",
    "    Ã‰tapes:\n",
    "    1. PrÃ©traitement audio\n",
    "    2. Transcription AssemblyAI (+ fallback Whisper)\n",
    "    3. Post-traitement LLM\n",
    "    4. Export JSON + TXT\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Chemin fichier audio (.mp3, .wav, .m4a)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec status, fichiers gÃ©nÃ©rÃ©s, mÃ©tadonnÃ©es\n",
    "    \"\"\"\n",
    "    \n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(OUTPUT_PATH)\n",
    "    \n",
    "    try:\n",
    "        # [1/4] PrÃ©traitement audio\n",
    "        print(\"\\n[1/4] ğŸ”Š PRÃ‰TRAITEMENT AUDIO\")\n",
    "        print(\"=\" * 60)\n",
    "        preprocessor = AudioPreprocessor(config.sample_rate)\n",
    "        clean_audio = preprocessor.process(audio_path)\n",
    "        \n",
    "        # [2/4] Transcription\n",
    "        print(\"\\n[2/4] ğŸ“ TRANSCRIPTION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        transcriber = AssemblyAITranscriber(config)\n",
    "        try:\n",
    "            result = transcriber.transcribe(clean_audio)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur AssemblyAI: {e}\")\n",
    "            fallback = WhisperFallback()\n",
    "            result = fallback.transcribe(clean_audio)\n",
    "            \n",
    "            if result[\"status\"] == \"error\":\n",
    "                raise Exception(\"Ã‰chec transcription (AssemblyAI + Whisper)\")\n",
    "        \n",
    "        # [3/4] Post-traitement LLM\n",
    "        print(\"\\n[3/4] âœ¨ POST-TRAITEMENT LLM\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        raw_text = result[\"text\"]\n",
    "        \n",
    "        if config.enable_llm and config.groq_key:\n",
    "            cleaner = LLMCleaner(config)\n",
    "            clean_text = cleaner.clean_text(raw_text)\n",
    "        else:\n",
    "            print(\"â„¹ï¸  LLM dÃ©sactivÃ©, utilisation texte brut\")\n",
    "            clean_text = raw_text\n",
    "        \n",
    "        # [4/4] Export\n",
    "        print(\"\\n[4/4] ğŸ’¾ EXPORT RÃ‰SULTATS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        formatter = OutputFormatter()\n",
    "        \n",
    "        # JSON structurÃ©\n",
    "        json_data = formatter.format_json(result, clean_text)\n",
    "        json_path = output_dir / f\"transcription_{timestamp}.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… JSON sauvegardÃ© : {json_path.name}\")\n",
    "        \n",
    "        # TXT lisible\n",
    "        txt_content = formatter.format_txt(result, clean_text)\n",
    "        txt_path = output_dir / f\"pv_reunion_{timestamp}.txt\"\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(txt_content)\n",
    "        print(f\"âœ… TXT sauvegardÃ©  : {txt_path.name}\")\n",
    "        \n",
    "        # JSON brut AssemblyAI (pour debug)\n",
    "        raw_json_path = output_dir / f\"raw_assemblyai_{timestamp}.json\"\n",
    "        with open(raw_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"ğŸ“‹ Raw JSON       : {raw_json_path.name}\")\n",
    "        \n",
    "        # Nettoyage fichiers temporaires\n",
    "        Path(clean_audio).unlink(missing_ok=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"âœ¨ PIPELINE TERMINÃ‰ AVEC SUCCÃˆS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"files\": {\n",
    "                \"json\": str(json_path),\n",
    "                \"txt\": str(txt_path),\n",
    "                \"raw\": str(raw_json_path)\n",
    "            },\n",
    "            \"metadata\": json_data[\"metadata\"],\n",
    "            \"duration\": result[\"duration\"],\n",
    "            \"speakers\": len(set(u[\"speaker\"] for u in result[\"utterances\"]))\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ERREUR PIPELINE : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": timestamp\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8287ede",
   "metadata": {
    "papermill": {
     "duration": 0.007079,
     "end_time": "2025-10-28T12:26:51.161252",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.154173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **EXÃ‰CUTION PRINCIPALE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bd1499d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.176924Z",
     "iopub.status.busy": "2025-10-28T12:26:51.176495Z",
     "iopub.status.idle": "2025-10-28T12:26:51.180233Z",
     "shell.execute_reply": "2025-10-28T12:26:51.179565Z"
    },
    "papermill": {
     "duration": 0.012689,
     "end_time": "2025-10-28T12:26:51.181290",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.168601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Fichier dÃ©tectÃ© : test_30mn.mp3\n"
     ]
    }
   ],
   "source": [
    "# Test avec votre fichier audio\n",
    "#audio_file = f\"{UPLOAD_PATH}atelier.mp3\"\n",
    "#audio_file = f\"{UPLOAD_PATH}test_1h.wav\"\n",
    "audio_file = f\"{UPLOAD_PATH}test_30mn.mp3\"\n",
    "#audio_info = prepare_audio_file(audio_file)\n",
    "\n",
    "print(f\"\\nğŸ“‚ Fichier dÃ©tectÃ© : {Path(audio_file).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b573fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.197043Z",
     "iopub.status.busy": "2025-10-28T12:26:51.196785Z",
     "iopub.status.idle": "2025-10-28T12:26:51.203692Z",
     "shell.execute_reply": "2025-10-28T12:26:51.203030Z"
    },
    "papermill": {
     "duration": 0.016204,
     "end_time": "2025-10-28T12:26:51.204814",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.188610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Point d'entrÃ©e principal\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"  PIPELINE TRANSCRIPTION ASSEMBLYAI\")\n",
    "    print(\"  RÃ©unions Conseil d'Administration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # VÃ©rification clÃ©s API\n",
    "    if not config.assemblyai_key:\n",
    "        print(\"\\nâŒ ERREUR : ASSEMBLYAI_API_KEY non configurÃ©e\")\n",
    "        print(\"   DÃ©finir dans les secrets Kaggle ou variables d'environnement\")\n",
    "        return\n",
    "    \n",
    "    # # Recherche fichier audio\n",
    "    # audio_files = list(Path(config.upload_path).glob(\"*.mp3\")) + \\\n",
    "    #               list(Path(config.upload_path).glob(\"*.wav\")) + \\\n",
    "    #               list(Path(config.upload_path).glob(\"*.m4a\"))\n",
    "    \n",
    "    # if not audio_files:\n",
    "    #     print(f\"\\nâŒ Aucun fichier audio trouvÃ© dans {config.upload_path}\")\n",
    "    #     print(\"   Formats supportÃ©s : .mp3, .wav, .m4a\")\n",
    "    #     return\n",
    "    \n",
    "    # audio_file = str(audio_files[0])\n",
    "    \n",
    "    # Analyse qualitÃ©\n",
    "    quality = analyze_audio_quality(audio_file)\n",
    "    \n",
    "    # Estimation coÃ»t\n",
    "    cost = estimate_cost(quality[\"duree_secondes\"], config.speech_model)\n",
    "    print(f\"\\nğŸ’° CoÃ»t estimÃ© : {cost['cout_usd']} (modÃ¨le: {cost['modele']})\")\n",
    "    \n",
    "    if not cost[\"gratuit\"]:\n",
    "        print(\"   ğŸ’¡ Utiliser speech_model='nano' pour tests gratuits\")\n",
    "    \n",
    "    # Lancer pipeline\n",
    "    result = transcribe_meeting(audio_file)\n",
    "    \n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(f\"\\nğŸ“Š RÃ‰SULTATS\")\n",
    "        print(f\"   DurÃ©e      : {int(result['duration'] // 60)}min {int(result['duration'] % 60)}s\")\n",
    "        print(f\"   Locuteurs  : {result['speakers']}\")\n",
    "        print(f\"   Confiance  : {result['metadata']['confiance_moyenne']}\")\n",
    "        print(f\"\\nğŸ“ Fichiers gÃ©nÃ©rÃ©s :\")\n",
    "        for file_type, path in result[\"files\"].items():\n",
    "            print(f\"   - {Path(path).name}\")\n",
    "        \n",
    "        # Proposition mapping locuteurs\n",
    "        with open(result[\"files\"][\"json\"], \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        mapping = create_speaker_mapping(data[\"transcription_detaillee\"])\n",
    "        \n",
    "        # Sauvegarder version avec mapping\n",
    "        data_mapped = apply_speaker_mapping(data, mapping)\n",
    "        mapped_path = Path(OUTPUT_PATH) / f\"transcription_mapped_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(mapped_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data_mapped, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nâœ… Version avec mapping sauvegardÃ©e : {mapped_path.name}\")\n",
    "        \n",
    "        # Afficher extrait\n",
    "        print(f\"\\nğŸ“ EXTRAIT (500 premiers caractÃ¨res) :\")\n",
    "        print(\"-\" * 60)\n",
    "        print(data[\"transcription_complete\"][:500] + \"...\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâŒ Ã‰chec : {result.get('error', 'Erreur inconnue')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d19740",
   "metadata": {
    "papermill": {
     "duration": 0.007088,
     "end_time": "2025-10-28T12:26:51.219382",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.212294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#**Analyse des RÃ©sultats de Debug**\n",
    "\n",
    "Les fichiers JSON de debug sont sauvegardÃ©s dans `/kaggle/working/debug_json/` avec le format:\n",
    "- `02_transcription_[timestamp].json` : RÃ©sultat brut de Whisper\n",
    "- `03_diarization_[timestamp].json` : AprÃ¨s identification des locuteurs\n",
    "- `04_postprocessing_[timestamp].json` : AprÃ¨s normalisation et dÃ©duplication\n",
    "- `05_llm_cleaning_[timestamp].json` : Version finale nettoyÃ©e par LLM\n",
    "\n",
    "Chaque fichier contient :\n",
    "- Un rÃ©sumÃ© (`summary`) avec aperÃ§u du texte et statistiques\n",
    "- Les mÃ©tadonnÃ©es de l'Ã©tape (`status`, `timestamp`)\n",
    "- Les donnÃ©es complÃ¨tes peuvent Ãªtre consultÃ©es dans le fichier principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79868ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.234957Z",
     "iopub.status.busy": "2025-10-28T12:26:51.234478Z",
     "iopub.status.idle": "2025-10-28T12:27:01.138768Z",
     "shell.execute_reply": "2025-10-28T12:27:01.137830Z"
    },
    "papermill": {
     "duration": 9.913377,
     "end_time": "2025-10-28T12:27:01.140015",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.226638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  PIPELINE TRANSCRIPTION ASSEMBLYAI\n",
      "  RÃ©unions Conseil d'Administration\n",
      "============================================================\n",
      "\n",
      "ğŸ” ANALYSE QUALITÃ‰ AUDIO\n",
      "============================================================\n",
      "DurÃ©e          : 32min 8s\n",
      "Sample rate    : 48000 Hz\n",
      "Niveau sonore  : 0.014\n",
      "Score qualitÃ©  : 100/100\n",
      "\n",
      "âœ… QualitÃ© acceptable\n",
      "============================================================\n",
      "\n",
      "ğŸ’° CoÃ»t estimÃ© : $0.48 (modÃ¨le: best)\n",
      "   ğŸ’¡ Utiliser speech_model='nano' pour tests gratuits\n",
      "\n",
      "[1/4] ğŸ”Š PRÃ‰TRAITEMENT AUDIO\n",
      "============================================================\n",
      "\n",
      "âŒ ERREUR PIPELINE : AudioPreprocessor.process() missing 1 required positional argument: 'output_dir'\n",
      "\n",
      "âŒ Ã‰chec : AudioPreprocessor.process() missing 1 required positional argument: 'output_dir'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_20/67426247.py\", line 26, in transcribe_meeting\n",
      "    clean_audio = preprocessor.process(audio_path)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: AudioPreprocessor.process() missing 1 required positional argument: 'output_dir'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8302666,
     "sourceId": 13136823,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39.51676,
   "end_time": "2025-10-28T12:27:02.870581",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-28T12:26:23.353821",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
