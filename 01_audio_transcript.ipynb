{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13136823,"sourceType":"datasetVersion","datasetId":8302666}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Installation des packages n√©cessaires**","metadata":{}},{"cell_type":"code","source":"%%capture\n# Installation silencieuse des d√©pendances avec gestion des conflits\n\n# 1. Mise √† jour pip pour √©viter les probl√®mes\n!pip install --upgrade pip -q\n\n# 2. Installation FFmpeg (syst√®me)\n!apt-get update -qq\n!apt-get install -qq ffmpeg sox\n\n# 3. Installation des packages de transcription\n!pip install -q openai-whisper==20250625\n!pip install -q faster-whisper==1.0.3\n\n# 4. Packages de d√©bruitage audio\n!pip install -q librosa==0.10.1\n!pip install -q soundfile==0.12.1\n!pip install -q noisereduce==3.0.0\n!pip install -q scipy==1.11.4\n!pip install -q pydub==0.25.1\n\n!pip -q install regex unidecode\n\n# 5. Diarization\n!pip -q install \"pyannote.audio>=3.1\" torch --index-url https://download.pytorch.org/whl/cu118\n!pip -q install whisperx\n\n# 5. Packages documents\n!pip install -q python-docx==1.2.0\n!pip install -q python-pptx==1.0.2\n\n# 6. Packages LLM et NLP\n!pip install -q openai==1.91.0\n!pip install -q assemblyai==0.44.3\n!pip install -q tiktoken==0.9.0\n\n# 7. LangChain\n!pip install -q langchain==0.3.27 langchain-community==0.3.29 langchain-core -q 2>/dev/null || true\n\n# 8. Packages utilitaires\n!pip install -q numpy\n!pip install -q pandas matplotlib seaborn\n\n# 9. Installation FAISS pour le RAG\n!pip install -q faiss-cpu==1.12.0\n\nprint(\"‚úÖ Installation termin√©e!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T12:16:06.011242Z","iopub.execute_input":"2025-09-24T12:16:06.011897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# V√©rification que tout est install√© correctement\nimport sys\nimport importlib\n\npackages_to_check = [\n    ('whisper', 'openai-whisper'),\n    ('faster_whisper', 'faster-whisper'),\n    ('librosa', 'librosa'),\n    ('soundfile', 'soundfile'),\n    ('noisereduce', 'noisereduce'),\n    ('scipy', 'scipy'),\n    ('pydub', 'pydub'),\n    ('docx', 'python-docx'),\n    ('pptx', 'python-pptx'),\n    ('openai', 'openai'),\n    ('langchain', 'langchain'),\n    ('langchain_community', 'langchain-community'),\n    ('faiss', 'faiss-cpu'),\n    ('assemblyai', 'assemblyai'),\n    ('tiktoken', 'tiktoken')\n]\n\nprint(\"üîç V√©rification des packages install√©s:\")\nprint(\"-\" * 50)\n\nall_ok = True\nfor import_name, package_name in packages_to_check:\n    try:\n        module = importlib.import_module(import_name)\n        version = getattr(module, '__version__', 'N/A')\n        print(f\"‚úÖ {package_name:20} : {version}\")\n    except ImportError:\n        print(f\"‚ùå {package_name:20} : Non install√©\")\n        all_ok = False\n\nif all_ok:\n    print(\"\\n‚ú® Tous les packages sont install√©s correctement!\")\nelse:\n    print(\"\\n‚ö†Ô∏è Certains packages manquent. Relancez la cellule 1.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Imports et configuration GPU**","metadata":{}},{"cell_type":"code","source":"# Imports standards\nimport os, sys, json, math, re, shutil, subprocess\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datetime import datetime, timezone\nimport time\ntry:\n    from zoneinfo import ZoneInfo\nexcept Exception:\n    ZoneInfo = None\n\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass, field\nimport gc  # Garbage collector\n\nimport numpy as np\nimport pandas as pd\n\n# Imports audio et d√©bruitage\nimport librosa\nimport soundfile as sf\nimport noisereduce as nr\nfrom scipy.signal import butter, filtfilt, medfilt\nfrom pydub import AudioSegment\n\n# Imports pour la transcription\nimport whisper\nfrom faster_whisper import WhisperModel\n\n# Imports pour les documents\nfrom docx import Document\nfrom pptx import Presentation\n\n# Imports pour le NLP et LLM\nimport openai\ntry:\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from langchain_community.vectorstores import FAISS\n    from langchain_community.embeddings import OpenAIEmbeddings\n    langchain_available = True\nexcept ImportError:\n    print(\"‚ö†Ô∏è LangChain non disponible\")\n    langchain_available = False\n\nimport torch\nprint(f\"üîß PyTorch: {torch.__version__}\")\nprint(f\"üéÆ CUDA disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Configuration des cl√©s API**","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\nASSEMBLYAI_API_KEY = user_secrets.get_secret(\"ASSEMBLYAI_API_KEY\")\nHUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Configuration des chemins**","metadata":{}},{"cell_type":"code","source":"UPLOAD_PATH = \"/kaggle/input/meeting-audio/\" # Chemin des fichiers upload√©s \nOUTPUT_PATH = \"/kaggle/working\" # Chemin de sortie\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Phase 1 : validation des configurations","metadata":{}},{"cell_type":"code","source":"PHASE1_DIR = f'{OUTPUT_PATH}/phase1_tests'\nos.makedirs(PHASE1_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Utilitaires de commande syst√®me**","metadata":{}},{"cell_type":"code","source":"def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True) #V√©rification cr√©ation de dossier\ndef run(cmd): # Lancement commande\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate(); return p.returncode, out.decode(), err.decode()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Configuration du pipeline**","metadata":{}},{"cell_type":"code","source":" @dataclass \nclass Config: \n    \"\"\"Configuration centralis√©e pour Kaggle\"\"\" \n    \n    timezone: str = \"Indian/Antananarivo\"\n    \n    # Mod√®le Whisper \n    whisper_model: str = \"large-v3\" # 'tiny', 'base', 'small', 'medium', 'large'\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    compute_type: str = \"float16\" if torch.cuda.is_available() else \"int8\"\n    \n    openai_model: str = \"gpt-3.5-turbo\" # Plus √©conomique que GPT-4 \n    \n    # Cl√©s API \n    openai_key: str = OPENAI_API_KEY \n    assemblyai_key: str = ASSEMBLYAI_API_KEY\n\n    # Param√®tres audio\n    # denoise_method: str = \"hybrid\"  # ffmpeg, noisereduce, hybrid\n    # denoise_aggressive: bool = True\n    sample_rate: int = 16000\n\n    # Param√®tres de traitement \n    \n    ## Longueur maximale d‚Äôun ‚Äúmorceau de texte‚Äù (chunk) qu‚Äôon d√©coupe avant d‚Äôenvoyer au LLM.\n    chunk_sec: int = 15\n    overlap_sec: int = 5\n    ## R√®gle : chunk_size ‚âà 20-30% de la capacit√© max du mod√®le.\n    #chunk_size: int = 1000 # nombre de caract√®re ‚âà 200‚Äì250 tokens (selon la langue et la densit√©) √† modifier selon la limitation du mod√®le choisie (ex. GPT-3.5 ‚âà 4k tokens, GPT-4 ‚âà 8k ou 32k).\n    \n    ## Nombre de caract√®res r√©p√©t√©s entre deux chunks.\n    ## R√®gle : overlap = 15-25% du chunk_size.\n    #chunk_overlap: int = 200 # nombre de caract√®re ‚âà 40 tokens. Suffisant pour garder la continuit√© (phrases coup√©es, dialogues, etc.).\n    \n    ## Proportion maximale de mots que le LLM a le droit de modifier dans une transcription brute.\n    ## R√®gle : plus l‚Äôaudio est bruit√©, plus tu tol√®res une correction √©lev√©e. [propre (dictaphone, micro-cravate) ‚Üí mettre bas (0.10 √† 0.15). / bruyant (claquements de porte, plusieurs intervenants) ‚Üí monter √† 0.20 voire 0.25]\n    #max_correction_rate: float = 0.15 # Max 15% du texte peut √™tre modifi√© (Pas de r√©√©criture compl√®te ‚Üí garde la fid√©lit√© au discours original.) Evite les hallucinations\n    \n    ## Score minimal de confiance (0‚Äì1) pour garder une phrase transcrite par Whisper/AssemblyAI.\n    #confidence_threshold: float = 0.85 #Segments dont la transcription est jug√©e correcte √† au moins 85%.\n\n    # Optimisation m√©moire pour Kaggle \n    num_workers: int = 2  # Ajust√© pour T4\n    batch_size: int = 4 # Pour le traitement par lots [Si CPU seulement ‚Üí descendre (1‚Äì2).]\n    use_gpu: bool = torch.cuda.is_available()\n\n    # NOUVEAUX PARAM√àTRES ANTI-HALLUCINATIONS\n    beam_size: int = 5  # Plus de beam = plus de pr√©cision\n    #best_of: int = 2    # Prendre le meilleur de 3 tentatives\n    #patience: float = 1.0\n    temperature: float = 0.0  # Pas de randomness\n    \n    # # Seuils de confiance stricts\n    # no_speech_threshold: float = 0.8 # Plus strict\n    # logprob_threshold: float = -0.5  # Plus strict\n    # compression_ratio_threshold: float = 2.8  # √âvite les r√©p√©titions\n\n    # # NOUVEAU: Param√®tres anti-r√©p√©tition\n    # max_initial_timestamp: float = 1.0\n    # suppress_blank: bool = True\n    # suppress_tokens: str = \"-1\"  # Supprime les tokens probl√©matiques\n    \n    # VAD (Voice Activity Detection) optimis√©\n    # use_vad: bool = True\n    vad_threshold: float = 0.5\n    # vad_min_speech_duration_ms: int = 500  # Minimum 250ms de parole\n    # vad_max_speech_duration_s: float = 60  # Max 30s par segment\n    # vad_min_silence_duration_ms: int = 1000  # 2s de silence minimum\n    # vad_speech_pad_ms: int = 400\n\n    # Post\n    max_repetitions: int = 3\n\n    # Phase 1\n    duration_limit: int = 600\n    \n    # Audio processing\n    sample_rate: int = 16000\n    # use_denoise: bool = \"auto\"  # auto, True, False\n    # denoise_stationary: float = 0.97\n    # denoise_prop_decrease: float = 1.0\n    \n    # # D√©tection r√©p√©titions\n    # repetition_penalty: float = 1.2  # NOUVEAU\n    # max_repetitions: int = 3  # NOUVEAU: max r√©p√©titions tol√©r√©es\n    \n    # # Prompt sp√©cialis√© CA - AM√âLIOR√â\n    # # PROMPT AM√âLIOR√â avec contexte financier malgache\n    # initial_prompt: str = (\n    #     \"Conseil d'administration Madagascar. Vocabulaire financier: Ariary, millions, \"\n    #     \"budget, rapport financier, r√©solution, d√©lib√©ration. \"\n    #     \"Termes sp√©cifiques: Fihariana, SON'INVEST, UNIMA, AQUALMA. \"\n    #     \"Intervenants: Pr√©sident, Directeur G√©n√©ral, Commissaire aux Comptes. \"\n    #     \"Format: discours naturel sans r√©p√©titions.\"\n    # )\n\nconfig = Config() \nprint(f\"‚úÖ Configuration charg√©e - Mod√®le Whisper: {config.whisper_model}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Pr√©paration de l'audio**","metadata":{}},{"cell_type":"markdown","source":"**Extrait**","metadata":{}},{"cell_type":"code","source":"def slice_audio(input_path: str, output_path: str, duration: int) -> str:\n    if duration <= 0:\n        return input_path\n    ensure_dir(Path(output_path).parent)\n    code, out, err = run(['ffmpeg','-y','-i',input_path,'-t',str(duration),'-ac','1','-ar',str(config.sample_rate),output_path])\n    if code!=0:\n        print('‚ö†Ô∏è FFmpeg slice error:', err); return input_path\n    return output_path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Pr√©traitraite l‚Äôaudio**","metadata":{}},{"cell_type":"markdown","source":"Nettoyage via FFmpeg (filtres audio rapides),\n\nR√©duction de bruit plus fine avec Python (librosa + noisereduce).","metadata":{}},{"cell_type":"code","source":"class AudioPreprocessor:\n    def __init__(self, sample_rate: int): self.sr = sample_rate\n    def ffmpeg_enhance(self, src: str, dst: str) -> str:\n        chain = 'highpass=f=100,lowpass=f=7500,adeclip,afftdn=nf=-25,compand=attacks=0.005:decays=0.05:points=-80/-90|-20/-20|0/-10:gain=5'\n        code,_,err = run(['ffmpeg','-y','-hide_banner','-loglevel','error','-i',src,'-ac','1','-ar',str(self.sr),'-af',chain,dst])\n        if code!=0: raise RuntimeError('FFmpeg failed: '+err)\n        return dst\n    def reduce_noise(self, src: str, dst: str) -> str:\n        y, sr = librosa.load(src, sr=self.sr)\n        y = nr.reduce_noise(y=y, sr=sr)\n        sf.write(dst, y, sr); return dst\n    def process(self, src: str, outdir: str) -> str:\n        ensure_dir(outdir)\n        ff = str(Path(outdir)/f'{Path(src).stem}_ffmpeg.wav')\n        dn = str(Path(outdir)/f'{Path(src).stem}_denoise.wav')\n        self.ffmpeg_enhance(src, ff)\n        return self.reduce_noise(ff, dn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_audio_file(audio_path: str) -> Dict:\n    \"\"\"Pr√©pare et valide le fichier audio pour la transcription\"\"\"\n    import wave\n    import contextlib\n    \n    file_info = {\n        \"path\": audio_path,\n        \"exists\": os.path.exists(audio_path),\n        \"size_mb\": 0,\n        \"duration_seconds\": 0,\n        \"format\": audio_path.split('.')[-1],\n        \"sample_rate\": 0,\n        \"channels\": 0\n    }\n    \n    if file_info[\"exists\"]:\n        file_info[\"size_mb\"] = os.path.getsize(audio_path) / (1024 * 1024)\n        \n        try:\n            # Charger avec librosa pour info\n            y, sr = librosa.load(audio_path, sr=None, duration=10)\n            file_info[\"sample_rate\"] = sr\n            \n            # Dur√©e totale\n            duration = librosa.get_duration(path=audio_path)\n            file_info[\"duration_seconds\"] = duration\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Erreur lecture audio: {e}\")\n    \n    return file_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Pr√©processing et D√©bruitage Audio**\n**Classe de d√©bruitage audio avanc√©**\n","metadata":{}},{"cell_type":"code","source":"class AudioPreprocessor:\n    def __init__(self, sample_rate: int): self.sr = sample_rate\n    def ffmpeg_enhance(self, src: str, dst: str) -> str:\n        chain = 'highpass=f=100,lowpass=f=7500,adeclip,afftdn=nf=-25,compand=attacks=0.005:decays=0.05:points=-80/-90|-20/-20|0/-10:gain=5'\n        code,_,err = run(['ffmpeg','-y','-hide_banner','-loglevel','error','-i',src,'-ac','1','-ar',str(self.sr),'-af',chain,dst])\n        if code!=0: raise RuntimeError('FFmpeg failed: '+err)\n        return dst\n    def reduce_noise(self, src: str, dst: str) -> str:\n        y, sr = librosa.load(src, sr=self.sr)\n        y = nr.reduce_noise(y=y, sr=sr)\n        sf.write(dst, y, sr); return dst\n    def process(self, src: str, outdir: str) -> str:\n        ensure_dir(outdir)\n        ff = str(Path(outdir)/f'{Path(src).stem}_ffmpeg.wav')\n        dn = str(Path(outdir)/f'{Path(src).stem}_denoise.wav')\n        self.ffmpeg_enhance(src, ff)\n        return self.reduce_noise(ff, dn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Transcription Audio**\n**Service de transcription avec audio nettoy√©**","metadata":{}},{"cell_type":"code","source":"class Transcriber:\n    def __init__(self, cfg: Config):\n        self.cfg = cfg; self.model=None\n    def load(self):\n        if self.model is None:\n            self.model = WhisperModel(self.cfg.whisper_model, device=self.cfg.device, compute_type=self.cfg.compute_type)\n        return self.model\n    def transcribe(self, wav_path: str) -> Dict[str, Any]:\n        m = self.load()\n        segs, info = m.transcribe(wav_path, language='fr', beam_size=self.cfg.beam_size,\n                                  temperature=self.cfg.temperature, vad_filter=True,\n                                  vad_parameters={'threshold': self.cfg.vad_threshold})\n        out=[]\n        for s in segs:\n            if float(getattr(s,'no_speech_prob',0.0))>0.9: continue\n            out.append({'start':float(s.start),'end':float(s.end),'text':s.text.strip()})\n        text=' '.join(x['text'] for x in out)\n        return {'status':'success','segments':out,'transcription':text}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***Comment r√©gler les param√®tres selon les cas***\n\nCas A ‚Äî Audio propre (dictaphones, salle calme)\n*  beam_size=3, best_of=1‚Äì2 (plus rapide)\n* no_speech_threshold=0.6 (ok)\n* temperature=0.0\n* VAD : min_silence_duration_ms=1500\n\nCas B ‚Äî Audio bruit√© (portes, brouhaha)\n* beam_size=5, best_of=5 (qualit√©)\n* baisser no_speech_threshold √† 0.5 si coupures\n* VAD : threshold=0.4‚Äì0.5, min_speech_duration_ms=200, min_silence_duration_ms=1800‚Äì2200\n* Garde-fous : garder compression_ratio_threshold=2.4\n\nCas C ‚Äî CPU-only (pas de GPU Kaggle)\n* compute_type=\"int8\", mod√®le tiny ou base\n* beam_size=3, best_of=1\n* Threads : cpu_threads=2, num_workers=1\n* Attends un RTF ‚âà 2‚Äì5 (selon longueur)","metadata":{}},{"cell_type":"code","source":"# Exemple d'utilisation\n#result = transcription_service.transcribe_audio(audio_file)\n#print(f\"Transcription: {result['transcription'][:500]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Diarization**","metadata":{}},{"cell_type":"code","source":"def diarize_stub(transcript: Dict[str,Any], wav_path: str, hf_token: Optional[str]):\n    try:\n        if not hf_token:\n            print('‚ÑπÔ∏è Pas de HUGGINGFACE_TOKEN ‚Üí diarisation ignor√©e (Phase 1).'); return transcript\n        import whisperx\n        align_model, meta = whisperx.load_align_model(language_code='fr', device=config.device)\n        aligned = whisperx.align([{k:v for k,v in s.items() if k in ('text','start','end')} for s in transcript['segments']],\n                                 align_model, meta, wav_path, device=config.device)\n        diar = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=config.device)\n        diar_segs = diar(wav_path)\n        result = whisperx.assign_word_speakers(diar_segs, aligned)\n        transcript['diarized_segments'] = result.get('segments',[])\n        return transcript\n    except Exception as e:\n        print('‚ö†Ô∏è Diarisation √©chou√©e:', e); return transcript","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Post-traitement du texte**","metadata":{}},{"cell_type":"code","source":"def postprocess_text(text: str) -> str:\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    seen=set(); out=[]\n    for s in sents:\n        k=s.lower()\n        if k in seen: continue\n        seen.add(k); out.append(s)\n    return '. '.join(out)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Test Phase 1**","metadata":{}},{"cell_type":"code","source":"def phase1_run(audio_file: str):\n    print('üöÄ Phase 1 ‚Äî extrait 5‚Äì10 min')\n    clip = str(Path(PHASE1_DIR)/f'clip_{Path(audio_file).stem}.wav')\n    clip = slice_audio(audio_file, clip, config.duration_limit)\n    pre = AudioPreprocessor(config.sample_rate).process(clip, PHASE1_DIR)\n    t = Transcriber(config).transcribe(pre)\n    if t.get('status')!='success': return t\n    t = diarize_stub(t, pre, HUGGINGFACE_TOKEN)\n    clean = postprocess_text(t.get('transcription',''))\n    t['transcription_postprocessed']=clean\n    out = Path(OUTPUT_PATH)/f\"phase1_result_{int(time.time())}.json\"\n    with open(out,'w',encoding='utf-8') as f: json.dump(t,f,ensure_ascii=False,indent=2)\n    print('üíæ Sauvegard√©:', out)\n    return t","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PHASE1_AUDIO_FILE = f\"{UPLOAD_PATH}test_30mn.mp3\"\n# R√©sultat (d√©commente pour ex√©cuter sur Kaggle)\nres = phase1_run(PHASE1_AUDIO_FILE)\nres.keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Fallback AssemblyAI (si √©chec Whisper)**","metadata":{}},{"cell_type":"code","source":"class AssemblyAIFallback:\n    \"\"\"Service de fallback avec AssemblyAI\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        \n    def transcribe_with_assemblyai(self, audio_path: str) -> Dict:\n        \"\"\"\n        Transcription de secours via AssemblyAI\n        \n        Args:\n            audio_path: Chemin du fichier audio\n            \n        Returns:\n            Dict avec la transcription\n        \"\"\"\n        if not self.api_key:\n            return {\n                \"status\": \"error\",\n                \"error\": \"Cl√© API AssemblyAI non configur√©e\"\n            }\n        \n        try:\n            import assemblyai as aai\n            \n            print(\"üîÑ Utilisation du fallback AssemblyAI...\")\n            \n            aai.settings.api_key = self.api_key\n            transcriber = aai.Transcriber()\n            \n            # Upload et transcription\n            config_lang = aai.TranscriptionConfig(\n                language_code=\"fr\",\n                punctuate=True,\n                format_text=True,\n                disfluencies=True,\n                speaker_labels=True\n            )\n            transcript = transcriber.transcribe(audio_path, config=config_lang)\n            \n            if transcript.status == aai.TranscriptStatus.error:\n                raise Exception(f\"Erreur AssemblyAI: {transcript.error}\")\n            \n            # Attente de la transcription\n            while transcript.status not in [aai.TranscriptStatus.completed, aai.TranscriptStatus.error]:\n                time.sleep(5)\n                transcript = transcriber.get_transcript(transcript.id)\n            \n            return {\n                \"status\": \"success\",\n                \"method\": \"assemblyai\",\n                \"transcription\": transcript.text,\n                \"confidence\": transcript.confidence if hasattr(transcript, 'confidence') else 0.85,\n                \"words\": transcript.words if hasattr(transcript, 'words') else []\n            }\n            \n        except Exception as e:\n            print(f\"‚ùå Erreur AssemblyAI: {str(e)}\")\n            return {\n                \"status\": \"error\",\n                \"error\": str(e),\n                \"method\": \"assemblyai\"\n            }\n\n# Service de fallback\nfallback_service = AssemblyAIFallback(config.assemblyai_key)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Par d√©faut, la langue est auto. Pour ton cas, force fran√ßais :\n        config = aai.TranscriptionConfig(language_code=\"fr\")\n2. Diarisation (orateurs)\n        config = aai.TranscriptionConfig(speaker_labels=True)\n\nExemple :\n    config = aai.TranscriptionConfig(language_code=\"fr\", speaker_labels=True)\n    transcript = transcriber.transcribe(audio_path, config=config)\n\nAppel :\n    Si TranscriptionService.transcribe_audio renvoie status=\"error\" ou un real_time_factor >> 5 (trop lent) ou trop de segments sous ton confidence_threshold, alors :\n        > result = fallback_service.transcribe_with_assemblyai(audio_path)","metadata":{}},{"cell_type":"markdown","source":"**Pipeline de transcription avec gestion automatique du fallback**","metadata":{}},{"cell_type":"code","source":"def transcribe_audio_pipeline(\n    audio_path: str, \n    config: Config,\n    force_denoise: Optional[bool] = None,\n    analyze_quality: bool = True\n) -> Dict:\n    \"\"\"\n    Pipeline complet de transcription avec analyse de qualit√©\n    \n    Args:\n        audio_path: Chemin du fichier audio\n        config: Configuration\n        force_denoise: Forcer le d√©bruitage (None=auto)\n        analyze_quality: Analyser la qualit√© apr√®s transcription\n    \"\"\"\n    \n    print(\"=\" * 70)\n    print(\"üéØ PIPELINE DE TRANSCRIPTION INTELLIGENT V2\")\n    print(\"=\" * 70)\n    \n    # Pr√©parer le fichier\n    file_info = prepare_audio_file(audio_path)\n    print(f\"üìÅ Fichier: {os.path.basename(audio_path)}\")\n    print(f\"   Format: {file_info['format']}\")\n    print(f\"   Dur√©e: {format_timestamp(file_info['duration_seconds'])}\")\n    print(f\"   Taille: {file_info['size_mb']:.1f} MB\")\n    \n    # Service de transcription\n    transcription_service = TranscriptionService(config)\n    \n    # D√©terminer si d√©bruitage n√©cessaire\n    if force_denoise is None:\n        force_denoise = \"auto\"\n    \n    # Transcription\n    result = transcription_service.transcribe_with_preprocessing(\n        audio_path,\n        preprocess=force_denoise,\n        language=\"fr\"\n    )\n    \n    # Analyse de qualit√©\n    if analyze_quality and result[\"status\"] == \"success\":\n        print(\"\\nüìä Analyse de la qualit√©...\")\n        analyzer = QualityAnalyzer(config)\n        quality = analyzer.analyze_transcription(result)\n        result[\"quality_analysis\"] = quality\n        \n        print(f\"   Score de qualit√©: {quality['quality_score']}/100\")\n        \n        if quality[\"quality_issues\"]:\n            print(\"   ‚ö†Ô∏è Probl√®mes d√©tect√©s:\")\n            for issue in quality[\"quality_issues\"]:\n                print(f\"      - {issue}\")\n        \n        if quality[\"repetitions\"]:\n            print(\"   üîÑ R√©p√©titions excessives:\")\n            for word, data in list(quality[\"repetitions\"].items())[:3]:\n                print(f\"      - '{word}': {data['count']} fois ({data['ratio']:.1%})\")\n    \n    # Sauvegarder le r√©sultat\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_file = f\"{config.output_dir}/transcription_{timestamp}.json\"\n    \n    # Convertir les types NumPy en types Python natifs pour JSON\n    def convert_numpy_types(obj):\n        \"\"\"Convertit r√©cursivement les types NumPy en types Python natifs\"\"\"\n        import numpy as np\n        \n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, (np.bool_, bool)):\n            return bool(obj)\n        elif isinstance(obj, dict):\n            return {key: convert_numpy_types(value) for key, value in obj.items()}\n        elif isinstance(obj, list):\n            return [convert_numpy_types(item) for item in obj]\n        else:\n            return obj\n    \n    # Nettoyer le r√©sultat avant sauvegarde\n    result_clean = convert_numpy_types(result)\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(result_clean, f, ensure_ascii=False, indent=2)\n    \n    print(f\"\\nüíæ R√©sultat sauvegard√©: {output_file}\")\n    \n    # R√©sum√© final\n    if result[\"status\"] == \"success\":\n        print(\"\\n\" + \"=\" * 70)\n        print(\"‚úÖ TRANSCRIPTION R√âUSSIE\")\n        print(\"=\" * 70)\n        print(f\"üìù M√©thode: Whisper {config.whisper_model}\")\n        print(f\"üìä Confiance: {result.get('confidence', 0):.2%}\")\n        print(f\"üìë Segments: {len(result.get('segments', []))}\")\n        print(f\"üìÑ Longueur: {len(result.get('transcription', ''))} caract√®res\")\n        \n        if analyze_quality:\n            print(f\"‚≠ê Qualit√©: {result['quality_analysis']['quality_score']}/100\")\n        \n        # Aper√ßu\n        text = result.get('transcription', '')\n        if text:\n            print(f\"\\nüìñ Aper√ßu (300 premiers caract√®res):\")\n            print(f\"   {text[:300]}...\")\n    else:\n        print(f\"\\n‚ùå √âchec transcription: {result.get('error')}\")\n    \n    return result","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test avec votre fichier audio\n#audio_file = f\"{UPLOAD_PATH}atelier.mp3\"\n#audio_file = f\"{UPLOAD_PATH}test_1h.wav\"\naudio_file = f\"{UPLOAD_PATH}test_30mn.mp3\"\n#audio_info = prepare_audio_file(audio_file)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}