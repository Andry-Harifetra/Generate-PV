{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12de406",
   "metadata": {
    "papermill": {
     "duration": 0.00807,
     "end_time": "2025-10-28T12:26:27.334381",
     "exception": false,
     "start_time": "2025-10-28T12:26:27.326311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transcription CA\n",
    "\n",
    "Ce notebook applique le **pipeline complet** :\n",
    "- **Prétraitement** audio (FFmpeg + noisereduce)\n",
    "- **Transcription** faster-whisper (réglages anti-hallucinations)\n",
    "- **Chunks longs** pour une meilleure cohérence (3–5 min)\n",
    "- **Diarisation** (pyannote → fallback whisperx)\n",
    "- **Post-traitement** (dédup + normalisation chiffres/unités)\n",
    "- **Nettoyage LLM** par morceaux (1000 caractères) avec borne de correction\n",
    "- **Sauvegarde JSON** des sorties (raw, diarized, cleaned, llm_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e0ddc",
   "metadata": {
    "papermill": {
     "duration": 0.007518,
     "end_time": "2025-10-28T12:26:27.348842",
     "exception": false,
     "start_time": "2025-10-28T12:26:27.341324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Installation des packages nécessaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5020c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:27.363730Z",
     "iopub.status.busy": "2025-10-28T12:26:27.363521Z",
     "iopub.status.idle": "2025-10-28T12:26:27.367904Z",
     "shell.execute_reply": "2025-10-28T12:26:27.367262Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013245,
     "end_time": "2025-10-28T12:26:27.369117",
     "exception": false,
     "start_time": "2025-10-28T12:26:27.355872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Installation silencieuse des dépendances avec gestion des conflits\n",
    "\n",
    "# # 1. Mise à jour pip pour éviter les problèmes\n",
    "# #!pip install --upgrade pip -q\n",
    "\n",
    "# # 2. Installation FFmpeg (système)\n",
    "# !apt-get update -qq\n",
    "# !apt-get install -qq ffmpeg sox\n",
    "\n",
    "# # 3. Nettoyage et verrouillage de la stack NumPy/Numba/Scipy\n",
    "# #!pip uninstall -y numpy numba >/dev/null 2>&1 || true\n",
    "# !pip install -q numpy==1.26.4 scipy==1.11.4\n",
    "# !pip install -q numba==0.58.1\n",
    "# !pip install -q torch==2.1.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# # 4. Installation des packages de transcription\n",
    "# #!pip install -q openai-whisper==20231117\n",
    "# !pip install -q faster-whisper==1.0.3\n",
    "\n",
    "# # 5. Packages de débruitage audio\n",
    "# !pip install -q librosa==0.10.1\n",
    "# !pip install -q soundfile==0.12.1\n",
    "# !pip install -q noisereduce==3.0.0\n",
    "# !pip install -q pydub==0.25.1\n",
    "\n",
    "# # 6. Diarization\n",
    "# !pip install -q \"pyannote.audio>=3.1\"\n",
    "# !pip install -q whisperx\n",
    "\n",
    "# !pip install -q regex==2023.12.25 unidecode==1.3.8\n",
    "\n",
    "# # 7. Packages documents\n",
    "# !pip install -q python-docx==1.2.0\n",
    "# !pip install -q python-pptx==1.0.2\n",
    "\n",
    "# # 8. Packages LLM et NLP\n",
    "# !pip install -q openai==1.91.0\n",
    "# !pip install -q assemblyai==0.44.3\n",
    "# !pip install -q tiktoken==0.9.0\n",
    "\n",
    "# # 9. LangChain\n",
    "# #!pip install -q langchain==0.3.27 langchain-community==0.3.29 langchain-core==0.3.30\n",
    "\n",
    "# # 10. Packages utilitaires\n",
    "# !pip install -q pandas==2.1.4 matplotlib==3.8.2 seaborn==0.13.2\n",
    "\n",
    "# # 11. Installation FAISS pour le RAG\n",
    "# #!pip install -q faiss-cpu==1.7.4\n",
    "\n",
    "# print(\"✅ Installation terminée!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b67cfc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:27.383014Z",
     "iopub.status.busy": "2025-10-28T12:26:27.382813Z",
     "iopub.status.idle": "2025-10-28T12:26:36.247300Z",
     "shell.execute_reply": "2025-10-28T12:26:36.246462Z"
    },
    "papermill": {
     "duration": 8.872936,
     "end_time": "2025-10-28T12:26:36.248693",
     "exception": false,
     "start_time": "2025-10-28T12:26:27.375757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 38.6 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.5/51.5 kB 268.2 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.8/135.8 kB 289.6 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 324.4 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.6/38.6 MB 225.3 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.4/17.4 MB 281.4 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 264.9 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 290.4 MB/s eta 0:00:00\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Installation minimale des dépendances nécessaires sans perturber l'environnement Kaggle\n",
    "import importlib\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def ensure_packages(requirements):\n",
    "    missing = []\n",
    "    for module_name, package_spec in requirements:\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "        except Exception:\n",
    "            missing.append(package_spec)\n",
    "    if missing:\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '-q'] + missing\n",
    "        subprocess.check_call(cmd)\n",
    "\n",
    "core_requirements = [\n",
    "    ('faster_whisper', 'faster-whisper==1.0.3'),\n",
    "    ('librosa', 'librosa==0.10.1'),\n",
    "    ('soundfile', 'soundfile==0.12.1'),\n",
    "    ('noisereduce', 'noisereduce==3.0.0'),\n",
    "    # ('pydub', 'pydub==0.25.1'),\n",
    "    # ('docx', 'python-docx==1.2.0'),\n",
    "    # ('pptx', 'python-pptx==1.0.2'),\n",
    "    # ('openai', 'openai==1.91.0'),\n",
    "    #('assemblyai', 'assemblyai==0.44.3'),\n",
    "    ('assemblyai', 'assemblyai==0.45.4'),\n",
    "    # ('tiktoken', 'tiktoken==0.9.0'),\n",
    "    ('groq', 'groq==0.33.0'),\n",
    "]\n",
    "\n",
    "ensure_packages(core_requirements)\n",
    "\n",
    "# if os.environ.get('INSTALL_LANGCHAIN', '0') == '1':\n",
    "#     optional_requirements = [\n",
    "#         ('langchain', 'langchain==0.3.27'),\n",
    "#         ('langchain_community', 'langchain-community==0.3.29'),\n",
    "#         ('faiss', 'faiss-cpu==1.7.4'),\n",
    "#     ]\n",
    "#     try:\n",
    "#         ensure_packages(optional_requirements)\n",
    "#     except subprocess.CalledProcessError:\n",
    "#         pass\n",
    "\n",
    "if not shutil.which('ffmpeg'):\n",
    "    subprocess.check_call(['apt-get', 'update', '-qq'])\n",
    "    subprocess.check_call(['apt-get', 'install', '-qq', 'ffmpeg'])\n",
    "\n",
    "print('✅ Vérification des dépendances terminée')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c9d2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:36.264688Z",
     "iopub.status.busy": "2025-10-28T12:26:36.263991Z",
     "iopub.status.idle": "2025-10-28T12:26:49.917502Z",
     "shell.execute_reply": "2025-10-28T12:26:49.916831Z"
    },
    "papermill": {
     "duration": 13.662313,
     "end_time": "2025-10-28T12:26:49.918603",
     "exception": false,
     "start_time": "2025-10-28T12:26:36.256290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Vérification des packages installés:\n",
      "--------------------------------------------------\n",
      "✅ numpy                : 1.26.4\n",
      "✅ scipy                : 1.15.3\n",
      "✅ numba                : 0.60.0\n",
      "❌ openai-whisper       : Non installé\n",
      "✅ faster-whisper       : 1.0.3\n",
      "✅ librosa              : 0.11.0\n",
      "✅ soundfile            : 0.13.1\n",
      "✅ noisereduce          : N/A\n",
      "✅ pydub                : N/A\n",
      "❌ python-docx          : Non installé\n",
      "❌ python-pptx          : Non installé\n",
      "✅ openai               : 1.91.0\n",
      "✅ langchain            : 0.3.26\n",
      "❌ langchain-community  : Non installé\n",
      "❌ faiss-cpu            : Non installé\n",
      "✅ assemblyai           : 0.45.4\n",
      "✅ tiktoken             : 0.9.0\n",
      "✅ groq                 : 0.33.0\n",
      "⚠️ Certains packages nécessitent une attention. Consultez les messages ci-dessus.\n"
     ]
    }
   ],
   "source": [
    "# Vérification que tout est installé correctement\n",
    "import importlib\n",
    "\n",
    "packages_to_check = [\n",
    "    ('numpy', 'numpy'),\n",
    "    ('scipy', 'scipy'),\n",
    "    ('numba', 'numba'),\n",
    "    ('whisper', 'openai-whisper'),\n",
    "    ('faster_whisper', 'faster-whisper'),\n",
    "    ('librosa', 'librosa'),\n",
    "    ('soundfile', 'soundfile'),\n",
    "    ('noisereduce', 'noisereduce'),\n",
    "    ('pydub', 'pydub'),\n",
    "    ('docx', 'python-docx'),\n",
    "    ('pptx', 'python-pptx'),\n",
    "    ('openai', 'openai'),\n",
    "    ('langchain', 'langchain'),\n",
    "    ('langchain_community', 'langchain-community'),\n",
    "    ('faiss', 'faiss-cpu'),\n",
    "    ('assemblyai', 'assemblyai'),\n",
    "    ('tiktoken', 'tiktoken'),\n",
    "    ('groq', 'groq')\n",
    "]\n",
    "\n",
    "print(\"🔍 Vérification des packages installés:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_ok = True\n",
    "for import_name, package_name in packages_to_check:\n",
    "    try:\n",
    "        module = importlib.import_module(import_name)\n",
    "        version = getattr(module, '__version__', 'N/A')\n",
    "        print(f\"✅ {package_name:20} : {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {package_name:20} : Non installé\")\n",
    "        all_ok = False\n",
    "    except Exception as exc:\n",
    "        print(f\"⚠️ {package_name:20} : Erreur lors de l'import ({type(exc).__name__}: {exc})\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"✨ Tous les packages sont installés correctement!\")\n",
    "else:\n",
    "    print(\"⚠️ Certains packages nécessitent une attention. Consultez les messages ci-dessus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e733a",
   "metadata": {
    "papermill": {
     "duration": 0.007535,
     "end_time": "2025-10-28T12:26:49.934287",
     "exception": false,
     "start_time": "2025-10-28T12:26:49.926752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Imports et configuration GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0679b707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:49.950484Z",
     "iopub.status.busy": "2025-10-28T12:26:49.950144Z",
     "iopub.status.idle": "2025-10-28T12:26:50.075745Z",
     "shell.execute_reply": "2025-10-28T12:26:50.075015Z"
    },
    "papermill": {
     "duration": 0.134857,
     "end_time": "2025-10-28T12:26:50.076793",
     "exception": false,
     "start_time": "2025-10-28T12:26:49.941936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 PyTorch: 2.6.0+cu124\n",
      "🎮 CUDA disponible: True\n",
      "   GPU: Tesla T4\n",
      "   Mémoire: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# Imports standards\n",
    "import os, sys, json, re, shutil, subprocess, tempfile\n",
    "from math import ceil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import gc  # Garbage collector\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Imports audio et débruitage\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "# from scipy.signal import butter, filtfilt, medfilt\n",
    "# from pydub import AudioSegment\n",
    "from groq import Groq\n",
    "\n",
    "# Imports pour la transcription\n",
    "import assemblyai as aai\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# # Imports pour les documents\n",
    "# from docx import Document\n",
    "# from pptx import Presentation\n",
    "\n",
    "# # Imports pour le NLP et LLM\n",
    "# import openai\n",
    "# try:\n",
    "#     from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#     from langchain_community.vectorstores import FAISS\n",
    "#     from langchain_community.embeddings import OpenAIEmbeddings\n",
    "#     langchain_available = True\n",
    "# except ImportError:\n",
    "#     print(\"⚠️ LangChain non disponible\")\n",
    "#     langchain_available = False\n",
    "\n",
    "import torch\n",
    "print(f\"🔧 PyTorch: {torch.__version__}\")\n",
    "print(f\"🎮 CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Mémoire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5450c1",
   "metadata": {
    "papermill": {
     "duration": 0.006817,
     "end_time": "2025-10-28T12:26:50.091133",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.084316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Configuration des clés API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc54e34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.106137Z",
     "iopub.status.busy": "2025-10-28T12:26:50.105908Z",
     "iopub.status.idle": "2025-10-28T12:26:50.386506Z",
     "shell.execute_reply": "2025-10-28T12:26:50.385992Z"
    },
    "papermill": {
     "duration": 0.289518,
     "end_time": "2025-10-28T12:26:50.387591",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.098073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    OPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\n",
    "    ASSEMBLYAI_API_KEY = user_secrets.get_secret(\"ASSEMBLYAI_API_KEY\")\n",
    "    HUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "except:\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "    ASSEMBLYAI_API_KEY = os.environ.get(\"ASSEMBLYAI_API_KEY\", \"\")\n",
    "    HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", \"\")\n",
    "    GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cad90f",
   "metadata": {
    "papermill": {
     "duration": 0.006823,
     "end_time": "2025-10-28T12:26:50.401550",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.394727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Configuration des chemins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e792d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.416691Z",
     "iopub.status.busy": "2025-10-28T12:26:50.416477Z",
     "iopub.status.idle": "2025-10-28T12:26:50.419405Z",
     "shell.execute_reply": "2025-10-28T12:26:50.418919Z"
    },
    "papermill": {
     "duration": 0.011797,
     "end_time": "2025-10-28T12:26:50.420426",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.408629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "UPLOAD_PATH = \"/kaggle/input/meeting-audio/\" # Chemin des fichiers uploadés \n",
    "OUTPUT_PATH = \"/kaggle/working\" # Chemin de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e56277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.435215Z",
     "iopub.status.busy": "2025-10-28T12:26:50.435005Z",
     "iopub.status.idle": "2025-10-28T12:26:50.438326Z",
     "shell.execute_reply": "2025-10-28T12:26:50.437788Z"
    },
    "papermill": {
     "duration": 0.011879,
     "end_time": "2025-10-28T12:26:50.439348",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.427469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG_PATH = Path(OUTPUT_PATH) / \"debug\"\n",
    "DEBUG_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124b5eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.455844Z",
     "iopub.status.busy": "2025-10-28T12:26:50.455425Z",
     "iopub.status.idle": "2025-10-28T12:26:50.458941Z",
     "shell.execute_reply": "2025-10-28T12:26:50.458474Z"
    },
    "papermill": {
     "duration": 0.013283,
     "end_time": "2025-10-28T12:26:50.460024",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.446741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMP_DIR = Path(OUTPUT_PATH) / \"temp_chunks\"\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def cleanup_temp_files():\n",
    "    \"\"\"Nettoyer les fichiers temporaires\"\"\"\n",
    "    if TEMP_DIR.exists():\n",
    "        shutil.rmtree(TEMP_DIR)\n",
    "    TEMP_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46d610",
   "metadata": {
    "papermill": {
     "duration": 0.007217,
     "end_time": "2025-10-28T12:26:50.474343",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.467126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Utilitaires de commande système**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe40d410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.489563Z",
     "iopub.status.busy": "2025-10-28T12:26:50.489179Z",
     "iopub.status.idle": "2025-10-28T12:26:50.492979Z",
     "shell.execute_reply": "2025-10-28T12:26:50.492323Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.012637,
     "end_time": "2025-10-28T12:26:50.494101",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.481464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensure_dir(p): \n",
    "    Path(p).mkdir(parents=True, exist_ok=True) #Vérification création de dossier\n",
    "    \n",
    "def run(cmd): # Lancement commande\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "    return p.returncode, out.decode(), err.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6533e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.509351Z",
     "iopub.status.busy": "2025-10-28T12:26:50.508783Z",
     "iopub.status.idle": "2025-10-28T12:26:50.512430Z",
     "shell.execute_reply": "2025-10-28T12:26:50.511902Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.012272,
     "end_time": "2025-10-28T12:26:50.513492",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.501220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        free, total = torch.cuda.mem_get_info()\n",
    "        print(f\"📊 GPU: {free/1e9:.2f}GB libres / {total/1e9:.2f}GB total\")\n",
    "        if free < 4e9:  # Moins de 4GB libres\n",
    "            print(\"⚠️ Mémoire GPU faible, utilisation de 'base' recommandée\")\n",
    "            return \"medium\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a3ed3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.528686Z",
     "iopub.status.busy": "2025-10-28T12:26:50.528284Z",
     "iopub.status.idle": "2025-10-28T12:26:50.535179Z",
     "shell.execute_reply": "2025-10-28T12:26:50.534677Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015557,
     "end_time": "2025-10-28T12:26:50.536154",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.520597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_debug_json(data: Dict, step_name: str, timestamp: Optional[str] = None) -> str:\n",
    "    \"\"\"Sauvegarde JSON de debug pour chaque étape\"\"\"\n",
    "    if not config.save_intermediate_json:\n",
    "        return \"\"\n",
    "    \n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    filename = f\"{step_name}_{timestamp}.json\"\n",
    "    filepath = DEBUG_PATH / filename\n",
    "    \n",
    "    # Créer un résumé pour les données volumineuses\n",
    "    debug_data = {\n",
    "        \"step\": step_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"status\": data.get(\"status\", \"unknown\"),\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    if \"segments\" in data and isinstance(data[\"segments\"], list):\n",
    "        debug_data[\"summary\"][\"total_segments\"] = len(data[\"segments\"])\n",
    "        debug_data[\"summary\"][\"sample_segments\"] = data[\"segments\"][:3] if data[\"segments\"] else []\n",
    "        debug_data[\"segments_count\"] = len(data[\"segments\"])\n",
    "    \n",
    "    if \"transcription\" in data:\n",
    "        debug_data[\"summary\"][\"text_length\"] = len(data[\"transcription\"])\n",
    "        debug_data[\"summary\"][\"text_preview\"] = data[\"transcription\"][:500] + \"...\" if len(data[\"transcription\"]) > 500 else data[\"transcription\"]\n",
    "    \n",
    "    if \"transcription_postprocessed\" in data:\n",
    "        debug_data[\"summary\"][\"postprocessed_length\"] = len(data[\"transcription_postprocessed\"])\n",
    "        debug_data[\"summary\"][\"postprocessed_preview\"] = data[\"transcription_postprocessed\"][:500] + \"...\"\n",
    "    \n",
    "    if \"transcription_llm\" in data:\n",
    "        debug_data[\"summary\"][\"llm_length\"] = len(data[\"transcription_llm\"])\n",
    "        debug_data[\"summary\"][\"llm_preview\"] = data[\"transcription_llm\"][:500] + \"...\"\n",
    "        debug_data[\"llm_correction_rate\"] = data.get(\"llm_correction_rate\", 0)\n",
    "    \n",
    "    # Ajouter les métadonnées complètes\n",
    "    debug_data[\"full_data_keys\"] = list(data.keys())\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(debug_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"📁 Debug JSON sauvé: {filepath}\")\n",
    "    return str(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cb688",
   "metadata": {
    "papermill": {
     "duration": 0.00702,
     "end_time": "2025-10-28T12:26:50.550268",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.543248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Monitoring et debug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e984f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.564975Z",
     "iopub.status.busy": "2025-10-28T12:26:50.564522Z",
     "iopub.status.idle": "2025-10-28T12:26:50.568357Z",
     "shell.execute_reply": "2025-10-28T12:26:50.567822Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.012259,
     "end_time": "2025-10-28T12:26:50.569397",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.557138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_memory_usage(step_name: str = \"\"):\n",
    "    \"\"\"Afficher l'utilisation mémoire\"\"\"\n",
    "    prefix = f\"[{step_name}] \" if step_name else \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"{prefix}GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB / {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    print(f\"{prefix}RAM Usage: {process.memory_info().rss / 1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837698e2",
   "metadata": {
    "papermill": {
     "duration": 0.006761,
     "end_time": "2025-10-28T12:26:50.583181",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.576420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Configuration du pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc809eff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.639859Z",
     "iopub.status.busy": "2025-10-28T12:26:50.639261Z",
     "iopub.status.idle": "2025-10-28T12:26:50.644272Z",
     "shell.execute_reply": "2025-10-28T12:26:50.643703Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013761,
     "end_time": "2025-10-28T12:26:50.645280",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.631519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimal_model_size() -> str:\n",
    "    \"\"\"\n",
    "    Détermine automatiquement la taille du modèle Whisper selon les ressources.\n",
    "    Adapté du projet SIIS pour une meilleure gestion mémoire.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            total_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "        except:\n",
    "            total_mem = 0\n",
    "        \n",
    "        if total_mem >= 12:\n",
    "            return \"large-v3\"\n",
    "        elif total_mem >= 8:\n",
    "            return \"medium\"\n",
    "        elif total_mem >= 4:\n",
    "            return \"small\"\n",
    "        else:\n",
    "            return \"base\"\n",
    "    \n",
    "    # Mode CPU\n",
    "    if psutil is not None:\n",
    "        try:\n",
    "            ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "        except:\n",
    "            ram_gb = 0\n",
    "    else:\n",
    "        ram_gb = 8  # Défaut conservateur\n",
    "    \n",
    "    if ram_gb >= 16:\n",
    "        return \"small\"\n",
    "    elif ram_gb >= 8:\n",
    "        return \"base\"\n",
    "    else:\n",
    "        return \"tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308885cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.660107Z",
     "iopub.status.busy": "2025-10-28T12:26:50.659910Z",
     "iopub.status.idle": "2025-10-28T12:26:50.666884Z",
     "shell.execute_reply": "2025-10-28T12:26:50.666121Z"
    },
    "papermill": {
     "duration": 0.015706,
     "end_time": "2025-10-28T12:26:50.667928",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.652222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration: AssemblyAI best | Device: cuda\n"
     ]
    }
   ],
   "source": [
    "@dataclass \n",
    "class Config: \n",
    "    \"\"\"Configuration centralisée pour Kaggle\"\"\" \n",
    "    \n",
    "    timezone: str = \"Indian/Antananarivo\"\n",
    "\n",
    "    # Debug\n",
    "    debug_mode: bool = True\n",
    "    save_intermediate_json: bool = True\n",
    "    \n",
    "    # Clés API \n",
    "    openai_key: str = OPENAI_API_KEY \n",
    "    assemblyai_key: str = ASSEMBLYAI_API_KEY\n",
    "    huggingface_token: str = HUGGINGFACE_TOKEN\n",
    "    \n",
    "    # # Whisper\n",
    "    # whisper_model: str = get_optimal_model_size() # 'tiny', 'base', 'small', 'medium', 'large', \"large-v3\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # compute_type: str = \"float16\" if torch.cuda.is_available() else \"int8\"\n",
    "    # num_workers: int = 4\n",
    "\n",
    "    # AssemblyAI\n",
    "    speech_model: str = \"best\"  # \"nano\" (gratuit) | \"best\" (production)\n",
    "    speakers_expected: int = 10  # Nombre de participants à la réunion\n",
    "    \n",
    "    # Audio\n",
    "    sample_rate: int = 16000\n",
    "\n",
    "    # Vocabulaire métier\n",
    "    word_boost: List[str] = None #['Ariary','Fihariana','Unima','Aqualma','rapport financier','budget']\n",
    "    \n",
    "    # # Decoding / anti-hallucination\n",
    "    # beam_size: int = 5\n",
    "    # best_of: int = 5\n",
    "    # patience: float = 1.0\n",
    "    # temperature: float = 0.0\n",
    "    # compression_ratio_threshold: float = 2.4\n",
    "    # log_prob_threshold: float = -1.0\n",
    "    # no_speech_threshold: float = 0.6\n",
    "    # condition_on_previous_text: bool = False\n",
    "    # suppress_blank: bool = True\n",
    "    # suppress_tokens: list[int] = field(default_factory=lambda: [-1])\n",
    "    # max_initial_timestamp: float = 1.0\n",
    "    \n",
    "    # # VAD\n",
    "    # use_vad: bool = True\n",
    "    # vad_threshold: float = 0.5\n",
    "    # vad_min_speech_duration_ms: int = 250\n",
    "    # vad_max_speech_duration_s: float = float('inf')\n",
    "    # vad_min_silence_duration_ms: int = 2000\n",
    "    # vad_speech_pad_ms: int = 400\n",
    "    \n",
    "    # # Chunks longs pour cohérence (3–5 min)\n",
    "    # chunk_length_s: int = 900\n",
    "    # chunk_overlap_s: int = 30\n",
    "    \n",
    "    # # Post-traitement\n",
    "    # max_repetitions: int = 3\n",
    "    \n",
    "    # # Prompt spécialisé\n",
    "    # initial_prompt: str = (\n",
    "    #     \"Transcription d'une réunion du conseil d'administration à Madagascar. \"\n",
    "    #     \"Vocabulaire: conseil d'administration, procès-verbal, quorum, \"\n",
    "    #     \"résolution, délibération, vote, ordre du jour, budget, \"\n",
    "    #     \"millions d'Ariary, rapport financier. \"\n",
    "    #     \"Termes spécifiques: Fihariana, SON'INVEST, UNIMA, AQUALMA. \"\n",
    "    #     \"Format: discours naturel sans répétitions ni hallucinations.\"\n",
    "    # )\n",
    "    \n",
    "    # LLM (activé par défaut en production)\n",
    "    enable_llm: bool = True\n",
    "    use_groq: bool = True\n",
    "    groq_model: str = \"llama-3.3-70b-versatile\"  # ou \"llama-3.3-70b-versatile\"\n",
    "    #openai_model: str = \"gpt-4o-mini\" # \"gpt-3.5-turbo\" : Plus économique que GPT-4 # Fallback\n",
    "    max_correction_rate: float = 0.20\n",
    "    chunk_size_chars: int = 1500\n",
    "    #chunk_overlap_chars: int = 200\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.word_boost is None:\n",
    "            self.word_boost = [\n",
    "                \"Fihariana\", \"SON'INVEST\", \"UNIMA\", \"AQUALMA\",\n",
    "                \"Ariary\", \"procès-verbal\", \"quorum\", \"résolution\",\n",
    "                \"conseil d'administration\", \"ordre du jour\",\"rapport financier\",\"budget\"\n",
    "            ]\n",
    "\n",
    "config = Config() \n",
    "\n",
    "print(f\"✅ Configuration: AssemblyAI {config.speech_model} | Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc036d",
   "metadata": {
    "papermill": {
     "duration": 0.006807,
     "end_time": "2025-10-28T12:26:50.681899",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.675092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Comment régler les paramètres selon les cas***\n",
    "\n",
    "Cas A — Audio propre (dictaphones, salle calme)\n",
    "*  beam_size=3, best_of=1–2 (plus rapide)\n",
    "* no_speech_threshold=0.6 (ok)\n",
    "* temperature=0.0\n",
    "* VAD : min_silence_duration_ms=1500\n",
    "\n",
    "Cas B — Audio bruité (portes, brouhaha)\n",
    "* beam_size=5, best_of=5 (qualité)\n",
    "* baisser no_speech_threshold à 0.5 si coupures\n",
    "* VAD : threshold=0.4–0.5, min_speech_duration_ms=200, min_silence_duration_ms=1800–2200\n",
    "* Garde-fous : garder compression_ratio_threshold=2.4\n",
    "\n",
    "Cas C — CPU-only (pas de GPU Kaggle)\n",
    "* compute_type=\"int8\", modèle tiny ou base\n",
    "* beam_size=3, best_of=1\n",
    "* Threads : cpu_threads=2, num_workers=1\n",
    "* Attends un RTF ≈ 2–5 (selon longueur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b632e",
   "metadata": {
    "papermill": {
     "duration": 0.006892,
     "end_time": "2025-10-28T12:26:50.695822",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.688930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Préparation de l'audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f499668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.710984Z",
     "iopub.status.busy": "2025-10-28T12:26:50.710624Z",
     "iopub.status.idle": "2025-10-28T12:26:50.715166Z",
     "shell.execute_reply": "2025-10-28T12:26:50.714541Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013183,
     "end_time": "2025-10-28T12:26:50.716150",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.702967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slice_audio(input_path: str, output_path: str, start: float = 0.0, duration: Optional[int] = None) -> str:\n",
    "    args = [\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\"-ss\",str(start),\"-i\",input_path,\"-ac\",\"1\",\"-ar\",str(config.sample_rate)]\n",
    "    if duration and duration > 0:\n",
    "        args += [\"-t\",str(duration)]\n",
    "    args += [output_path]\n",
    "    ensure_dir(str(Path(output_path).parent))\n",
    "    code, _, err = run(args)\n",
    "    if code!=0:\n",
    "        raise RuntimeError(\"FFmpeg slice failed: \" + err)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5a708d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.731510Z",
     "iopub.status.busy": "2025-10-28T12:26:50.731211Z",
     "iopub.status.idle": "2025-10-28T12:26:50.744678Z",
     "shell.execute_reply": "2025-10-28T12:26:50.744209Z"
    },
    "papermill": {
     "duration": 0.022309,
     "end_time": "2025-10-28T12:26:50.745668",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.723359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_audio_quality(audio_path: str) -> Dict:\n",
    "    \"\"\"Analyse qualité audio avant transcription\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 ANALYSE QUALITÉ AUDIO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    y, sr = librosa.load(audio_path, sr=None, duration=60)  # Premier minute\n",
    "    \n",
    "    # Métriques\n",
    "    duration = librosa.get_duration(path=audio_path)\n",
    "    rms = librosa.feature.rms(y=y)[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "    \n",
    "    # Score qualité simplifié\n",
    "    avg_rms = rms.mean()\n",
    "    avg_zcr = zcr.mean()\n",
    "    \n",
    "    quality_score = 0\n",
    "    issues = []\n",
    "    \n",
    "    # Vérification niveau sonore\n",
    "    if avg_rms < 0.01:\n",
    "        issues.append(\"Volume très faible (préamplification recommandée)\")\n",
    "    elif avg_rms > 0.5:\n",
    "        issues.append(\"Possible saturation audio\")\n",
    "    else:\n",
    "        quality_score += 30\n",
    "    \n",
    "    # Vérification bruit\n",
    "    if avg_zcr > 0.15:\n",
    "        issues.append(\"Bruit de fond élevé (débruitage recommandé)\")\n",
    "    else:\n",
    "        quality_score += 30\n",
    "    \n",
    "    # Durée\n",
    "    if duration > 7200:  # >2h\n",
    "        issues.append(\"Fichier très long (>2h) - surveillance recommandée\")\n",
    "        quality_score += 20\n",
    "    else:\n",
    "        quality_score += 40\n",
    "    \n",
    "    result = {\n",
    "        \"duree_totale\": f\"{int(duration // 60)}min {int(duration % 60)}s\",\n",
    "        \"duree_secondes\": duration,\n",
    "        \"sample_rate\": sr,\n",
    "        \"niveau_sonore\": f\"{avg_rms:.3f}\",\n",
    "        \"taux_bruit\": f\"{avg_zcr:.3f}\",\n",
    "        \"score_qualite\": f\"{quality_score}/100\",\n",
    "        \"problemes\": issues,\n",
    "        \"recommandation\": \"✅ Qualité acceptable\" if quality_score >= 60 else \"⚠️ Prétraitement fortement recommandé\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Durée          : {result['duree_totale']}\")\n",
    "    print(f\"Sample rate    : {sr} Hz\")\n",
    "    print(f\"Niveau sonore  : {result['niveau_sonore']}\")\n",
    "    print(f\"Score qualité  : {result['score_qualite']}\")\n",
    "    print(f\"\\n{result['recommandation']}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n⚠️  Problèmes détectés :\")\n",
    "        for issue in issues:\n",
    "            print(f\"   - {issue}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def estimate_cost(duration_seconds: float, model: str = \"best\") -> Dict:\n",
    "    \"\"\"Estime le coût AssemblyAI\"\"\"\n",
    "    \n",
    "    # Tarifs AssemblyAI (2025)\n",
    "    rates = {\n",
    "        \"nano\": 0.0,  # Gratuit\n",
    "        \"best\": 0.00025  # $0.00025/seconde\n",
    "    }\n",
    "    \n",
    "    rate = rates.get(model, 0.00025)\n",
    "    cost_usd = duration_seconds * rate\n",
    "    cost_eur = cost_usd * 0.92  # Conversion approximative\n",
    "    \n",
    "    return {\n",
    "        \"duree\": f\"{int(duration_seconds // 60)}min {int(duration_seconds % 60)}s\",\n",
    "        \"modele\": model,\n",
    "        \"cout_usd\": f\"${cost_usd:.2f}\",\n",
    "        \"cout_eur\": f\"€{cost_eur:.2f}\",\n",
    "        \"gratuit\": model == \"nano\"\n",
    "    }\n",
    "\n",
    "def create_speaker_mapping(utterances: List[Dict]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Aide interactive pour renommer les locuteurs\n",
    "    SPEAKER_A → \"Président\", etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    speakers = sorted(set(u[\"speaker\"] for u in utterances))\n",
    "    \n",
    "    print(\"\\n👥 IDENTIFICATION DES LOCUTEURS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Détectés : {', '.join(speakers)}\")\n",
    "    print(\"\\nAperçu des premières interventions :\\n\")\n",
    "    \n",
    "    # Afficher premières phrases de chaque locuteur\n",
    "    shown = set()\n",
    "    for u in utterances:\n",
    "        if u[\"speaker\"] not in shown:\n",
    "            print(f\"[{u['speaker']}] \\\"{u['text'][:100]}...\\\"\")\n",
    "            shown.add(u[\"speaker\"])\n",
    "            if len(shown) == len(speakers):\n",
    "                break\n",
    "    \n",
    "    print(\"\\n💡 Suggestions de mapping :\")\n",
    "    suggestions = {\n",
    "        \"SPEAKER_A\": \"Président\",\n",
    "        \"SPEAKER_B\": \"Directeur Général\",\n",
    "        \"SPEAKER_C\": \"Directeur Financier\",\n",
    "        \"SPEAKER_D\": \"Participant 4\",\n",
    "        \"SPEAKER_E\": \"Participant 5\"\n",
    "    }\n",
    "    \n",
    "    mapping = {}\n",
    "    for speaker in speakers:\n",
    "        default = suggestions.get(speaker, speaker)\n",
    "        print(f\"   {speaker} → {default}\")\n",
    "        mapping[speaker] = default\n",
    "    \n",
    "    print(\"\\n📝 Appliquer ce mapping ? (Modifier manuellement dans le JSON si nécessaire)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "def apply_speaker_mapping(data: Dict, mapping: Dict[str, str]) -> Dict:\n",
    "    \"\"\"Applique le renommage des locuteurs\"\"\"\n",
    "    \n",
    "    for utterance in data.get(\"utterances\", []):\n",
    "        original = utterance[\"speaker\"]\n",
    "        if original in mapping:\n",
    "            utterance[\"speaker\"] = mapping[original]\n",
    "    \n",
    "    for utterance in data.get(\"transcription_detaillee\", []):\n",
    "        original = utterance[\"speaker\"]\n",
    "        if original in mapping:\n",
    "            utterance[\"speaker\"] = mapping[original]\n",
    "    \n",
    "    # Mettre à jour stats locuteurs\n",
    "    if \"statistiques_locuteurs\" in data:\n",
    "        new_stats = {}\n",
    "        for speaker, stats in data[\"statistiques_locuteurs\"].items():\n",
    "            new_name = mapping.get(speaker, speaker)\n",
    "            new_stats[new_name] = stats\n",
    "        data[\"statistiques_locuteurs\"] = new_stats\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d284b90",
   "metadata": {
    "papermill": {
     "duration": 0.007092,
     "end_time": "2025-10-28T12:26:50.759978",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.752886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Préprocessing et Débruitage Audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f212a8c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.775285Z",
     "iopub.status.busy": "2025-10-28T12:26:50.775110Z",
     "iopub.status.idle": "2025-10-28T12:26:50.780930Z",
     "shell.execute_reply": "2025-10-28T12:26:50.780467Z"
    },
    "papermill": {
     "duration": 0.014739,
     "end_time": "2025-10-28T12:26:50.781916",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.767177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioPreprocessor:\n",
    "    \"\"\"Prétraitement audio avec FFmpeg et réduction de bruit\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate: int = 16000):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def ffmpeg_enhance(self, input_path: str, output_path: str):\n",
    "        \"\"\"Améliore l'audio avec FFmpeg\"\"\"\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "            \"-i\", input_path,\n",
    "            \"-ac\", \"1\",  # Mono\n",
    "            \"-ar\", str(self.sample_rate),  # 16kHz\n",
    "            \"-af\", \"highpass=f=200,lowpass=f=3000,afftdn=nf=-20\",  # Filtres\n",
    "            output_path\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True)\n",
    "    \n",
    "    def reduce_noise(self, input_path: str, output_path: str):\n",
    "        \"\"\"Réduit le bruit avec noisereduce\"\"\"\n",
    "        y, sr = librosa.load(input_path, sr=self.sample_rate)\n",
    "        y_clean = nr.reduce_noise(y=y, sr=sr, stationary=True, prop_decrease=0.8)\n",
    "        sf.write(output_path, y_clean, sr)\n",
    "        return output_path\n",
    "    \n",
    "    def process(self, input_path: str, output_dir: str) -> str:\n",
    "        \"\"\"Pipeline complet de prétraitement\"\"\"\n",
    "        base_name = Path(input_path).stem\n",
    "        ffmpeg_path = str(Path(output_dir) / f\"{base_name}_ffmpeg.wav\")\n",
    "        clean_path = str(Path(output_dir) / f\"{base_name}_clean.wav\")\n",
    "\n",
    "        print(\"🔊 Prétraitement audio...\")\n",
    "        self.ffmpeg_enhance(input_path, str(ffmpeg_path))\n",
    "        self.reduce_noise(str(ffmpeg_path), str(clean_path))\n",
    "        \n",
    "        # Nettoyage fichier intermédiaire\n",
    "        # if Path(ffmpeg_path).exists():\n",
    "        #     Path(ffmpeg_path).unlink()\n",
    "        ffmpeg_path.unlink(missing_ok=True)\n",
    "        \n",
    "        return str(clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef1c3a25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.796888Z",
     "iopub.status.busy": "2025-10-28T12:26:50.796718Z",
     "iopub.status.idle": "2025-10-28T12:26:50.801432Z",
     "shell.execute_reply": "2025-10-28T12:26:50.800764Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013432,
     "end_time": "2025-10-28T12:26:50.802561",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.789129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_audio_file(audio_path: str) -> Dict:\n",
    "    \"\"\"Prépare et valide le fichier audio pour la transcription\"\"\"\n",
    "    file_info = {\n",
    "        \"path\": audio_path,\n",
    "        \"exists\": os.path.exists(audio_path),\n",
    "        \"size_mb\": 0,\n",
    "        \"duration_seconds\": 0,\n",
    "        \"format\": audio_path.split('.')[-1],\n",
    "        \"sample_rate\": 0,\n",
    "        \"channels\": 0\n",
    "    }\n",
    "    \n",
    "    if file_info[\"exists\"]:\n",
    "        file_info[\"size_mb\"] = os.path.getsize(audio_path) / (1024 * 1024)\n",
    "        \n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None, duration=10)\n",
    "            file_info[\"sample_rate\"] = sr\n",
    "            duration = librosa.get_duration(path=audio_path)\n",
    "            file_info[\"duration_seconds\"] = duration\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur lecture audio: {e}\")\n",
    "    \n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d3888",
   "metadata": {
    "papermill": {
     "duration": 0.007015,
     "end_time": "2025-10-28T12:26:50.816791",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.809776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Transcription Audio**\n",
    "**Service de transcription avec audio nettoyé**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061350ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.831994Z",
     "iopub.status.busy": "2025-10-28T12:26:50.831783Z",
     "iopub.status.idle": "2025-10-28T12:26:50.844703Z",
     "shell.execute_reply": "2025-10-28T12:26:50.844191Z"
    },
    "papermill": {
     "duration": 0.021682,
     "end_time": "2025-10-28T12:26:50.845618",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.823936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TranscriptionService:\n",
    "    \"\"\"Service de transcription avec configurations SIIS optimisées\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.model = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Charge le modèle Whisper avec gestion mémoire\"\"\"\n",
    "        if self.model is None:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            self.model = WhisperModel(\n",
    "                self.cfg.whisper_model,\n",
    "                device=self.cfg.device,\n",
    "                compute_type=self.cfg.compute_type,\n",
    "                num_workers=self.cfg.num_workers,  # 4 au lieu de 1\n",
    "                cpu_threads=4 if self.cfg.device == \"cpu\" else 0\n",
    "            )\n",
    "        return self.model\n",
    "    \n",
    "    def unload_model(self):\n",
    "        \"\"\"Libère le modèle de la mémoire\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    def transcribe_chunk(self, audio_path: str) -> Tuple[List, Dict]:\n",
    "        \"\"\"Transcrit un chunk audio\"\"\"\n",
    "        model = self.load_model()\n",
    "        \n",
    "        segments, info = model.transcribe(\n",
    "            audio_path,\n",
    "            language=\"fr\",\n",
    "            beam_size=self.cfg.beam_size,\n",
    "            best_of=self.cfg.best_of,\n",
    "            patience=self.cfg.patience,\n",
    "            temperature=self.cfg.temperature,\n",
    "            compression_ratio_threshold=self.cfg.compression_ratio_threshold,\n",
    "            log_prob_threshold=self.cfg.log_prob_threshold,\n",
    "            no_speech_threshold=self.cfg.no_speech_threshold,\n",
    "            condition_on_previous_text=self.cfg.condition_on_previous_text,  # TRUE!\n",
    "            initial_prompt=self.cfg.initial_prompt,\n",
    "            word_timestamps=True,\n",
    "            suppress_tokens=self.cfg.suppress_tokens,\n",
    "            suppress_blank=self.cfg.suppress_blank,\n",
    "            max_initial_timestamp=self.cfg.max_initial_timestamp,\n",
    "            vad_filter=self.cfg.use_vad,\n",
    "            vad_parameters={\n",
    "                \"threshold\": self.cfg.vad_threshold,\n",
    "                \"min_speech_duration_ms\": self.cfg.vad_min_speech_duration_ms,\n",
    "                \"max_speech_duration_s\": self.cfg.vad_max_speech_duration_s,\n",
    "                \"min_silence_duration_ms\": self.cfg.vad_min_silence_duration_ms,\n",
    "                \"speech_pad_ms\": self.cfg.vad_speech_pad_ms,\n",
    "            } if self.cfg.use_vad else None\n",
    "        )\n",
    "        \n",
    "        return list(segments), info\n",
    "    \n",
    "    def transcribe_long_audio(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Transcrit un audio long avec chunking optimisé SIIS\n",
    "        Chunks de 900s au lieu de 180-300s pour moins de dérive\n",
    "        \"\"\"\n",
    "        # Obtenir la durée totale\n",
    "        y, sr = librosa.load(audio_path, sr=self.cfg.sample_rate, duration=1)\n",
    "        info = sf.info(audio_path)\n",
    "        total_duration = info.duration\n",
    "        \n",
    "        # Calcul des chunks\n",
    "        chunk_length = self.cfg.chunk_length_s\n",
    "        chunk_overlap = self.cfg.chunk_overlap_s\n",
    "        num_chunks = max(1, ceil(total_duration / chunk_length))\n",
    "        \n",
    "        print(f\"📊 Audio: {total_duration:.1f}s | {num_chunks} chunks de {chunk_length}s\")\n",
    "        \n",
    "        all_segments = []\n",
    "        all_text = []\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_time = max(0, i * chunk_length - (chunk_overlap if i > 0 else 0))\n",
    "            duration = min(chunk_length + chunk_overlap, total_duration - start_time)\n",
    "            \n",
    "            # Extraire le chunk avec ffmpeg\n",
    "            chunk_path = str(TEMP_DIR / f\"chunk_{i:04d}.wav\")\n",
    "            cmd = [\n",
    "                \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "                \"-ss\", str(start_time),\n",
    "                \"-t\", str(duration),\n",
    "                \"-i\", audio_path,\n",
    "                \"-ac\", \"1\",\n",
    "                \"-ar\", str(self.cfg.sample_rate),\n",
    "                chunk_path\n",
    "            ]\n",
    "            subprocess.run(cmd, check=True)\n",
    "            \n",
    "            # Transcrire le chunk\n",
    "            print(f\"  Chunk {i+1}/{num_chunks}: {start_time:.1f}s - {start_time+duration:.1f}s\")\n",
    "            segments, chunk_info = self.transcribe_chunk(chunk_path)\n",
    "            \n",
    "            # Ajuster les timestamps\n",
    "            for seg in segments:\n",
    "                # Créer un nouveau dictionnaire pour chaque segment\n",
    "                segment_dict = {\n",
    "                    \"start\": seg.start + start_time,\n",
    "                    \"end\": seg.end + start_time,\n",
    "                    \"text\": seg.text.strip(),\n",
    "                }\n",
    "                \n",
    "                # Ajouter les mots avec timestamps ajustés si disponibles\n",
    "                if hasattr(seg, 'words') and seg.words:\n",
    "                    segment_dict[\"words\"] = [\n",
    "                        {\n",
    "                            \"start\": w.start + start_time,\n",
    "                            \"end\": w.end + start_time,\n",
    "                            \"word\": w.word,\n",
    "                            \"probability\": getattr(w, 'probability', 0.0)\n",
    "                        }\n",
    "                        for w in seg.words\n",
    "                    ]\n",
    "                \n",
    "                # Ajouter d'autres métadonnées si disponibles\n",
    "                if hasattr(seg, 'no_speech_prob'):\n",
    "                    segment_dict[\"no_speech_prob\"] = seg.no_speech_prob\n",
    "                if hasattr(seg, 'avg_logprob'):\n",
    "                    segment_dict[\"avg_logprob\"] = seg.avg_logprob\n",
    "                if hasattr(seg, 'compression_ratio'):\n",
    "                    segment_dict[\"compression_ratio\"] = seg.compression_ratio\n",
    "                \n",
    "                all_segments.append(segment_dict)\n",
    "                all_text.append(seg.text.strip())\n",
    "            \n",
    "            # Nettoyer le chunk temporaire\n",
    "            Path(chunk_path).unlink()\n",
    "        \n",
    "        # Assembler le résultat\n",
    "        result = {\n",
    "            \"status\": \"success\",\n",
    "            \"duration\": total_duration,\n",
    "            \"language\": \"fr\",\n",
    "            \"segments\": all_segments,\n",
    "            \"text\": \" \".join(all_text),\n",
    "            \"metadata\": {\n",
    "                \"model\": self.cfg.whisper_model,\n",
    "                \"chunks\": num_chunks,\n",
    "                \"chunk_length\": chunk_length,\n",
    "                \"condition_on_previous\": self.cfg.condition_on_previous_text\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16711249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.860737Z",
     "iopub.status.busy": "2025-10-28T12:26:50.860364Z",
     "iopub.status.idle": "2025-10-28T12:26:50.863450Z",
     "shell.execute_reply": "2025-10-28T12:26:50.862781Z"
    },
    "papermill": {
     "duration": 0.011667,
     "end_time": "2025-10-28T12:26:50.864445",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.852778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "#result = transcription_service.transcribe_audio(audio_file)\n",
    "#print(f\"Transcription: {result['transcription'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938d2e3",
   "metadata": {
    "papermill": {
     "duration": 0.006956,
     "end_time": "2025-10-28T12:26:50.878466",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.871510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Diarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30599b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.893648Z",
     "iopub.status.busy": "2025-10-28T12:26:50.893452Z",
     "iopub.status.idle": "2025-10-28T12:26:50.900500Z",
     "shell.execute_reply": "2025-10-28T12:26:50.899858Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015813,
     "end_time": "2025-10-28T12:26:50.901471",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.885658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def diarize(transcription_data: Dict, audio_path: str, hf_token: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Diarisation avec pyannote (ou fallback whisperx)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pyannote.audio import Pipeline\n",
    "        \n",
    "        print(\"🎙️ Diarisation avec pyannote...\")\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        \n",
    "        diarization = pipeline(audio_path)\n",
    "        \n",
    "        # Mapper les segments aux locuteurs\n",
    "        segments_with_speakers = []\n",
    "        for seg in transcription_data.get(\"segments\", []):\n",
    "            start, end = seg[\"start\"], seg[\"end\"]\n",
    "            \n",
    "            # Trouver le locuteur majoritaire pour ce segment\n",
    "            speaker_times = {}\n",
    "            for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "                overlap_start = max(start, turn.start)\n",
    "                overlap_end = min(end, turn.end)\n",
    "                if overlap_start < overlap_end:\n",
    "                    overlap_duration = overlap_end - overlap_start\n",
    "                    speaker_times[speaker] = speaker_times.get(speaker, 0) + overlap_duration\n",
    "            \n",
    "            # Assigner le locuteur avec le plus de temps de parole\n",
    "            if speaker_times:\n",
    "                main_speaker = max(speaker_times, key=speaker_times.get)\n",
    "                seg[\"speaker\"] = main_speaker\n",
    "            else:\n",
    "                seg[\"speaker\"] = \"Unknown\"\n",
    "            \n",
    "            segments_with_speakers.append(seg)\n",
    "        \n",
    "        transcription_data[\"segments_diarized\"] = segments_with_speakers\n",
    "        transcription_data[\"diarization_method\"] = \"pyannote\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Diarisation pyannote échouée: {e}\")\n",
    "        \n",
    "        # Fallback sur whisperx si disponible\n",
    "        try:\n",
    "            import whisperx\n",
    "            print(\"🔄 Fallback sur whisperx...\")\n",
    "            \n",
    "            # Aligner avec whisperx\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            align_model, metadata = whisperx.load_align_model(\n",
    "                language_code=\"fr\",\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            result_aligned = whisperx.align(\n",
    "                transcription_data[\"segments\"],\n",
    "                align_model,\n",
    "                metadata,\n",
    "                audio_path,\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Diarisation\n",
    "            diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token)\n",
    "            diarize_segments = diarize_model(audio_path)\n",
    "            result_diarized = whisperx.assign_word_speakers(diarize_segments, result_aligned)\n",
    "            \n",
    "            transcription_data[\"segments_diarized\"] = result_diarized[\"segments\"]\n",
    "            transcription_data[\"diarization_method\"] = \"whisperx\"\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"⚠️ Diarisation whisperx échouée: {e2}\")\n",
    "            transcription_data[\"diarization_method\"] = \"none\"\n",
    "    \n",
    "    return transcription_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7b0dd",
   "metadata": {
    "papermill": {
     "duration": 0.007049,
     "end_time": "2025-10-28T12:26:50.915698",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.908649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Post-traitement du texte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e91343bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.930652Z",
     "iopub.status.busy": "2025-10-28T12:26:50.930464Z",
     "iopub.status.idle": "2025-10-28T12:26:50.935580Z",
     "shell.execute_reply": "2025-10-28T12:26:50.935101Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013628,
     "end_time": "2025-10-28T12:26:50.936546",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.922918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_numbers_and_units(text: str) -> str:\n",
    "    \"\"\"Normalise les nombres et unités monétaires\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Normaliser les millions\n",
    "    text = re.sub(r'(\\d+)\\s*,\\s*(\\d+)\\s*millions?', r'\\1.\\2 millions', text)\n",
    "    text = re.sub(r'(\\d+)\\s*virgule\\s*(\\d+)\\s*millions?', r'\\1.\\2 millions', text)\n",
    "    \n",
    "    # Ajouter Ariary si manquant après les montants\n",
    "    text = re.sub(r'(\\d+(?:\\.\\d+)?)\\s*millions?\\s*(?!d\\'?[Aa]riary)', r'\\1 millions d\\'Ariary', text)\n",
    "    \n",
    "    # Normaliser les pourcentages\n",
    "    text = re.sub(r'(\\d+)\\s*pour\\s*cent', r'\\1%', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def deduplicate_sentences(text: str) -> str:\n",
    "    \"\"\"Supprime les répétitions de phrases\"\"\"\n",
    "    import re\n",
    "    \n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    seen = set()\n",
    "    unique_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent_lower = sent.lower().strip()\n",
    "        if sent_lower and sent_lower not in seen:\n",
    "            seen.add(sent_lower)\n",
    "            unique_sentences.append(sent)\n",
    "    \n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def postprocess_text(text: str) -> str:\n",
    "    # Nettoyer les espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = normalize_numbers_and_units(text)\n",
    "    text = deduplicate_sentences(text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a054a3c",
   "metadata": {
    "papermill": {
     "duration": 0.007124,
     "end_time": "2025-10-28T12:26:50.950988",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.943864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Transcription AssemblyAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaeb331f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:50.966522Z",
     "iopub.status.busy": "2025-10-28T12:26:50.966351Z",
     "iopub.status.idle": "2025-10-28T12:26:50.974194Z",
     "shell.execute_reply": "2025-10-28T12:26:50.973730Z"
    },
    "papermill": {
     "duration": 0.016978,
     "end_time": "2025-10-28T12:26:50.975204",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.958226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AssemblyAITranscriber:\n",
    "    \"\"\"Service de fallback avec AssemblyAI\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        aai.settings.api_key = cfg.assemblyai_key\n",
    "        \n",
    "        self.config = aai.TranscriptionConfig(\n",
    "            language_code=\"fr\",\n",
    "            speaker_labels=True,\n",
    "            speakers_expected=cfg.speakers_expected,\n",
    "            punctuate=True,\n",
    "            format_text=True,\n",
    "            disfluencies=True,\n",
    "            word_boost=cfg.word_boost,\n",
    "            speech_model=cfg.speech_model,\n",
    "            auto_chapters=True  # Découpage thématique\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def transcribe(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Transcription avec polling jusqu'à completion\n",
    "        \n",
    "        Returns:\n",
    "            Dict avec text, utterances (locuteur + timestamps), metadata\n",
    "        \"\"\"\n",
    "        print(\"📝 Transcription AssemblyAI...\")\n",
    "        print(f\"   Modèle: {self.cfg.speech_model}\")\n",
    "        \n",
    "        transcriber = aai.Transcriber(config=self.config)\n",
    "        transcript = transcriber.transcribe(audio_path)\n",
    "        \n",
    "        # Attente completion (polling toutes les 5s)\n",
    "        start_time = time.time()\n",
    "        while transcript.status not in [\n",
    "            aai.TranscriptStatus.completed,\n",
    "            aai.TranscriptStatus.error\n",
    "        ]:\n",
    "            elapsed = int(time.time() - start_time)\n",
    "            print(f\"   Traitement en cours... ({elapsed}s)\", end=\"\\r\")\n",
    "            time.sleep(5)\n",
    "            transcript = transcriber.get_transcript(transcript.id)\n",
    "        \n",
    "        if transcript.status == aai.TranscriptStatus.error:\n",
    "            raise Exception(f\"Erreur AssemblyAI: {transcript.error}\")\n",
    "        \n",
    "        print(f\"\\n✅ Transcription terminée ({int(time.time() - start_time)}s)\")\n",
    "        \n",
    "        # Structurer output\n",
    "        result = {\n",
    "            \"status\": \"success\",\n",
    "            \"text\": transcript.text,\n",
    "            \"duration\": transcript.audio_duration / 1000,  # ms → secondes\n",
    "            \"confidence\": transcript.confidence,\n",
    "            \"utterances\": [\n",
    "                {\n",
    "                    \"speaker\": u.speaker,\n",
    "                    \"text\": u.text.strip(),\n",
    "                    \"start\": u.start / 1000,\n",
    "                    \"end\": u.end / 1000,\n",
    "                    \"confidence\": u.confidence,\n",
    "                    \"words\": [\n",
    "                        {\n",
    "                            \"word\": w.text,\n",
    "                            \"start\": w.start / 1000,\n",
    "                            \"end\": w.end / 1000,\n",
    "                            \"confidence\": w.confidence\n",
    "                        }\n",
    "                        for w in u.words\n",
    "                    ]\n",
    "                }\n",
    "                for u in transcript.utterances\n",
    "            ],\n",
    "            \"chapters\": [\n",
    "                {\n",
    "                    \"summary\": c.summary,\n",
    "                    \"headline\": c.headline,\n",
    "                    \"start\": c.start / 1000,\n",
    "                    \"end\": c.end / 1000\n",
    "                }\n",
    "                for c in (transcript.chapters or [])\n",
    "            ] if hasattr(transcript, 'chapters') else [],\n",
    "            \"metadata\": {\n",
    "                \"model\": self.cfg.speech_model,\n",
    "                \"speakers_detected\": len(set(u.speaker for u in transcript.utterances)),\n",
    "                \"provider\": \"AssemblyAI\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09dd54",
   "metadata": {
    "papermill": {
     "duration": 0.006922,
     "end_time": "2025-10-28T12:26:50.989646",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.982724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ** FALLBACK WHISPER **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea66cb6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.005247Z",
     "iopub.status.busy": "2025-10-28T12:26:51.004544Z",
     "iopub.status.idle": "2025-10-28T12:26:51.008498Z",
     "shell.execute_reply": "2025-10-28T12:26:51.007865Z"
    },
    "papermill": {
     "duration": 0.012691,
     "end_time": "2025-10-28T12:26:51.009532",
     "exception": false,
     "start_time": "2025-10-28T12:26:50.996841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WhisperFallback:\n",
    "    \"\"\"Fallback si AssemblyAI échoue (quota, timeout)\"\"\"\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Utilise faster-whisper large-v3 (code existant)\"\"\"\n",
    "        print(\"⚠️ Fallback sur Whisper (AssemblyAI indisponible)\")\n",
    "        \n",
    "        # Importer votre TranscriptionService existant\n",
    "        # return existing_transcription_service.transcribe_long_audio(audio_path)\n",
    "        \n",
    "        # Placeholder pour exemple\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": \"Fallback Whisper non implémenté (conserver code existant)\",\n",
    "            \"text\": \"\",\n",
    "            \"utterances\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb5335",
   "metadata": {
    "papermill": {
     "duration": 0.00723,
     "end_time": "2025-10-28T12:26:51.024125",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.016895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Nettoyage LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe0d9357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.039503Z",
     "iopub.status.busy": "2025-10-28T12:26:51.039145Z",
     "iopub.status.idle": "2025-10-28T12:26:51.049396Z",
     "shell.execute_reply": "2025-10-28T12:26:51.048751Z"
    },
    "papermill": {
     "duration": 0.019343,
     "end_time": "2025-10-28T12:26:51.050542",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.031199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLMCleaner:\n",
    "    \"\"\"Nettoyage du texte avec LLM (Groq ou OpenAI)\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.client = None\n",
    "        \n",
    "        if cfg.use_groq and GROQ_API_KEY:\n",
    "            try:\n",
    "                from groq import Groq\n",
    "                self.client = Groq(api_key=GROQ_API_KEY)\n",
    "                self.provider = \"groq\"\n",
    "                print(\"✅ Utilisation de Groq pour le nettoyage LLM\")\n",
    "            except ImportError:\n",
    "                print(\"⚠️ Package groq non installé, installation...\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"groq\"])\n",
    "                from groq import Groq\n",
    "                self.client = Groq(api_key=GROQ_API_KEY)\n",
    "                self.provider = \"groq\"\n",
    "        elif OPENAI_API_KEY:\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "            self.provider = \"openai\"\n",
    "            print(\"✅ Utilisation d'OpenAI pour le nettoyage LLM\")\n",
    "        else:\n",
    "            print(\"⚠️ Aucune clé API LLM disponible\")\n",
    "    \n",
    "    def _create_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"Découpe le texte en chunks pour traitement LLM avec overlap\"\"\"\n",
    "        chunks = []\n",
    "        chunk_size = self.cfg.chunk_size_chars\n",
    "        overlap = 200\n",
    "        \n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunks.append(text[i:i + chunk_size])\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def clean_text(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"Nettoie le texte avec le LLM\"\"\"\n",
    "        if not self.client or not text:\n",
    "            return text, 0.0\n",
    "        \n",
    "        chunks = self._create_chunks(text)\n",
    "        cleaned_chunks = []\n",
    "        total_delta = 0\n",
    "        \n",
    "        system_prompt = \"\"\"Tu es un assistant de correction de transcription.\n",
    "            Tu corriges UNIQUEMENT : orthographe, grammaire, ponctuation, noms propres malgaches.\n",
    "            RÈGLES STRICTES :\n",
    "            1. NE JAMAIS ajouter d'information non présente\n",
    "            2. NE PAS changer le sens des phrases\n",
    "            3. Conserver tous les chiffres et montants exacts\n",
    "            Contexte: Réunion du conseil d'administration à Madagascar.\n",
    "            Termes valides: Fihariana, SON'INVEST, UNIMA, AQUALMA, Ariary.\"\"\"\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"  Nettoyage chunk {i}/{len(chunks)}...\")\n",
    "            \n",
    "            try:\n",
    "                if self.provider == \"groq\":\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.cfg.groq_model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": f\"Corrige ce texte:\\n\\n{chunk}\"}\n",
    "                        ],\n",
    "                        temperature=0.2,\n",
    "                        max_tokens=2000\n",
    "                    )\n",
    "                    cleaned = response.choices[0].message.content.strip()\n",
    "                    cleaned_chunks.append(cleaned)\n",
    "                    \n",
    "                else:  # OpenAI\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.cfg.openai_model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": chunk}\n",
    "                        ],\n",
    "                        temperature=0.2,\n",
    "                        max_tokens=1400\n",
    "                    )\n",
    "                    cleaned = response.choices[0].message.content.strip()\n",
    "                \n",
    "                cleaned_chunks.append(cleaned)\n",
    "                total_delta += abs(len(cleaned) - len(chunk))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ⚠️ Erreur LLM chunk {i}: {e}\")\n",
    "                cleaned_chunks.append(chunk)  # Garder l'original si erreur\n",
    "        \n",
    "        # Assembler et calculer le taux de correction\n",
    "        merged_text = ' '.join(cleaned_chunks)\n",
    "        correction_rate = total_delta / max(len(text), 1)\n",
    "        \n",
    "        # Vérifier le taux de correction\n",
    "        if correction_rate > self.cfg.max_correction_rate:\n",
    "            print(f\"⚠️ Taux de correction {correction_rate:.1%} > seuil {self.cfg.max_correction_rate:.0%}\")\n",
    "            print(\"   → Conservation du texte post-traité sans LLM\")\n",
    "            return text#, correction_rate\n",
    "\n",
    "        print(f\"\\n✅ Nettoyage terminé (taux: {correction_rate:.1%})\")\n",
    "        return merged_text#, correction_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f0dd3",
   "metadata": {
    "papermill": {
     "duration": 0.007208,
     "end_time": "2025-10-28T12:26:51.066254",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.059046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Par défaut, la langue est auto. Pour ton cas, force français :\n",
    "        config = aai.TranscriptionConfig(language_code=\"fr\")\n",
    "2. Diarisation (orateurs)\n",
    "        config = aai.TranscriptionConfig(speaker_labels=True)\n",
    "\n",
    "Exemple :\n",
    "    config = aai.TranscriptionConfig(language_code=\"fr\", speaker_labels=True)\n",
    "    transcript = transcriber.transcribe(audio_path, config=config)\n",
    "\n",
    "Appel :\n",
    "    Si TranscriptionService.transcribe_audio renvoie status=\"error\" ou un real_time_factor >> 5 (trop lent) ou trop de segments sous ton confidence_threshold, alors :\n",
    "        > result = fallback_service.transcribe_with_assemblyai(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eba983",
   "metadata": {
    "papermill": {
     "duration": 0.007065,
     "end_time": "2025-10-28T12:26:51.080639",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.073574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **FORMATAGE SORTIE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17da9f71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.096014Z",
     "iopub.status.busy": "2025-10-28T12:26:51.095816Z",
     "iopub.status.idle": "2025-10-28T12:26:51.105392Z",
     "shell.execute_reply": "2025-10-28T12:26:51.104871Z"
    },
    "papermill": {
     "duration": 0.018482,
     "end_time": "2025-10-28T12:26:51.106393",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.087911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OutputFormatter:\n",
    "    \"\"\"Génère JSON structuré + TXT lisible pour comité\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_json(data: Dict, clean_text: str) -> Dict:\n",
    "        \"\"\"JSON enrichi pour archivage\"\"\"\n",
    "        \n",
    "        # Calculer statistiques locuteurs\n",
    "        speaker_stats = {}\n",
    "        for u in data.get(\"utterances\", []):\n",
    "            speaker = u[\"speaker\"]\n",
    "            duration = u[\"end\"] - u[\"start\"]\n",
    "            if speaker not in speaker_stats:\n",
    "                speaker_stats[speaker] = {\"temps_parole\": 0, \"interventions\": 0}\n",
    "            speaker_stats[speaker][\"temps_parole\"] += duration\n",
    "            speaker_stats[speaker][\"interventions\"] += 1\n",
    "        \n",
    "        # Formater temps de parole\n",
    "        for speaker, stats in speaker_stats.items():\n",
    "            minutes = int(stats[\"temps_parole\"] / 60)\n",
    "            secondes = int(stats[\"temps_parole\"] % 60)\n",
    "            stats[\"temps_parole_fmt\"] = f\"{minutes}min {secondes}s\"\n",
    "        \n",
    "        return {\n",
    "            \"metadata\": {\n",
    "                \"date\": time.strftime(\"%Y-%m-%d\"),\n",
    "                \"duree_totale\": f\"{int(data['duration'] // 60)}min {int(data['duration'] % 60)}s\",\n",
    "                \"participants\": list(speaker_stats.keys()),\n",
    "                \"modele\": data[\"metadata\"][\"model\"],\n",
    "                \"confiance_moyenne\": f\"{data.get('confidence', 0):.1%}\",\n",
    "                \"provider\": data[\"metadata\"][\"provider\"]\n",
    "            },\n",
    "            \"resume_executif\": {\n",
    "                \"chapitres\": data.get(\"chapters\", []),\n",
    "                \"decisions\": [],  # À extraire manuellement ou via NLP\n",
    "                \"actions\": []\n",
    "            },\n",
    "            \"transcription_complete\": clean_text,\n",
    "            \"transcription_detaillee\": data[\"utterances\"],\n",
    "            \"statistiques_locuteurs\": speaker_stats\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_txt(data: Dict, clean_text: str) -> str:\n",
    "        \"\"\"TXT formaté pour lecture comité\"\"\"\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"═\" * 70)\n",
    "        lines.append(\"  PROCÈS-VERBAL - CONSEIL D'ADMINISTRATION\")\n",
    "        lines.append(f\"  Date : {time.strftime('%d %B %Y')}\")\n",
    "        lines.append(f\"  Durée : {int(data['duration'] // 60)}min {int(data['duration'] % 60)}s\")\n",
    "        lines.append(\"═\" * 70)\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Transcription avec locuteurs\n",
    "        lines.append(\"TRANSCRIPTION\")\n",
    "        lines.append(\"-\" * 70)\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        current_speaker = None\n",
    "        for u in data.get(\"utterances\", []):\n",
    "            timestamp = f\"[{int(u['start'] // 60):02d}:{int(u['start'] % 60):02d}]\"\n",
    "            speaker = u[\"speaker\"]\n",
    "            \n",
    "            if speaker != current_speaker:\n",
    "                lines.append(\"\")\n",
    "                lines.append(f\"{timestamp} {speaker}\")\n",
    "                current_speaker = speaker\n",
    "            \n",
    "            lines.append(u[\"text\"])\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        lines.append(\"=\" * 70)\n",
    "        \n",
    "        # Chapitres thématiques\n",
    "        if data.get(\"chapters\"):\n",
    "            lines.append(\"CHAPITRES THÉMATIQUES\")\n",
    "            lines.append(\"-\" * 70)\n",
    "            for i, ch in enumerate(data[\"chapters\"], 1):\n",
    "                lines.append(f\"\\n{i}. {ch['headline']}\")\n",
    "                lines.append(f\"   [{int(ch['start'] // 60):02d}:{int(ch['start'] % 60):02d}] {ch['summary']}\")\n",
    "            lines.append(\"\")\n",
    "            lines.append(\"=\" * 70)\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5439fd",
   "metadata": {
    "papermill": {
     "duration": 0.007031,
     "end_time": "2025-10-28T12:26:51.120712",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.113681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Pipeline de transcription avec gestion automatique du fallback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08de1e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.136377Z",
     "iopub.status.busy": "2025-10-28T12:26:51.136184Z",
     "iopub.status.idle": "2025-10-28T12:26:51.145814Z",
     "shell.execute_reply": "2025-10-28T12:26:51.145319Z"
    },
    "papermill": {
     "duration": 0.018708,
     "end_time": "2025-10-28T12:26:51.146844",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.128136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcribe_meeting(audio_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pipeline complet de transcription\n",
    "    \n",
    "    Étapes:\n",
    "    1. Prétraitement audio\n",
    "    2. Transcription AssemblyAI (+ fallback Whisper)\n",
    "    3. Post-traitement LLM\n",
    "    4. Export JSON + TXT\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Chemin fichier audio (.mp3, .wav, .m4a)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec status, fichiers générés, métadonnées\n",
    "    \"\"\"\n",
    "    \n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(OUTPUT_PATH)\n",
    "    \n",
    "    try:\n",
    "        # [1/4] Prétraitement audio\n",
    "        print(\"\\n[1/4] 🔊 PRÉTRAITEMENT AUDIO\")\n",
    "        print(\"=\" * 60)\n",
    "        preprocessor = AudioPreprocessor(config.sample_rate)\n",
    "        clean_audio = preprocessor.process(audio_path)\n",
    "        \n",
    "        # [2/4] Transcription\n",
    "        print(\"\\n[2/4] 📝 TRANSCRIPTION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        transcriber = AssemblyAITranscriber(config)\n",
    "        try:\n",
    "            result = transcriber.transcribe(clean_audio)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur AssemblyAI: {e}\")\n",
    "            fallback = WhisperFallback()\n",
    "            result = fallback.transcribe(clean_audio)\n",
    "            \n",
    "            if result[\"status\"] == \"error\":\n",
    "                raise Exception(\"Échec transcription (AssemblyAI + Whisper)\")\n",
    "        \n",
    "        # [3/4] Post-traitement LLM\n",
    "        print(\"\\n[3/4] ✨ POST-TRAITEMENT LLM\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        raw_text = result[\"text\"]\n",
    "        \n",
    "        if config.enable_llm and config.groq_key:\n",
    "            cleaner = LLMCleaner(config)\n",
    "            clean_text = cleaner.clean_text(raw_text)\n",
    "        else:\n",
    "            print(\"ℹ️  LLM désactivé, utilisation texte brut\")\n",
    "            clean_text = raw_text\n",
    "        \n",
    "        # [4/4] Export\n",
    "        print(\"\\n[4/4] 💾 EXPORT RÉSULTATS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        formatter = OutputFormatter()\n",
    "        \n",
    "        # JSON structuré\n",
    "        json_data = formatter.format_json(result, clean_text)\n",
    "        json_path = output_dir / f\"transcription_{timestamp}.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✅ JSON sauvegardé : {json_path.name}\")\n",
    "        \n",
    "        # TXT lisible\n",
    "        txt_content = formatter.format_txt(result, clean_text)\n",
    "        txt_path = output_dir / f\"pv_reunion_{timestamp}.txt\"\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(txt_content)\n",
    "        print(f\"✅ TXT sauvegardé  : {txt_path.name}\")\n",
    "        \n",
    "        # JSON brut AssemblyAI (pour debug)\n",
    "        raw_json_path = output_dir / f\"raw_assemblyai_{timestamp}.json\"\n",
    "        with open(raw_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"📋 Raw JSON       : {raw_json_path.name}\")\n",
    "        \n",
    "        # Nettoyage fichiers temporaires\n",
    "        Path(clean_audio).unlink(missing_ok=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"✨ PIPELINE TERMINÉ AVEC SUCCÈS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"files\": {\n",
    "                \"json\": str(json_path),\n",
    "                \"txt\": str(txt_path),\n",
    "                \"raw\": str(raw_json_path)\n",
    "            },\n",
    "            \"metadata\": json_data[\"metadata\"],\n",
    "            \"duration\": result[\"duration\"],\n",
    "            \"speakers\": len(set(u[\"speaker\"] for u in result[\"utterances\"]))\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERREUR PIPELINE : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": timestamp\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8287ede",
   "metadata": {
    "papermill": {
     "duration": 0.007079,
     "end_time": "2025-10-28T12:26:51.161252",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.154173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **EXÉCUTION PRINCIPALE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bd1499d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.176924Z",
     "iopub.status.busy": "2025-10-28T12:26:51.176495Z",
     "iopub.status.idle": "2025-10-28T12:26:51.180233Z",
     "shell.execute_reply": "2025-10-28T12:26:51.179565Z"
    },
    "papermill": {
     "duration": 0.012689,
     "end_time": "2025-10-28T12:26:51.181290",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.168601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Fichier détecté : test_30mn.mp3\n"
     ]
    }
   ],
   "source": [
    "# Test avec votre fichier audio\n",
    "#audio_file = f\"{UPLOAD_PATH}atelier.mp3\"\n",
    "#audio_file = f\"{UPLOAD_PATH}test_1h.wav\"\n",
    "audio_file = f\"{UPLOAD_PATH}test_30mn.mp3\"\n",
    "#audio_info = prepare_audio_file(audio_file)\n",
    "\n",
    "print(f\"\\n📂 Fichier détecté : {Path(audio_file).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b573fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.197043Z",
     "iopub.status.busy": "2025-10-28T12:26:51.196785Z",
     "iopub.status.idle": "2025-10-28T12:26:51.203692Z",
     "shell.execute_reply": "2025-10-28T12:26:51.203030Z"
    },
    "papermill": {
     "duration": 0.016204,
     "end_time": "2025-10-28T12:26:51.204814",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.188610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Point d'entrée principal\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"  PIPELINE TRANSCRIPTION ASSEMBLYAI\")\n",
    "    print(\"  Réunions Conseil d'Administration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Vérification clés API\n",
    "    if not config.assemblyai_key:\n",
    "        print(\"\\n❌ ERREUR : ASSEMBLYAI_API_KEY non configurée\")\n",
    "        print(\"   Définir dans les secrets Kaggle ou variables d'environnement\")\n",
    "        return\n",
    "    \n",
    "    # # Recherche fichier audio\n",
    "    # audio_files = list(Path(config.upload_path).glob(\"*.mp3\")) + \\\n",
    "    #               list(Path(config.upload_path).glob(\"*.wav\")) + \\\n",
    "    #               list(Path(config.upload_path).glob(\"*.m4a\"))\n",
    "    \n",
    "    # if not audio_files:\n",
    "    #     print(f\"\\n❌ Aucun fichier audio trouvé dans {config.upload_path}\")\n",
    "    #     print(\"   Formats supportés : .mp3, .wav, .m4a\")\n",
    "    #     return\n",
    "    \n",
    "    # audio_file = str(audio_files[0])\n",
    "    \n",
    "    # Analyse qualité\n",
    "    quality = analyze_audio_quality(audio_file)\n",
    "    \n",
    "    # Estimation coût\n",
    "    cost = estimate_cost(quality[\"duree_secondes\"], config.speech_model)\n",
    "    print(f\"\\n💰 Coût estimé : {cost['cout_usd']} (modèle: {cost['modele']})\")\n",
    "    \n",
    "    if not cost[\"gratuit\"]:\n",
    "        print(\"   💡 Utiliser speech_model='nano' pour tests gratuits\")\n",
    "    \n",
    "    # Lancer pipeline\n",
    "    result = transcribe_meeting(audio_file)\n",
    "    \n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(f\"\\n📊 RÉSULTATS\")\n",
    "        print(f\"   Durée      : {int(result['duration'] // 60)}min {int(result['duration'] % 60)}s\")\n",
    "        print(f\"   Locuteurs  : {result['speakers']}\")\n",
    "        print(f\"   Confiance  : {result['metadata']['confiance_moyenne']}\")\n",
    "        print(f\"\\n📁 Fichiers générés :\")\n",
    "        for file_type, path in result[\"files\"].items():\n",
    "            print(f\"   - {Path(path).name}\")\n",
    "        \n",
    "        # Proposition mapping locuteurs\n",
    "        with open(result[\"files\"][\"json\"], \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        mapping = create_speaker_mapping(data[\"transcription_detaillee\"])\n",
    "        \n",
    "        # Sauvegarder version avec mapping\n",
    "        data_mapped = apply_speaker_mapping(data, mapping)\n",
    "        mapped_path = Path(OUTPUT_PATH) / f\"transcription_mapped_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(mapped_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data_mapped, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\n✅ Version avec mapping sauvegardée : {mapped_path.name}\")\n",
    "        \n",
    "        # Afficher extrait\n",
    "        print(f\"\\n📝 EXTRAIT (500 premiers caractères) :\")\n",
    "        print(\"-\" * 60)\n",
    "        print(data[\"transcription_complete\"][:500] + \"...\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n❌ Échec : {result.get('error', 'Erreur inconnue')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d19740",
   "metadata": {
    "papermill": {
     "duration": 0.007088,
     "end_time": "2025-10-28T12:26:51.219382",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.212294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#**Analyse des Résultats de Debug**\n",
    "\n",
    "Les fichiers JSON de debug sont sauvegardés dans `/kaggle/working/debug_json/` avec le format:\n",
    "- `02_transcription_[timestamp].json` : Résultat brut de Whisper\n",
    "- `03_diarization_[timestamp].json` : Après identification des locuteurs\n",
    "- `04_postprocessing_[timestamp].json` : Après normalisation et déduplication\n",
    "- `05_llm_cleaning_[timestamp].json` : Version finale nettoyée par LLM\n",
    "\n",
    "Chaque fichier contient :\n",
    "- Un résumé (`summary`) avec aperçu du texte et statistiques\n",
    "- Les métadonnées de l'étape (`status`, `timestamp`)\n",
    "- Les données complètes peuvent être consultées dans le fichier principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79868ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:26:51.234957Z",
     "iopub.status.busy": "2025-10-28T12:26:51.234478Z",
     "iopub.status.idle": "2025-10-28T12:27:01.138768Z",
     "shell.execute_reply": "2025-10-28T12:27:01.137830Z"
    },
    "papermill": {
     "duration": 9.913377,
     "end_time": "2025-10-28T12:27:01.140015",
     "exception": false,
     "start_time": "2025-10-28T12:26:51.226638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  PIPELINE TRANSCRIPTION ASSEMBLYAI\n",
      "  Réunions Conseil d'Administration\n",
      "============================================================\n",
      "\n",
      "🔍 ANALYSE QUALITÉ AUDIO\n",
      "============================================================\n",
      "Durée          : 32min 8s\n",
      "Sample rate    : 48000 Hz\n",
      "Niveau sonore  : 0.014\n",
      "Score qualité  : 100/100\n",
      "\n",
      "✅ Qualité acceptable\n",
      "============================================================\n",
      "\n",
      "💰 Coût estimé : $0.48 (modèle: best)\n",
      "   💡 Utiliser speech_model='nano' pour tests gratuits\n",
      "\n",
      "[1/4] 🔊 PRÉTRAITEMENT AUDIO\n",
      "============================================================\n",
      "\n",
      "❌ ERREUR PIPELINE : AudioPreprocessor.process() missing 1 required positional argument: 'output_dir'\n",
      "\n",
      "❌ Échec : AudioPreprocessor.process() missing 1 required positional argument: 'output_dir'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_20/67426247.py\", line 26, in transcribe_meeting\n",
      "    clean_audio = preprocessor.process(audio_path)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: AudioPreprocessor.process() missing 1 required positional argument: 'output_dir'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8302666,
     "sourceId": 13136823,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39.51676,
   "end_time": "2025-10-28T12:27:02.870581",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-28T12:26:23.353821",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
