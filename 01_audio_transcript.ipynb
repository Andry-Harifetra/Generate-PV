{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13136823,"sourceType":"datasetVersion","datasetId":8302666}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transcription CA\n\nCe notebook applique le **pipeline complet** :\n- **Pr√©traitement** audio (FFmpeg + noisereduce)\n- **Transcription** faster-whisper (r√©glages anti-hallucinations)\n- **Chunks longs** pour une meilleure coh√©rence (3‚Äì5 min)\n- **Diarisation** (pyannote ‚Üí fallback whisperx)\n- **Post-traitement** (d√©dup + normalisation chiffres/unit√©s)\n- **Nettoyage LLM** par morceaux (1000 caract√®res) avec borne de correction\n- **Sauvegarde JSON** des sorties (raw, diarized, cleaned, llm_cleaned)","metadata":{}},{"cell_type":"markdown","source":"# **Installation des packages n√©cessaires**","metadata":{}},{"cell_type":"code","source":"%%capture\n# Installation silencieuse des d√©pendances avec gestion des conflits\n\n# 1. Mise √† jour pip pour √©viter les probl√®mes\n!pip install --upgrade pip -q\n\n# 2. Installation FFmpeg (syst√®me)\n!apt-get update -qq\n!apt-get install -qq ffmpeg sox\n\n# 3. Nettoyage et verrouillage de la stack NumPy/Numba/Scipy\n!pip uninstall -y numpy numba >/dev/null 2>&1 || true\n!pip install -q numpy==1.26.4 scipy==1.11.4\n!pip install -q numba==0.58.1\n!pip install -q torch==2.1.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n\n# 4. Installation des packages de transcription\n#!pip install -q openai-whisper==20231117\n!pip install -q faster-whisper==1.0.3\n\n# 5. Packages de d√©bruitage audio\n!pip install -q librosa==0.10.1\n!pip install -q soundfile==0.12.1\n!pip install -q noisereduce==3.0.0\n!pip install -q pydub==0.25.1\n\n# 6. Diarization\n!pip install -q \"pyannote.audio>=3.1\"\n!pip install -q whisperx\n\n!pip install -q regex==2023.12.25 unidecode==1.3.8\n\n\n\n# 7. Packages documents\n!pip install -q python-docx==1.2.0\n!pip install -q python-pptx==1.0.2\n\n# 8. Packages LLM et NLP\n!pip install -q openai==1.91.0\n!pip install -q assemblyai==0.44.3\n!pip install -q tiktoken==0.9.0\n\n# 9. LangChain\n!pip install -q langchain==0.3.27 langchain-community==0.3.29 langchain-core==0.3.30\n\n# 10. Packages utilitaires\n!pip install -q pandas==2.1.4 matplotlib==3.8.2 seaborn==0.13.2\n\n# 11. Installation FAISS pour le RAG\n!pip install -q faiss-cpu==1.7.4\n\nprint(\"‚úÖ Installation termin√©e!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:18:45.750467Z","iopub.execute_input":"2025-09-25T07:18:45.750712Z","iopub.status.idle":"2025-09-25T07:26:33.292503Z","shell.execute_reply.started":"2025-09-25T07:18:45.750692Z","shell.execute_reply":"2025-09-25T07:26:33.291447Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# V√©rification que tout est install√© correctement\nimport importlib\n\npackages_to_check = [\n    ('numpy', 'numpy'),\n    ('scipy', 'scipy'),\n    ('numba', 'numba'),\n    ('whisper', 'openai-whisper'),\n    ('faster_whisper', 'faster-whisper'),\n    ('librosa', 'librosa'),\n    ('soundfile', 'soundfile'),\n    ('noisereduce', 'noisereduce'),\n    ('pydub', 'pydub'),\n    ('docx', 'python-docx'),\n    ('pptx', 'python-pptx'),\n    ('openai', 'openai'),\n    ('langchain', 'langchain'),\n    ('langchain_community', 'langchain-community'),\n    ('faiss', 'faiss-cpu'),\n    ('assemblyai', 'assemblyai'),\n    ('tiktoken', 'tiktoken')\n]\n\nprint(\"üîç V√©rification des packages install√©s:\")\nprint(\"-\" * 50)\n\nall_ok = True\nfor import_name, package_name in packages_to_check:\n    try:\n        module = importlib.import_module(import_name)\n        version = getattr(module, '__version__', 'N/A')\n        print(f\"‚úÖ {package_name:20} : {version}\")\n    except ImportError:\n        print(f\"‚ùå {package_name:20} : Non install√©\")\n        all_ok = False\n    except Exception as exc:\n        print(f\"‚ö†Ô∏è {package_name:20} : Erreur lors de l'import ({type(exc).__name__}: {exc})\")\n        all_ok = False\n\nif all_ok:\n    print(\"‚ú® Tous les packages sont install√©s correctement!\")\nelse:\n    print(\"‚ö†Ô∏è Certains packages n√©cessitent une attention. Consultez les messages ci-dessus.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:33.293607Z","iopub.execute_input":"2025-09-25T07:26:33.293852Z","iopub.status.idle":"2025-09-25T07:26:44.725787Z","shell.execute_reply.started":"2025-09-25T07:26:33.293824Z","shell.execute_reply":"2025-09-25T07:26:44.725121Z"}},"outputs":[{"name":"stdout","text":"üîç V√©rification des packages install√©s:\n--------------------------------------------------\n‚úÖ numpy                : 1.26.4\n‚úÖ scipy                : 1.16.2\n‚úÖ numba                : 0.58.1\n‚ùå openai-whisper       : Non install√©\n‚úÖ faster-whisper       : 1.2.0\n‚úÖ librosa              : 0.10.1\n‚úÖ soundfile            : 0.12.1\n‚úÖ noisereduce          : N/A\n‚úÖ pydub                : N/A\n‚úÖ python-docx          : 1.2.0\n‚úÖ python-pptx          : 1.0.2\n‚úÖ openai               : 1.91.0\n‚úÖ langchain            : 0.3.26\n‚ùå langchain-community  : Non install√©\n‚úÖ faiss-cpu            : 1.7.4\n‚úÖ assemblyai           : 0.44.3\n‚úÖ tiktoken             : 0.9.0\n‚ö†Ô∏è Certains packages n√©cessitent une attention. Consultez les messages ci-dessus.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Imports et configuration GPU**","metadata":{}},{"cell_type":"code","source":"# Imports standards\nimport os, sys, json, math, re, shutil, subprocess\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datetime import datetime, timezone\nimport time\ntry:\n    from zoneinfo import ZoneInfo\nexcept Exception:\n    ZoneInfo = None\n\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass, field\nimport gc  # Garbage collector\n\nimport numpy as np\nimport pandas as pd\n\n# Imports audio et d√©bruitage\nimport librosa\nimport soundfile as sf\nimport noisereduce as nr\nfrom scipy.signal import butter, filtfilt, medfilt\nfrom pydub import AudioSegment\n\n# Imports pour la transcription\n#import whisper\nfrom faster_whisper import WhisperModel\n\n# Imports pour les documents\nfrom docx import Document\nfrom pptx import Presentation\n\n# Imports pour le NLP et LLM\nimport openai\ntry:\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from langchain_community.vectorstores import FAISS\n    from langchain_community.embeddings import OpenAIEmbeddings\n    langchain_available = True\nexcept ImportError:\n    print(\"‚ö†Ô∏è LangChain non disponible\")\n    langchain_available = False\n\nimport torch\nprint(f\"üîß PyTorch: {torch.__version__}\")\nprint(f\"üéÆ CUDA disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:44.726551Z","iopub.execute_input":"2025-09-25T07:26:44.726892Z","iopub.status.idle":"2025-09-25T07:26:45.244919Z","shell.execute_reply.started":"2025-09-25T07:26:44.726875Z","shell.execute_reply":"2025-09-25T07:26:45.244025Z"}},"outputs":[{"name":"stdout","text":"‚ö†Ô∏è LangChain non disponible\nüîß PyTorch: 2.8.0+cu128\nüéÆ CUDA disponible: True\n   GPU: Tesla T4\n   M√©moire: 15.83 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Configuration des cl√©s API**","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\nASSEMBLYAI_API_KEY = user_secrets.get_secret(\"ASSEMBLYAI_API_KEY\")\nHUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.246865Z","iopub.execute_input":"2025-09-25T07:26:45.247301Z","iopub.status.idle":"2025-09-25T07:26:45.495662Z","shell.execute_reply.started":"2025-09-25T07:26:45.247280Z","shell.execute_reply":"2025-09-25T07:26:45.495057Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# **Configuration des chemins**","metadata":{}},{"cell_type":"code","source":"UPLOAD_PATH = \"/kaggle/input/meeting-audio/\" # Chemin des fichiers upload√©s \nOUTPUT_PATH = \"/kaggle/working\" # Chemin de sortie","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.496309Z","iopub.execute_input":"2025-09-25T07:26:45.496558Z","iopub.status.idle":"2025-09-25T07:26:45.500218Z","shell.execute_reply.started":"2025-09-25T07:26:45.496533Z","shell.execute_reply":"2025-09-25T07:26:45.499426Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"FULL_DIR = Path(OUTPUT_PATH) / \"temp_chunks\"\nFULL_DIR.mkdir(parents=True, exist_ok=True)\n\ndef cleanup_temp_files():\n    \"\"\"Nettoyer les fichiers temporaires\"\"\"\n    if FULL_DIR.exists():\n        shutil.rmtree(FULL_DIR)\n    FULL_DIR.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.501002Z","iopub.execute_input":"2025-09-25T07:26:45.501278Z","iopub.status.idle":"2025-09-25T07:26:45.518800Z","shell.execute_reply.started":"2025-09-25T07:26:45.501256Z","shell.execute_reply":"2025-09-25T07:26:45.518068Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **Utilitaires de commande syst√®me**","metadata":{}},{"cell_type":"code","source":"def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True) #V√©rification cr√©ation de dossier\ndef run(cmd): # Lancement commande\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate(); return p.returncode, out.decode(), err.decode()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.519486Z","iopub.execute_input":"2025-09-25T07:26:45.519674Z","iopub.status.idle":"2025-09-25T07:26:45.544147Z","shell.execute_reply.started":"2025-09-25T07:26:45.519659Z","shell.execute_reply":"2025-09-25T07:26:45.543643Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**Optimiser la m√©moire**","metadata":{}},{"cell_type":"code","source":"def check_gpu_memory():\n    if torch.cuda.is_available():\n        free, total = torch.cuda.mem_get_info()\n        print(f\"üìä GPU: {free/1e9:.2f}GB libres / {total/1e9:.2f}GB total\")\n        if free < 4e9:  # Moins de 4GB libres\n            print(\"‚ö†Ô∏è M√©moire GPU faible, utilisation de 'base' recommand√©e\")\n            return \"base\"\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.544901Z","iopub.execute_input":"2025-09-25T07:26:45.545143Z","iopub.status.idle":"2025-09-25T07:26:45.561506Z","shell.execute_reply.started":"2025-09-25T07:26:45.545125Z","shell.execute_reply":"2025-09-25T07:26:45.560936Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Monitoring et debug**","metadata":{}},{"cell_type":"code","source":"def print_memory_usage():\n    \"\"\"Afficher l'utilisation m√©moire GPU/RAM\"\"\"\n    if torch.cuda.is_available():\n        print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB / {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n    \n    import psutil\n    process = psutil.Process()\n    print(f\"RAM Usage: {process.memory_info().rss / 1e9:.2f}GB\")\n\n# Appeler apr√®s chaque √©tape majeure\nprint_memory_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.562213Z","iopub.execute_input":"2025-09-25T07:26:45.562484Z","iopub.status.idle":"2025-09-25T07:26:45.582455Z","shell.execute_reply.started":"2025-09-25T07:26:45.562459Z","shell.execute_reply":"2025-09-25T07:26:45.581855Z"}},"outputs":[{"name":"stdout","text":"GPU Memory: 0.00GB / 0.00GB\nRAM Usage: 0.95GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# **Configuration du pipeline**","metadata":{}},{"cell_type":"code","source":" @dataclass \nclass Config: \n    \"\"\"Configuration centralis√©e pour Kaggle\"\"\" \n    \n    timezone: str = \"Indian/Antananarivo\"\n    # Cl√©s API \n    openai_key: str = OPENAI_API_KEY \n    assemblyai_key: str = ASSEMBLYAI_API_KEY\n    \n    # Whisper\n    whisper_model: str = \"medium\" # 'tiny', 'base', 'small', 'medium', 'large', \"large-v3\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    compute_type: str = \"float16\" if torch.cuda.is_available() else \"int8\"\n    # Audio\n    sample_rate: int = 16000\n    # Decoding / anti-hallucination\n    beam_size: int = 5\n    best_of: int = 2\n    patience: float = 1.0\n    temperature: float = 0.0\n    compression_ratio_threshold: float = 2.8\n    log_prob_threshold: float = -0.50\n    no_speech_threshold: float = 0.80\n    max_initial_timestamp: float = 1.0\n    suppress_blank: bool = True\n    suppress_tokens: list[int] = field(default_factory=lambda: [-1])\n    # VAD\n    use_vad: bool = True\n    vad_threshold: float = 0.45\n    vad_min_speech_duration_ms: int = 500\n    vad_max_speech_duration_s: float = 60.0\n    vad_min_silence_duration_ms: int = 1000\n    vad_speech_pad_ms: int = 400\n    # Chunks longs pour coh√©rence (3‚Äì5 min)\n    chunk_length_s: int = 300\n    chunk_overlap_s: int = 30\n    # Post-traitement\n    max_repetitions: int = 3\n    # Prompt sp√©cialis√©\n    initial_prompt: str = (\n        \"Conseil d'administration Madagascar. Vocabulaire financier: Ariary, millions, \"\n        \"budget, rapport financier, r√©solution, d√©lib√©ration. \"\n        \"Termes sp√©cifiques: Fihariana, SON'INVEST, UNIMA, AQUALMA. \"\n        \"Intervenants: Pr√©sident, Directeur G√©n√©ral, Commissaire aux Comptes. \"\n        \"Format: discours naturel sans r√©p√©titions.\"\n    )\n    # LLM (activ√© par d√©faut en production)\n    enable_llm: bool = True\n    openai_model: str = \"gpt-4o-mini\" # \"gpt-3.5-turbo\" : Plus √©conomique que GPT-4\n    max_correction_rate: float = 0.18\n    chunk_size_chars: int = 1000\n    chunk_overlap_chars: int = 200\n\nconfig = Config() \nprint(f\"‚úÖ FullRun ‚Äî Whisper: {config.whisper_model} | device: {config.device} | compute: {config.compute_type}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.583177Z","iopub.execute_input":"2025-09-25T07:26:45.583612Z","iopub.status.idle":"2025-09-25T07:26:45.602918Z","shell.execute_reply.started":"2025-09-25T07:26:45.583594Z","shell.execute_reply":"2025-09-25T07:26:45.602193Z"}},"outputs":[{"name":"stdout","text":"‚úÖ FullRun ‚Äî Whisper: medium | device: cuda | compute: float16\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"***Comment r√©gler les param√®tres selon les cas***\n\nCas A ‚Äî Audio propre (dictaphones, salle calme)\n*  beam_size=3, best_of=1‚Äì2 (plus rapide)\n* no_speech_threshold=0.6 (ok)\n* temperature=0.0\n* VAD : min_silence_duration_ms=1500\n\nCas B ‚Äî Audio bruit√© (portes, brouhaha)\n* beam_size=5, best_of=5 (qualit√©)\n* baisser no_speech_threshold √† 0.5 si coupures\n* VAD : threshold=0.4‚Äì0.5, min_speech_duration_ms=200, min_silence_duration_ms=1800‚Äì2200\n* Garde-fous : garder compression_ratio_threshold=2.4\n\nCas C ‚Äî CPU-only (pas de GPU Kaggle)\n* compute_type=\"int8\", mod√®le tiny ou base\n* beam_size=3, best_of=1\n* Threads : cpu_threads=2, num_workers=1\n* Attends un RTF ‚âà 2‚Äì5 (selon longueur)","metadata":{}},{"cell_type":"markdown","source":"# **Pr√©paration de l'audio**","metadata":{}},{"cell_type":"markdown","source":"**Extrait**","metadata":{}},{"cell_type":"code","source":"def slice_audio(input_path: str, output_path: str, start: float = 0.0, duration: Optional[int] = None) -> str:\n    args = [\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\"-ss\",str(start),\"-i\",input_path,\"-ac\",\"1\",\"-ar\",str(config.sample_rate)]\n    if duration and duration > 0:\n        args += [\"-t\",str(duration)]\n    args += [output_path]\n    ensure_dir(str(Path(output_path).parent))\n    code, _, err = run(args)\n    if code!=0:\n        raise RuntimeError(\"FFmpeg slice failed: \" + err)\n    return output_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.603677Z","iopub.execute_input":"2025-09-25T07:26:45.603923Z","iopub.status.idle":"2025-09-25T07:26:45.623527Z","shell.execute_reply.started":"2025-09-25T07:26:45.603907Z","shell.execute_reply":"2025-09-25T07:26:45.622959Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# **Pr√©processing et D√©bruitage Audio**\n**Classe de d√©bruitage audio avanc√©**\n","metadata":{}},{"cell_type":"code","source":"def prepare_audio_file(audio_path: str) -> Dict:\n    \"\"\"Pr√©pare et valide le fichier audio pour la transcription\"\"\"\n    import wave\n    import contextlib\n    \n    file_info = {\n        \"path\": audio_path,\n        \"exists\": os.path.exists(audio_path),\n        \"size_mb\": 0,\n        \"duration_seconds\": 0,\n        \"format\": audio_path.split('.')[-1],\n        \"sample_rate\": 0,\n        \"channels\": 0\n    }\n    \n    if file_info[\"exists\"]:\n        file_info[\"size_mb\"] = os.path.getsize(audio_path) / (1024 * 1024)\n        \n        try:\n            # Charger avec librosa pour info\n            y, sr = librosa.load(audio_path, sr=None, duration=10)\n            file_info[\"sample_rate\"] = sr\n            \n            # Dur√©e totale\n            duration = librosa.get_duration(path=audio_path)\n            file_info[\"duration_seconds\"] = duration\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Erreur lecture audio: {e}\")\n    \n    return file_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.624276Z","iopub.execute_input":"2025-09-25T07:26:45.624480Z","iopub.status.idle":"2025-09-25T07:26:45.647295Z","shell.execute_reply.started":"2025-09-25T07:26:45.624464Z","shell.execute_reply":"2025-09-25T07:26:45.646577Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class AudioPreprocessor:\n    def __init__(self, sample_rate: int):\n        self.sr = sample_rate\n    def ffmpeg_enhance(self, src: str, dst: str) -> str:\n        chain = \"highpass=f=100,lowpass=f=7500,adeclip,afftdn=nf=-25,compand=attacks=0.005:decays=0.05:points=-80/-90|-20/-20|0/-10:gain=5\"\n        code,_,err = run([\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\"-i\",src,\"-ac\",\"1\",\"-ar\",str(self.sr),\"-af\",chain,dst])\n        if code!=0: raise RuntimeError(\"FFmpeg failed: \"+err)\n        return dst\n    def reduce_noise(self, src: str, dst: str) -> str:\n        y, sr = librosa.load(src, sr=self.sr)\n        y = nr.reduce_noise(y=y, sr=sr)\n        sf.write(dst, y, sr); return dst\n    def process(self, src: str, outdir: str) -> str:\n        ensure_dir(outdir)\n        ff = str(Path(outdir)/f\"{Path(src).stem}_ffmpeg.wav\")\n        dn = str(Path(outdir)/f\"{Path(src).stem}_denoise.wav\")\n        self.ffmpeg_enhance(src, ff)\n        return self.reduce_noise(ff, dn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.650098Z","iopub.execute_input":"2025-09-25T07:26:45.650290Z","iopub.status.idle":"2025-09-25T07:26:45.666414Z","shell.execute_reply.started":"2025-09-25T07:26:45.650277Z","shell.execute_reply":"2025-09-25T07:26:45.665642Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# **Transcription Audio**\n**Service de transcription avec audio nettoy√©**","metadata":{}},{"cell_type":"code","source":"class Transcriber:\n    def __init__(self, cfg: Config):\n        self.cfg = cfg; self.model=None\n\n    def load(self):\n        if self.model is None:\n            # Nettoyer le GPU avant chargement\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            self.model = WhisperModel(\n                self.cfg.whisper_model, \n                device=self.cfg.device, \n                compute_type=self.cfg.compute_type,\n                cpu_threads=4,  # Ajouter pour limiter les threads\n                num_workers=1   # Limiter les workers\n            )\n        return self.model\n    \n    def unload(self):\n        \"\"\"Lib√©rer le mod√®le de la m√©moire\"\"\"\n        if self.model is not None:\n            del self.model\n            self.model = None\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    def _decode(self, wav_path: str):\n        m = self.load()\n        kwargs = dict(\n            language=\"fr\",\n            beam_size=self.cfg.beam_size,\n            best_of=self.cfg.best_of,\n            patience=self.cfg.patience,\n            temperature=self.cfg.temperature,\n            compression_ratio_threshold=self.cfg.compression_ratio_threshold,\n            log_prob_threshold=self.cfg.log_prob_threshold,\n            no_speech_threshold=self.cfg.no_speech_threshold,\n            condition_on_previous_text=False,\n            initial_prompt=self.cfg.initial_prompt,\n            word_timestamps=True,\n            suppress_tokens=self.cfg.suppress_tokens,\n            suppress_blank=self.cfg.suppress_blank,\n            max_initial_timestamp=self.cfg.max_initial_timestamp,\n            vad_filter=self.cfg.use_vad,\n            vad_parameters={\n                \"threshold\": self.cfg.vad_threshold,\n                \"min_speech_duration_ms\": self.cfg.vad_min_speech_duration_ms,\n                \"max_speech_duration_s\": self.cfg.vad_max_speech_duration_s,\n                \"min_silence_duration_ms\": self.cfg.vad_min_silence_duration_ms,\n                \"speech_pad_ms\": self.cfg.vad_speech_pad_ms,\n            } if self.cfg.use_vad else None\n        )\n        return m.transcribe(wav_path, **kwargs)\n\n    def transcribe_long_audio(self, audio_path: str) -> Dict[str, Any]:\n        # D√©coupage manuel en gros segments (3‚Äì5 min) pour robustesse m√©moire et coh√©rence\n        # Charger l'audio par petits morceaux\n        y, sr = librosa.load(audio_path, sr=self.cfg.sample_rate, mono=True)\n        total = len(y) / sr\n        \n        # Lib√©rer l'audio complet apr√®s avoir la dur√©e\n        del y\n        gc.collect()\n        \n        L = self.cfg.chunk_length_s\n        O = self.cfg.chunk_overlap_s\n        segs_all = []\n        text_parts = []\n        start = 0.0\n        idx = 0\n\n        while start < total:\n            end = min(start + L, total)\n\n            # Traiter un chunk\n            clip_path = str(FULL_DIR / f\"chunk_{idx:04d}.wav\")\n            slice_audio(audio_path, clip_path, start=start, duration=int(end-start))\n\n            # Transcription\n            segs, info = self._decode(clip_path)\n\n            # Supprimer le chunk imm√©diatement apr√®s traitement\n            Path(clip_path).unlink(missing_ok=True)\n            \n            for s in segs:\n                segs_all.append({\n                    \"start\": float(s.start + start),\n                    \"end\": float(s.end + start),\n                    \"text\": s.text.strip(),\n                    \"no_speech_prob\": float(getattr(s, \"no_speech_prob\", 0.0))\n                })\n                \n            text_parts.append(\" \".join(s.text.strip() for s in segs if float(getattr(s,\"no_speech_prob\",0.0))<=0.9))\n            \n            if end >= total: \n                break\n            \n            start = end - O\n            idx += 1\n\n        transcription = \" \".join(text_parts).strip()\n        return {\"status\":\"success\",\"duration\": float(total), \"segments\": segs_all, \"transcription\": transcription}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.667240Z","iopub.execute_input":"2025-09-25T07:26:45.667438Z","iopub.status.idle":"2025-09-25T07:26:45.685382Z","shell.execute_reply.started":"2025-09-25T07:26:45.667423Z","shell.execute_reply":"2025-09-25T07:26:45.684599Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Exemple d'utilisation\n#result = transcription_service.transcribe_audio(audio_file)\n#print(f\"Transcription: {result['transcription'][:500]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.686176Z","iopub.execute_input":"2025-09-25T07:26:45.686680Z","iopub.status.idle":"2025-09-25T07:26:45.708700Z","shell.execute_reply.started":"2025-09-25T07:26:45.686655Z","shell.execute_reply":"2025-09-25T07:26:45.708126Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# **Diarization**","metadata":{}},{"cell_type":"code","source":"def diarize(transcript: Dict[str,Any], audio_path: str, hf_token: Optional[str]):\n    if transcript.get(\"status\")!=\"success\" or not transcript.get(\"segments\"):\n        return transcript\n    try:\n        if not hf_token:\n            print(\"‚ÑπÔ∏è Pas de HUGGINGFACE_TOKEN ‚Äî diarisation ignor√©e.\")\n            return transcript\n        import whisperx\n        align_model, meta = whisperx.load_align_model(language_code='fr', device=config.device)\n        aligned = whisperx.align(\n            [{\"text\":s[\"text\"],\"start\":s[\"start\"],\"end\":s[\"end\"]} for s in transcript[\"segments\"]],\n            align_model, meta, audio_path, device=config.device\n        )\n        diar = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=config.device)\n        dsegs = diar(audio_path)\n        result = whisperx.assign_word_speakers(dsegs, aligned)\n        transcript[\"diarized_segments\"] = result.get(\"segments\", [])\n        return transcript\n    except Exception as e:\n        print(\"‚ö†Ô∏è Diarisation √©chou√©e:\", e)\n        return transcript\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.709499Z","iopub.execute_input":"2025-09-25T07:26:45.709703Z","iopub.status.idle":"2025-09-25T07:26:45.726157Z","shell.execute_reply.started":"2025-09-25T07:26:45.709680Z","shell.execute_reply":"2025-09-25T07:26:45.725576Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# **Post-traitement du texte**","metadata":{}},{"cell_type":"code","source":"def normalize_compound_numbers(text: str) -> str:\n    pattern = re.compile(r\"(\\\\d+[\\\\s ]*)milliards?\\\\s+(\\\\d+[\\\\s ]*)millions?\", re.IGNORECASE)\n    def repl(m):\n        b = int(m.group(1).replace(\" \",\"\"))\n        M = int(m.group(2).replace(\" \",\"\"))\n        total = b*1_000_000_000 + M*1_000_000\n        return f\"{total:,}\".replace(\",\", \" \")\n    return pattern.sub(repl, text)\n\ndef normalize_units(text: str) -> str:\n    pattern = re.compile(r\"(\\\\d+[\\\\d\\\\s,.]*)\\\\s*(millions?|milliards?)\", re.IGNORECASE)\n    def repl(m):\n        raw = m.group(1); unit = m.group(2).lower()\n        try:\n            val = float(raw.replace(\" \",\"\").replace(\",\", \".\"))\n        except ValueError:\n            return m.group(0)\n        factor = 1_000_000 if \"million\" in unit else 1_000_000_000\n        total = val*factor\n        return f\"{total:,.0f}\".replace(\",\", \" \")\n    return pattern.sub(repl, text)\n\ndef deduplicate(text: str) -> str:\n    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\\\s+\", text) if s.strip()]\n    seen=set(); out=[]\n    for s in sents:\n        k=s.lower()\n        if k in seen: continue\n        seen.add(k); out.append(s)\n    return \". \".join(out)\n\ndef postprocess_text(text: str) -> str:\n    text = normalize_compound_numbers(text)\n    text = normalize_units(text)\n    text = deduplicate(text)\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.726919Z","iopub.execute_input":"2025-09-25T07:26:45.727166Z","iopub.status.idle":"2025-09-25T07:26:45.748960Z","shell.execute_reply.started":"2025-09-25T07:26:45.727129Z","shell.execute_reply":"2025-09-25T07:26:45.748451Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# **Nettoyage LLM**","metadata":{}},{"cell_type":"code","source":"class LLMPostEditor:\n    def __init__(self, api_key: Optional[str], model: str, max_rate: float, size: int, overlap: int):\n        self.api_key = api_key; self.model = model\n        self.max_rate = max_rate; self.size=size; self.overlap=overlap\n        self.client = None\n        if api_key:\n            from openai import OpenAI\n            self.client = OpenAI(api_key=api_key)\n\n    def chunks(self, text: str) -> List[str]:\n        if not text: return []\n        step = max(1, self.size - self.overlap)\n        return [text[i:i+self.size] for i in range(0, len(text), step)]\n\n    def clean(self, text: str) -> Tuple[str, float]:\n        if not self.client or not text: return text, 0.0\n        cleaned=[]; delta=0\n        for i, chunk in enumerate(self.chunks(text), 1):\n            messages=[\n                {\"role\":\"system\",\"content\":(\n                    \"Tu corriges une transcription FR: orthographe, grammaire, ponctuation, noms propres. \"\n                    \"NE JAMAIS ajouter d'information non pr√©sente dans la transcription\"\n                    \"Ne change pas le sens. Applique un style r√©union formel.\"\n                )},\n                {\"role\":\"user\",\"content\":chunk}\n            ]\n            try:\n                resp = self.client.chat.completions.create(model=self.model, messages=messages, temperature=0.2, max_tokens=1400)\n                ct = resp.choices[0].message.content.strip()\n            except Exception as e:\n                print(f\"‚ö†Ô∏è LLM chunk {i} √©chou√©:\", e); ct = chunk\n            cleaned.append(ct); delta += abs(len(ct)-len(chunk))\n        merged = \" \".join(cleaned)\n        rate = delta/max(len(text),1)\n        if rate > self.max_rate:\n            print(f\"‚ö†Ô∏è LLM correction rate {rate:.1%} > seuil {self.max_rate:.0%}. On garde le texte post-trait√©.\")\n            return text, rate\n        return merged, rate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.749821Z","iopub.execute_input":"2025-09-25T07:26:45.750127Z","iopub.status.idle":"2025-09-25T07:26:45.770286Z","shell.execute_reply.started":"2025-09-25T07:26:45.750102Z","shell.execute_reply":"2025-09-25T07:26:45.769638Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# **Fallback AssemblyAI (si √©chec Whisper)**","metadata":{}},{"cell_type":"code","source":"class AssemblyAIFallback:\n    \"\"\"Service de fallback avec AssemblyAI\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        \n    def transcribe_with_assemblyai(self, audio_path: str) -> Dict:\n        \"\"\"\n        Transcription de secours via AssemblyAI\n        \n        Args:\n            audio_path: Chemin du fichier audio\n            \n        Returns:\n            Dict avec la transcription\n        \"\"\"\n        if not self.api_key:\n            return {\n                \"status\": \"error\",\n                \"error\": \"Cl√© API AssemblyAI non configur√©e\"\n            }\n        \n        try:\n            import assemblyai as aai\n            \n            print(\"üîÑ Utilisation du fallback AssemblyAI...\")\n            \n            aai.settings.api_key = self.api_key\n            transcriber = aai.Transcriber()\n            \n            # Upload et transcription\n            config_lang = aai.TranscriptionConfig(\n                language_code=\"fr\",\n                punctuate=True,\n                format_text=True,\n                disfluencies=True,\n                speaker_labels=True\n            )\n            transcript = transcriber.transcribe(audio_path, config=config_lang)\n            \n            if transcript.status == aai.TranscriptStatus.error:\n                raise Exception(f\"Erreur AssemblyAI: {transcript.error}\")\n            \n            # Attente de la transcription\n            while transcript.status not in [aai.TranscriptStatus.completed, aai.TranscriptStatus.error]:\n                time.sleep(5)\n                transcript = transcriber.get_transcript(transcript.id)\n            \n            return {\n                \"status\": \"success\",\n                \"method\": \"assemblyai\",\n                \"transcription\": transcript.text,\n                \"confidence\": transcript.confidence if hasattr(transcript, 'confidence') else 0.85,\n                \"words\": transcript.words if hasattr(transcript, 'words') else []\n            }\n            \n        except Exception as e:\n            print(f\"‚ùå Erreur AssemblyAI: {str(e)}\")\n            return {\n                \"status\": \"error\",\n                \"error\": str(e),\n                \"method\": \"assemblyai\"\n            }\n\n# Service de fallback\nfallback_service = AssemblyAIFallback(config.assemblyai_key)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-09-25T07:26:45.771155Z","iopub.execute_input":"2025-09-25T07:26:45.771373Z","iopub.status.idle":"2025-09-25T07:26:45.797536Z","shell.execute_reply.started":"2025-09-25T07:26:45.771357Z","shell.execute_reply":"2025-09-25T07:26:45.796880Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"1. Par d√©faut, la langue est auto. Pour ton cas, force fran√ßais :\n        config = aai.TranscriptionConfig(language_code=\"fr\")\n2. Diarisation (orateurs)\n        config = aai.TranscriptionConfig(speaker_labels=True)\n\nExemple :\n    config = aai.TranscriptionConfig(language_code=\"fr\", speaker_labels=True)\n    transcript = transcriber.transcribe(audio_path, config=config)\n\nAppel :\n    Si TranscriptionService.transcribe_audio renvoie status=\"error\" ou un real_time_factor >> 5 (trop lent) ou trop de segments sous ton confidence_threshold, alors :\n        > result = fallback_service.transcribe_with_assemblyai(audio_path)","metadata":{}},{"cell_type":"markdown","source":"# **Pipeline de transcription avec gestion automatique du fallback**","metadata":{}},{"cell_type":"code","source":"def transcribe_audio_pipeline(audio_path: str, cfg: Config, save_json: bool=True) -> Dict[str, Any]:\n    # V√©rifier la m√©moire avant de commencer\n    suggested_model = check_gpu_memory()\n    if suggested_model:\n        cfg.whisper_model = suggested_model\n        \n    print(\"=\"*80); print(\"üéØ PHASE 2 ‚Äî Pipeline complet (production)\"); print(\"=\"*80)\n    # Nettoyage initial\n    import gc\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        free, total = torch.cuda.mem_get_info()\n        print(f\"üìä M√©moire GPU au d√©marrage: {free/1e9:.2f}GB libres / {total/1e9:.2f}GB total\")\n        if free < 6e9:  # Moins de 6GB libres\n            print(\"‚ö†Ô∏è M√©moire GPU faible - passage en mode √©conomique\")\n            cfg.compute_type = \"int8\"\n    \n    if not Path(audio_path).exists():\n        raise FileNotFoundError(audio_path)\n\n    # Pr√©traitement\n    print(\"[1/5] Pr√©traitement audio (FFmpeg + NR)\")\n    clean_path = AudioPreprocessor(cfg.sample_rate).process(audio_path, OUTPUT_PATH)\n\n    # Transcription (long audio, chunks 3‚Äì5 min)\n    print(\"[2/5] Transcription faster-whisper (chunks longs)\")\n    transcriber = Transcriber(cfg)\n    asr = transcriber.transcribe_long_audio(clean_path)\n    if asr.get(\"status\")!=\"success\":\n        raise RuntimeError(\"Transcription √©chou√©e\")\n\n    # Diarisation\n    print(\"[3/5] Diarisation (pyannote -> whisperx)\")\n    asr = diarize(asr, clean_path, HUGGINGFACE_TOKEN)\n\n    # Post-traitement r√®gles\n    print(\"[4/5] Post-traitement (d√©dup + chiffres)\")\n    post_text = postprocess_text(asr.get(\"transcription\",\"\"))\n    asr[\"transcription_postprocessed\"] = post_text\n\n    # LLM (si activ√©)\n    print(\"[5/5] Nettoyage LLM (chunks 1000 caract√®res)\")\n    final_text = post_text; rate = 0.0\n    if cfg.enable_llm and OPENAI_API_KEY:\n        editor = LLMPostEditor(OPENAI_API_KEY, cfg.openai_model, cfg.max_correction_rate, cfg.chunk_size_chars, cfg.chunk_overlap_chars)\n        final_text, rate = editor.clean(post_text)\n    else:\n        print(\"‚ÑπÔ∏è LLM non activ√© ou cl√© absente ‚Äî on garde le post-traitement r√®gles.\")\n\n    asr[\"transcription_llm\"] = final_text\n    asr[\"llm_correction_rate\"] = rate\n\n    # Sauvegarde JSONs\n    if save_json:\n        base = f\"full_{int(time.time())}\"\n        out_path = str(Path(OUTPUT_PATH)/f\"{base}.json\")\n        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(asr, f, ensure_ascii=False, indent=2)\n        print(\"üíæ Sauvegard√©:\", out_path)\n    return asr\n\ndef transcribe_audio_pipeline(audio_path: str, cfg: Config, save_json: bool=True) -> Dict[str, Any]:\n    try:\n        # Initialisation\n        cleanup_temp_files()\n        \n        # [1] Pr√©traitement\n        print(\"[1/5] Pr√©traitement audio...\")\n        preprocessor = AudioPreprocessor(cfg.sample_rate)\n        clean_path = preprocessor.process(audio_path, str(FULL_DIR))\n        \n        # Lib√©rer m√©moire apr√®s pr√©traitement\n        print_memory_usage()\n        del preprocessor\n        gc.collect()\n        \n        # [2] Transcription avec gestion m√©moire\n        print(\"[2/5] Transcription...\")\n        transcriber = Transcriber(cfg)\n        asr = transcriber.transcribe_long_audio(clean_path)\n        print_memory_usage()\n        \n        # Lib√©rer le mod√®le apr√®s transcription\n        print_memory_usage()\n        transcriber.unload()\n        \n        # Diarisation\n        print(\"[3/5] Diarisation (pyannote -> whisperx)\")\n        asr = diarize(asr, clean_path, HUGGINGFACE_TOKEN)\n        print_memory_usage()\n    \n        # Post-traitement r√®gles\n        print(\"[4/5] Post-traitement (d√©dup + chiffres)\")\n        post_text = postprocess_text(asr.get(\"transcription\",\"\"))\n        asr[\"transcription_postprocessed\"] = post_text\n        print_memory_usage()\n    \n        # LLM (si activ√©)\n        print(\"[5/5] Nettoyage LLM (chunks 1000 caract√®res)\")\n        final_text = post_text; rate = 0.0\n        if cfg.enable_llm and OPENAI_API_KEY:\n            editor = LLMPostEditor(OPENAI_API_KEY, cfg.openai_model, cfg.max_correction_rate, cfg.chunk_size_chars, cfg.chunk_overlap_chars)\n            final_text, rate = editor.clean(post_text)\n        else:\n            print(\"‚ÑπÔ∏è LLM non activ√© ou cl√© absente ‚Äî on garde le post-traitement r√®gles.\")\n    \n        asr[\"transcription_llm\"] = final_text\n        asr[\"llm_correction_rate\"] = rate\n\n        print_memory_usage()\n    \n        # Sauvegarde JSONs\n        if save_json:\n            base = f\"full_{int(time.time())}\"\n            out_path = str(Path(OUTPUT_PATH)/f\"{base}.json\")\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(asr, f, ensure_ascii=False, indent=2)\n            print(\"üíæ Sauvegard√©:\", out_path)\n        \n        return asr\n        \n    finally:\n        # Nettoyage final\n        cleanup_temp_files()\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.798431Z","iopub.execute_input":"2025-09-25T07:26:45.798643Z","iopub.status.idle":"2025-09-25T07:26:45.820534Z","shell.execute_reply.started":"2025-09-25T07:26:45.798627Z","shell.execute_reply":"2025-09-25T07:26:45.819880Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Test avec votre fichier audio\n#audio_file = f\"{UPLOAD_PATH}atelier.mp3\"\n#audio_file = f\"{UPLOAD_PATH}test_1h.wav\"\naudio_file = f\"{UPLOAD_PATH}test_30mn.mp3\"\n#audio_info = prepare_audio_file(audio_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.821233Z","iopub.execute_input":"2025-09-25T07:26:45.821444Z","iopub.status.idle":"2025-09-25T07:26:45.840299Z","shell.execute_reply.started":"2025-09-25T07:26:45.821428Z","shell.execute_reply":"2025-09-25T07:26:45.839584Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"result = transcribe_audio_pipeline(audio_file, config, save_json=True)\nprint(result.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:26:45.840976Z","iopub.execute_input":"2025-09-25T07:26:45.841179Z","execution_failed":"2025-09-25T07:30:11.434Z"}},"outputs":[{"name":"stdout","text":"[1/5] Pr√©traitement audio...\nGPU Memory: 0.00GB / 0.00GB\nRAM Usage: 1.14GB\n[2/5] Transcription...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b69bf1125ce490ea5e943d949e0ad43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocabulary.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d495fe3f499743889017781db6c16a77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b53852605e04cbd88a574891e37b8f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.bin:   0%|          | 0.00/1.53G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7221d01dfd9a4bc4b5ccfa8277707170"}},"metadata":{}}],"execution_count":null}]}