{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13136823,"sourceType":"datasetVersion","datasetId":8302666}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# **Installation des packages n√©cessaires**","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n# Installation silencieuse des d√©pendances avec gestion des conflits\n\n# 1. Mise √† jour pip pour √©viter les probl√®mes\n!pip install --upgrade pip -q\n\n# 2. Installation FFmpeg (syst√®me)\n!apt-get update -qq\n!apt-get install -qq ffmpeg\n\n# 3. Installation des packages de transcription\n!pip install -q openai-whisper==20250625\n!pip install -q faster-whisper==1.2.0\n\n# 4. Packages de d√©bruitage audio\n!pip install -q librosa==0.10.1\n!pip install -q soundfile==0.12.1\n!pip install -q noisereduce==3.0.0\n!pip install -q scipy==1.11.4\n!pip install -q pydub==0.25.1\n\n# 5. Packages documents\n!pip install -q python-docx==1.2.0\n!pip install -q python-pptx==1.0.2\n\n# 6. Packages LLM et NLP\n!pip install -q openai==1.91.0\n!pip install -q assemblyai==0.44.3\n!pip install -q tiktoken==0.9.0\n\n# 7. LangChain\n!pip install -q langchain==0.3.27 langchain-community==0.3.29 langchain-core -q 2>/dev/null || true\n\n# 8. Packages utilitaires\n!pip install -q numpy==1.24.3\n!pip install -q pandas matplotlib seaborn\n\n# 9. Installation FAISS pour le RAG\n!pip install -q faiss-cpu==1.12.0\n\nprint(\"‚úÖ Installation termin√©e!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:30:04.329729Z","iopub.execute_input":"2025-09-24T05:30:04.330437Z","iopub.status.idle":"2025-09-24T05:32:45.381602Z","shell.execute_reply.started":"2025-09-24T05:30:04.330413Z","shell.execute_reply":"2025-09-24T05:32:45.380496Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# V√©rification que tout est install√© correctement\nimport sys\nimport importlib\n\npackages_to_check = [\n    ('whisper', 'openai-whisper'),\n    ('faster_whisper', 'faster-whisper'),\n    ('librosa', 'librosa'),\n    ('soundfile', 'soundfile'),\n    ('noisereduce', 'noisereduce'),\n    ('scipy', 'scipy'),\n    ('pydub', 'pydub'),\n    ('docx', 'python-docx'),\n    ('pptx', 'python-pptx'),\n    ('openai', 'openai'),\n    ('langchain', 'langchain'),\n    ('langchain_community', 'langchain-community'),\n    ('faiss', 'faiss-cpu'),\n    ('assemblyai', 'assemblyai'),\n    ('tiktoken', 'tiktoken')\n]\n\nprint(\"üîç V√©rification des packages install√©s:\")\nprint(\"-\" * 50)\n\nall_ok = True\nfor import_name, package_name in packages_to_check:\n    try:\n        module = importlib.import_module(import_name)\n        version = getattr(module, '__version__', 'N/A')\n        print(f\"‚úÖ {package_name:20} : {version}\")\n    except ImportError:\n        print(f\"‚ùå {package_name:20} : Non install√©\")\n        all_ok = False\n\nif all_ok:\n    print(\"\\n‚ú® Tous les packages sont install√©s correctement!\")\nelse:\n    print(\"\\n‚ö†Ô∏è Certains packages manquent. Relancez la cellule 1.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:45.383593Z","iopub.execute_input":"2025-09-24T05:32:45.383865Z","iopub.status.idle":"2025-09-24T05:32:56.478961Z","shell.execute_reply.started":"2025-09-24T05:32:45.383839Z","shell.execute_reply":"2025-09-24T05:32:56.478242Z"}},"outputs":[{"name":"stdout","text":"üîç V√©rification des packages install√©s:\n--------------------------------------------------\n‚úÖ openai-whisper       : 20250625\n‚úÖ faster-whisper       : 1.2.0\n‚úÖ librosa              : 0.10.1\n‚úÖ soundfile            : 0.12.1\n‚úÖ noisereduce          : N/A\n‚úÖ scipy                : 1.11.4\n‚úÖ pydub                : N/A\n‚úÖ python-docx          : 1.2.0\n‚úÖ python-pptx          : 1.0.2\n‚úÖ openai               : 1.91.0\n‚úÖ langchain            : 0.3.27\n‚úÖ langchain-community  : 0.3.29\n‚úÖ faiss-cpu            : 1.12.0\n‚úÖ assemblyai           : 0.44.3\n‚úÖ tiktoken             : 0.9.0\n\n‚ú® Tous les packages sont install√©s correctement!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Imports et configuration GPU**","metadata":{}},{"cell_type":"code","source":"# Imports standards\nimport os\nimport sys\nimport json\nimport warnings\nimport re\nimport subprocess\nwarnings.filterwarnings('ignore')\n\nfrom datetime import datetime, timezone\nimport time\ntry:\n    from zoneinfo import ZoneInfo\nexcept Exception:\n    ZoneInfo = None\n\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nimport gc  # Garbage collector\n\n# Imports audio et d√©bruitage\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport noisereduce as nr\nfrom scipy.signal import butter, filtfilt, medfilt\nfrom pydub import AudioSegment\n\n# Imports pour la transcription\nimport whisper\nfrom faster_whisper import WhisperModel\n\n# Imports pour les documents\nfrom docx import Document\nfrom pptx import Presentation\n\n# Imports pour le NLP et LLM\nimport openai\ntry:\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from langchain_community.vectorstores import FAISS\n    from langchain_community.embeddings import OpenAIEmbeddings\n    langchain_available = True\nexcept ImportError:\n    print(\"‚ö†Ô∏è LangChain non disponible\")\n    langchain_available = False\n\nimport torch\nprint(f\"üîß PyTorch: {torch.__version__}\")\nprint(f\"üéÆ CUDA disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:56.479711Z","iopub.execute_input":"2025-09-24T05:32:56.480042Z","iopub.status.idle":"2025-09-24T05:32:56.997747Z","shell.execute_reply.started":"2025-09-24T05:32:56.480023Z","shell.execute_reply":"2025-09-24T05:32:56.997054Z"}},"outputs":[{"name":"stdout","text":"üîß PyTorch: 2.6.0+cu124\nüéÆ CUDA disponible: True\n   GPU: Tesla T4\n   M√©moire: 15.83 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Configuration des cl√©s API\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\nASSEMBLYAI_API_KEY = user_secrets.get_secret(\"ASSEMBLYAI_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:56.998457Z","iopub.execute_input":"2025-09-24T05:32:56.998724Z","iopub.status.idle":"2025-09-24T05:32:57.235781Z","shell.execute_reply.started":"2025-09-24T05:32:56.998696Z","shell.execute_reply":"2025-09-24T05:32:57.235194Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Configuration des chemins \nUPLOAD_PATH = \"/kaggle/input/meeting-audio/\" # Chemin des fichiers upload√©s \nOUTPUT_PATH = \"/kaggle/working\" # Chemin de sortie","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.237445Z","iopub.execute_input":"2025-09-24T05:32:57.237721Z","iopub.status.idle":"2025-09-24T05:32:57.241147Z","shell.execute_reply.started":"2025-09-24T05:32:57.237701Z","shell.execute_reply":"2025-09-24T05:32:57.240425Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Configuration du pipeline \n@dataclass \nclass Config: \n    \"\"\"Configuration centralis√©e pour Kaggle\"\"\" \n\n    # Chemins\n    input_dir: str = UPLOAD_PATH\n    output_dir: str = OUTPUT_PATH\n    \n    timezone: str = \"Indian/Antananarivo\"\n    \n    # Mod√®le Whisper \n    whisper_model: str = \"large-v3\" # 'tiny', 'base', 'small', 'medium', 'large'\n    whisper_device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    whisper_compute_type: str = \"float16\" if torch.cuda.is_available() else \"int8\"\n    #device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    compute_type: str = \"float16\" if torch.cuda.is_available() else \"int8\"\n    \n    openai_model: str = \"gpt-3.5-turbo\" # Plus √©conomique que GPT-4 \n    \n    # Cl√©s API \n    openai_key: str = OPENAI_API_KEY \n    assemblyai_key: str = ASSEMBLYAI_API_KEY\n\n    # Param√®tres audio\n    denoise_method: str = \"hybrid\"  # ffmpeg, noisereduce, hybrid\n    denoise_aggressive: bool = True\n    sample_rate: int = 16000\n\n    # Param√®tres de traitement \n    \n    ## Longueur maximale d‚Äôun ‚Äúmorceau de texte‚Äù (chunk) qu‚Äôon d√©coupe avant d‚Äôenvoyer au LLM.\n    ## R√®gle : chunk_size ‚âà 20-30% de la capacit√© max du mod√®le.\n    chunk_size: int = 1000 # nombre de caract√®re ‚âà 200‚Äì250 tokens (selon la langue et la densit√©) √† modifier selon la limitation du mod√®le choisie (ex. GPT-3.5 ‚âà 4k tokens, GPT-4 ‚âà 8k ou 32k).\n    \n    ## Nombre de caract√®res r√©p√©t√©s entre deux chunks.\n    ## R√®gle : overlap = 15-25% du chunk_size.\n    chunk_overlap: int = 200 # nombre de caract√®re ‚âà 40 tokens. Suffisant pour garder la continuit√© (phrases coup√©es, dialogues, etc.).\n    \n    ## Proportion maximale de mots que le LLM a le droit de modifier dans une transcription brute.\n    ## R√®gle : plus l‚Äôaudio est bruit√©, plus tu tol√®res une correction √©lev√©e. [propre (dictaphone, micro-cravate) ‚Üí mettre bas (0.10 √† 0.15). / bruyant (claquements de porte, plusieurs intervenants) ‚Üí monter √† 0.20 voire 0.25]\n    max_correction_rate: float = 0.15 # Max 15% du texte peut √™tre modifi√© (Pas de r√©√©criture compl√®te ‚Üí garde la fid√©lit√© au discours original.) Evite les hallucinations\n    \n    ## Score minimal de confiance (0‚Äì1) pour garder une phrase transcrite par Whisper/AssemblyAI.\n    confidence_threshold: float = 0.85 #Segments dont la transcription est jug√©e correcte √† au moins 85%.\n\n    # Optimisation m√©moire pour Kaggle \n    num_workers: int = 2  # Ajust√© pour T4\n    batch_size: int = 4 # Pour le traitement par lots [Si CPU seulement ‚Üí descendre (1‚Äì2).]\n    use_gpu: bool = torch.cuda.is_available()\n\n    # NOUVEAUX PARAM√àTRES ANTI-HALLUCINATIONS\n    beam_size: int = 3  # Plus de beam = plus de pr√©cision\n    best_of: int = 2    # Prendre le meilleur de 3 tentatives\n    patience: float = 1.0\n    temperature: float = 0.0  # Pas de randomness\n    \n    # Seuils de confiance stricts\n    no_speech_threshold: float = 0.8 # Plus strict\n    logprob_threshold: float = -0.5  # Plus strict\n    compression_ratio_threshold: float = 2.8  # √âvite les r√©p√©titions\n\n    # NOUVEAU: Param√®tres anti-r√©p√©tition\n    max_initial_timestamp: float = 1.0\n    suppress_blank: bool = True\n    suppress_tokens: str = \"-1\"  # Supprime les tokens probl√©matiques\n    \n    # VAD (Voice Activity Detection) optimis√©\n    use_vad: bool = True\n    vad_threshold: float = 0.45\n    vad_min_speech_duration_ms: int = 500  # Minimum 250ms de parole\n    vad_max_speech_duration_s: float = 60  # Max 30s par segment\n    vad_min_silence_duration_ms: int = 1000  # 2s de silence minimum\n    vad_speech_pad_ms: int = 400\n\n    # NOUVEAU: Chunking intelligent\n    chunk_length_s: int = 300  # Chunks de 5 minutes max\n    chunk_overlap_s: int = 30   # Overlap de 30 secondes\n    \n    # Audio processing\n    sample_rate: int = 16000\n    use_denoise: bool = \"auto\"  # auto, True, False\n    denoise_stationary: float = 0.97\n    denoise_prop_decrease: float = 1.0\n    \n    # D√©tection r√©p√©titions\n    repetition_penalty: float = 1.2  # NOUVEAU\n    max_repetitions: int = 3  # NOUVEAU: max r√©p√©titions tol√©r√©es\n    \n    # Prompt sp√©cialis√© CA - AM√âLIOR√â\n    # PROMPT AM√âLIOR√â avec contexte financier malgache\n    initial_prompt: str = (\n        \"Conseil d'administration Madagascar. Vocabulaire financier: Ariary, millions, \"\n        \"budget, rapport financier, r√©solution, d√©lib√©ration. \"\n        \"Termes sp√©cifiques: Fihariana, SON'INVEST, UNIMA, AQUALMA. \"\n        \"Intervenants: Pr√©sident, Directeur G√©n√©ral, Commissaire aux Comptes. \"\n        \"Format: discours naturel sans r√©p√©titions.\"\n    )\n\nconfig = Config() \nprint(f\"‚úÖ Configuration charg√©e - Mod√®le Whisper: {config.whisper_model}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.241917Z","iopub.execute_input":"2025-09-24T05:32:57.242152Z","iopub.status.idle":"2025-09-24T05:32:57.261322Z","shell.execute_reply.started":"2025-09-24T05:32:57.242129Z","shell.execute_reply":"2025-09-24T05:32:57.260587Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Configuration charg√©e - Mod√®le Whisper: large-v3\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def prepare_audio_file(audio_path: str) -> Dict:\n    \"\"\"Pr√©pare et valide le fichier audio pour la transcription\"\"\"\n    import wave\n    import contextlib\n    \n    file_info = {\n        \"path\": audio_path,\n        \"exists\": os.path.exists(audio_path),\n        \"size_mb\": 0,\n        \"duration_seconds\": 0,\n        \"format\": audio_path.split('.')[-1],\n        \"sample_rate\": 0,\n        \"channels\": 0\n    }\n    \n    if file_info[\"exists\"]:\n        file_info[\"size_mb\"] = os.path.getsize(audio_path) / (1024 * 1024)\n        \n        try:\n            # Charger avec librosa pour info\n            y, sr = librosa.load(audio_path, sr=None, duration=10)\n            file_info[\"sample_rate\"] = sr\n            \n            # Dur√©e totale\n            duration = librosa.get_duration(path=audio_path)\n            file_info[\"duration_seconds\"] = duration\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Erreur lecture audio: {e}\")\n    \n    return file_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.262167Z","iopub.execute_input":"2025-09-24T05:32:57.262396Z","iopub.status.idle":"2025-09-24T05:32:57.279272Z","shell.execute_reply.started":"2025-09-24T05:32:57.262372Z","shell.execute_reply":"2025-09-24T05:32:57.278507Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def format_timestamp(seconds: float) -> str:\n    \"\"\"Convertit des secondes en format HH:MM:SS\"\"\"\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    secs = int(seconds % 60)\n    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.279959Z","iopub.execute_input":"2025-09-24T05:32:57.280203Z","iopub.status.idle":"2025-09-24T05:32:57.293955Z","shell.execute_reply.started":"2025-09-24T05:32:57.280170Z","shell.execute_reply":"2025-09-24T05:32:57.293305Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **Pr√©processing et D√©bruitage Audio**\n**Classe de d√©bruitage audio avanc√©**\n","metadata":{}},{"cell_type":"code","source":"class AudioPreprocessor:\n    \"\"\"Service de pr√©traitement audio avec d√©tection intelligente du bruit\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n    \n    def analyze_noise_profile(self, audio_path: str, duration: int = 30) -> Dict:\n        \"\"\"Analyse le profil de bruit de l'audio\"\"\"\n        print(\"üîç Analyse du niveau de bruit...\")\n        \n        y, sr = librosa.load(audio_path, sr=self.config.sample_rate, duration=duration)\n        \n        # Calculer le SNR\n        signal_power = np.mean(y ** 2)\n        noise_floor = np.percentile(np.abs(y), 5) ** 2\n        snr = 10 * np.log10(signal_power / (noise_floor + 1e-10))\n        \n        # D√©tecter les silences\n        silence_threshold = np.percentile(np.abs(y), 20)\n        silence_ratio = np.sum(np.abs(y) < silence_threshold) / len(y)\n        \n        # D√©tecter les impulsions (clics, pops)\n        impulses = np.sum(np.abs(np.diff(y)) > 0.5) / len(y)\n        \n        return {\n            \"snr\": float(snr),\n            \"silence_ratio\": float(silence_ratio),\n            \"impulse_ratio\": float(impulses),\n            \"needs_denoising\": bool(snr < 20 or impulses > 0.001)\n        }\n    \n    def apply_denoising(self, audio_path: str, output_path: str = None) -> str:\n        \"\"\"Applique un d√©bruitage intelligent\"\"\"\n        print(\"üîß Application du d√©bruitage adaptatif...\")\n        \n        if output_path is None:\n            output_path = audio_path.replace(\".mp3\", \"_denoised.wav\")\n        \n        # Charger l'audio complet\n        y, sr = librosa.load(audio_path, sr=self.config.sample_rate)\n        \n        # D√©bruitage stationnaire\n        y_denoised = nr.reduce_noise(\n            y=y,\n            sr=sr,\n            stationary=True,\n            prop_decrease=self.config.denoise_prop_decrease\n        )\n        \n        # Normalisation douce\n        max_val = np.max(np.abs(y_denoised))\n        if max_val > 0:\n            y_denoised = y_denoised * (0.95 / max_val)\n        \n        # Sauvegarder\n        sf.write(output_path, y_denoised, sr)\n        print(f\"‚úÖ Audio d√©bruit√© sauvegard√©: {output_path}\")\n        \n        return output_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.294688Z","iopub.execute_input":"2025-09-24T05:32:57.295010Z","iopub.status.idle":"2025-09-24T05:32:57.313123Z","shell.execute_reply.started":"2025-09-24T05:32:57.294984Z","shell.execute_reply":"2025-09-24T05:32:57.312441Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# **Transcription Audio**\n**Service de transcription avec audio nettoy√©**","metadata":{}},{"cell_type":"code","source":"class TranscriptionService:\n    \"\"\"Service de transcription Whisper avec d√©tection et correction des r√©p√©titions\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.model = None\n        self.preprocessor = AudioPreprocessor(config)\n        self.repetition_buffer = []  # Buffer pour d√©tecter les r√©p√©titions\n        \n    def load_whisper_model(self):\n        \"\"\"Charge le mod√®le Whisper avec gestion m√©moire optimis√©e\"\"\"\n        if self.model is None:\n            print(f\"‚è≥ Chargement Whisper {self.config.whisper_model}...\")\n            \n            # Lib√©rer la m√©moire GPU\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gc.collect()\n            \n            # Charger avec faster-whisper (plus efficace)\n            self.model = WhisperModel(\n                self.config.whisper_model,\n                device=self.config.whisper_device,\n                compute_type=self.config.whisper_compute_type,\n                cpu_threads=8 if self.config.whisper_device == \"cpu\" else 0,\n                num_workers=2  # R√©duit pour √©viter les probl√®mes\n            )\n            \n            print(\"‚úÖ Mod√®le charg√© sur\", self.config.whisper_device.upper())\n        \n        return self.model\n    \n    def detect_repetitions(self, text: str, window_size: int = 100) -> bool:\n        \"\"\"D√©tecte les r√©p√©titions dans le texte\"\"\"\n        words = text.lower().split()\n        \n        if len(words) < window_size:\n            return False\n        \n        # V√©rifier les r√©p√©titions dans une fen√™tre glissante\n        for i in range(len(words) - window_size):\n            window = words[i:i + window_size]\n            unique_ratio = len(set(window)) / len(window)\n            \n            # Si moins de 30% de mots uniques, c'est une r√©p√©tition\n            if unique_ratio < 0.3:\n                return True\n        \n        # V√©rifier les r√©p√©titions exactes de phrases\n        sentences = text.split('.')\n        if len(sentences) > 3:\n            last_3 = sentences[-3:]\n            if len(set(last_3)) == 1 and len(last_3[0]) > 10:\n                return True\n        \n        return False\n    \n    def clean_repetitions(self, segments: List[Dict]) -> List[Dict]:\n        \"\"\"Nettoie les segments r√©p√©titifs\"\"\"\n        cleaned = []\n        repetition_count = {}\n        \n        for segment in segments:\n            text = segment['text'].strip()\n            \n            # Compter les occurrences\n            if text in repetition_count:\n                repetition_count[text] += 1\n                \n                # Si trop de r√©p√©titions, ignorer\n                if repetition_count[text] > self.config.max_repetitions:\n                    continue\n            else:\n                repetition_count[text] = 1\n            \n            # D√©tecter les r√©p√©titions partielles\n            if len(cleaned) > 0:\n                last_text = cleaned[-1]['text']\n                \n                # Si le texte est identique ou tr√®s similaire au pr√©c√©dent\n                if text == last_text or (len(text) > 20 and text in last_text):\n                    continue\n            \n            cleaned.append(segment)\n        \n        return cleaned\n    \n    def transcribe_chunk(self, audio_chunk: np.ndarray, sr: int, offset: float = 0) -> List[Dict]:\n        \"\"\"Transcrit un chunk audio avec gestion des r√©p√©titions\"\"\"\n        \n        # Sauvegarder temporairement le chunk\n        temp_file = f\"/tmp/chunk_{offset}.wav\"\n        sf.write(temp_file, audio_chunk, sr)\n        \n        try:\n            # Transcrire avec param√®tres optimis√©s\n            segments, info = self.model.transcribe(\n                temp_file,\n                language=\"fr\",\n                beam_size=self.config.beam_size,\n                best_of=self.config.best_of,\n                patience=self.config.patience,\n                temperature=self.config.temperature,\n                compression_ratio_threshold=self.config.compression_ratio_threshold,\n                log_prob_threshold=self.config.logprob_threshold,\n                no_speech_threshold=self.config.no_speech_threshold,\n                condition_on_previous_text=False,  # IMPORTANT: D√©sactiv√© pour √©viter propagation\n                initial_prompt=self.config.initial_prompt,\n                vad_filter=self.config.use_vad,\n                vad_parameters={\n                    \"threshold\": self.config.vad_threshold,\n                    \"min_speech_duration_ms\": self.config.vad_min_speech_duration_ms,\n                    \"min_silence_duration_ms\": self.config.vad_min_silence_duration_ms,\n                    \"speech_pad_ms\": self.config.vad_speech_pad_ms,\n                    \"max_speech_duration_s\": self.config.vad_max_speech_duration_s\n                },\n                word_timestamps=True,\n                suppress_blank=self.config.suppress_blank,\n                max_initial_timestamp=self.config.max_initial_timestamp\n            )\n            \n            # Convertir en liste et ajuster les timestamps\n            segment_list = []\n            for seg in segments:\n                segment_dict = {\n                    'id': len(segment_list),\n                    'start': float(seg.start + offset),\n                    'end': float(seg.end + offset),\n                    'text': seg.text,\n                    'confidence': float(getattr(seg, 'confidence', 0)),\n                    'no_speech_prob': float(seg.no_speech_prob) if hasattr(seg, 'no_speech_prob') else 0.0\n                }\n                \n                # Filtrer les segments de faible qualit√©\n                if segment_dict['confidence'] < -1.0 or segment_dict['no_speech_prob'] > 0.9:\n                    continue\n                \n                segment_list.append(segment_dict)\n            \n            return segment_list\n            \n        finally:\n            # Nettoyer le fichier temporaire\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n    \n    def transcribe_with_chunking(self, audio_path: str) -> Dict:\n        \"\"\"Transcription avec chunking intelligent pour √©viter les d√©rives\"\"\"\n        print(\"üéØ Transcription avec chunking intelligent...\")\n        \n        # Charger l'audio\n        y, sr = librosa.load(audio_path, sr=self.config.sample_rate)\n        duration = len(y) / sr\n        \n        # Calculer les chunks\n        chunk_samples = int(self.config.chunk_length_s * sr)\n        overlap_samples = int(self.config.chunk_overlap_s * sr)\n        \n        all_segments = []\n        \n        # Traiter par chunks\n        num_chunks = max(1, int(np.ceil((len(y) - overlap_samples) / (chunk_samples - overlap_samples))))\n        \n        for i in range(num_chunks):\n            start_sample = i * (chunk_samples - overlap_samples)\n            end_sample = min(start_sample + chunk_samples, len(y))\n            \n            chunk = y[start_sample:end_sample]\n            offset = start_sample / sr\n            \n            print(f\"  Chunk {i+1}/{num_chunks}: {format_timestamp(offset)} - {format_timestamp(end_sample/sr)}\")\n            \n            # Transcrire le chunk\n            chunk_segments = self.transcribe_chunk(chunk, sr, offset)\n            \n            # D√©tecter et nettoyer les r√©p√©titions\n            chunk_segments = self.clean_repetitions(chunk_segments)\n            \n            # Fusionner avec les segments pr√©c√©dents\n            if i > 0 and len(all_segments) > 0:\n                # G√©rer l'overlap - garder seulement les nouveaux segments apr√®s l'overlap\n                overlap_time = offset + self.config.chunk_overlap_s / 2\n                chunk_segments = [s for s in chunk_segments if s['start'] > overlap_time]\n            \n            all_segments.extend(chunk_segments)\n            \n            # V√©rification anti-d√©rive\n            if len(all_segments) > 10:\n                recent_texts = [s['text'] for s in all_segments[-10:]]\n                if len(set(recent_texts)) == 1:\n                    print(\"‚ö†Ô∏è R√©p√©tition d√©tect√©e - r√©initialisation du contexte\")\n                    # R√©initialiser pour le prochain chunk\n                    self.config.initial_prompt = \"Transcription suite. Nouveau contexte.\"\n        \n        # Nettoyer une derni√®re fois l'ensemble\n        all_segments = self.clean_repetitions(all_segments)\n        \n        # Construire la transcription finale\n        transcription = \" \".join([s['text'] for s in all_segments])\n        \n        return {\n            \"transcription\": transcription,\n            \"segments\": all_segments,\n            \"duration\": float(duration),\n            \"language\": \"fr\"\n        }\n    \n    def transcribe_with_preprocessing(self, audio_path: str, preprocess: bool = None, language: str = \"fr\") -> Dict:\n        \"\"\"Pipeline complet avec pr√©traitement optionnel\"\"\"\n        print(\"=\" * 60)\n        print(\"üéØ TRANSCRIPTION AVEC PR√âPROCESSING INTELLIGENT\")\n        print(\"=\" * 60)\n        \n        result = {\n            \"status\": \"processing\",\n            \"original_audio\": audio_path,\n            \"preprocessing_applied\": preprocess\n        }\n        \n        # √âtape 1: Analyse du bruit si auto\n        if preprocess == \"auto\" or preprocess is None:\n            noise_profile = self.preprocessor.analyze_noise_profile(audio_path)\n            preprocess = noise_profile[\"needs_denoising\"]\n            result[\"noise_profile\"] = noise_profile\n            \n            print(f\"  SNR: {noise_profile['snr']:.1f} dB\")\n            print(f\"  Silence: {noise_profile['silence_ratio']*100:.1f}%\")\n            print(f\"  Impulsions: {noise_profile['impulse_ratio']*1000:.2f}/1000 samples\")\n            \n            if preprocess:\n                print(\"  ‚Üí D√©bruitage recommand√©\")\n            else:\n                print(\"  ‚Üí Audio propre, pas de d√©bruitage n√©cessaire\")\n        \n        # √âtape 2: Pr√©processing si n√©cessaire\n        if preprocess:\n            print(\"\\nüîß Application du d√©bruitage...\")\n            audio_to_transcribe = self.preprocessor.apply_denoising(audio_path)\n        else:\n            audio_to_transcribe = audio_path\n        \n        # √âtape 3: Transcription avec chunking\n        print(f\"\\nüìù Transcription de l'audio {'nettoy√©' if preprocess else 'original'}...\")\n        \n        try:\n            # Charger le mod√®le\n            self.load_whisper_model()\n            \n            # Transcrire avec chunking intelligent\n            transcription_result = self.transcribe_with_chunking(audio_to_transcribe)\n            \n            # Calculer la confiance moyenne\n            if transcription_result[\"segments\"]:\n                avg_confidence = np.mean([s.get('confidence', 0) for s in transcription_result[\"segments\"]])\n            else:\n                avg_confidence = 0\n            \n            result.update({\n                \"status\": \"success\",\n                \"transcription\": transcription_result[\"transcription\"],\n                \"segments\": transcription_result[\"segments\"],\n                \"duration\": transcription_result[\"duration\"],\n                \"language\": transcription_result[\"language\"],\n                \"confidence\": avg_confidence\n            })\n            \n            print(f\"\\n‚úÖ Transcription r√©ussie!\")\n            print(f\"  üìä Confiance moyenne: {avg_confidence:.2%}\")\n            print(f\"  üìù Longueur: {len(result['transcription'])} caract√®res\")\n            print(f\"  ‚è±Ô∏è Dur√©e audio: {result['duration']:.1f}s\")\n            print(f\"  üìë Segments: {len(result['segments'])}\")\n            \n        except Exception as e:\n            print(f\"‚ùå Erreur transcription: {e}\")\n            result[\"status\"] = \"error\"\n            result[\"error\"] = str(e)\n        \n        finally:\n            # Nettoyer les fichiers temporaires\n            if preprocess and audio_to_transcribe != audio_path:\n                if os.path.exists(audio_to_transcribe):\n                    os.remove(audio_to_transcribe)\n        \n        return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.313981Z","iopub.execute_input":"2025-09-24T05:32:57.314237Z","iopub.status.idle":"2025-09-24T05:32:57.344500Z","shell.execute_reply.started":"2025-09-24T05:32:57.314193Z","shell.execute_reply":"2025-09-24T05:32:57.343850Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"***Comment r√©gler les param√®tres selon les cas***\n\nCas A ‚Äî Audio propre (dictaphones, salle calme)\n*  beam_size=3, best_of=1‚Äì2 (plus rapide)\n* no_speech_threshold=0.6 (ok)\n* temperature=0.0\n* VAD : min_silence_duration_ms=1500\n\nCas B ‚Äî Audio bruit√© (portes, brouhaha)\n* beam_size=5, best_of=5 (qualit√©)\n* baisser no_speech_threshold √† 0.5 si coupures\n* VAD : threshold=0.4‚Äì0.5, min_speech_duration_ms=200, min_silence_duration_ms=1800‚Äì2200\n* Garde-fous : garder compression_ratio_threshold=2.4\n\nCas C ‚Äî CPU-only (pas de GPU Kaggle)\n* compute_type=\"int8\", mod√®le tiny ou base\n* beam_size=3, best_of=1\n* Threads : cpu_threads=2, num_workers=1\n* Attends un RTF ‚âà 2‚Äì5 (selon longueur)","metadata":{}},{"cell_type":"code","source":"# Exemple d'utilisation\n#result = transcription_service.transcribe_audio(audio_file)\n#print(f\"Transcription: {result['transcription'][:500]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.345377Z","iopub.execute_input":"2025-09-24T05:32:57.345609Z","iopub.status.idle":"2025-09-24T05:32:57.362613Z","shell.execute_reply.started":"2025-09-24T05:32:57.345574Z","shell.execute_reply":"2025-09-24T05:32:57.362025Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# **Service d'analyse de qualit√©**","metadata":{}},{"cell_type":"code","source":"class QualityAnalyzer:\n    \"\"\"Analyse la qualit√© de la transcription et d√©tecte les probl√®mes\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n    \n    def analyze_transcription(self, result: Dict) -> Dict:\n        \"\"\"Analyse compl√®te de la qualit√©\"\"\"\n        \n        if result[\"status\"] != \"success\":\n            return {\"status\": \"error\", \"message\": \"Transcription √©chou√©e\"}\n        \n        text = result[\"transcription\"]\n        segments = result[\"segments\"]\n        \n        analysis = {\n            \"total_length\": len(text),\n            \"total_segments\": len(segments),\n            \"repetitions\": {},\n            \"quality_issues\": [],\n            \"statistics\": {}\n        }\n        \n        # D√©tecter les r√©p√©titions\n        words = text.lower().split()\n        word_freq = {}\n        for word in words:\n            if len(word) > 3:  # Ignorer les mots courts\n                word_freq[word] = word_freq.get(word, 0) + 1\n        \n        # Identifier les mots trop fr√©quents\n        total_words = len(words)\n        for word, count in word_freq.items():\n            ratio = count / total_words\n            if ratio > 0.05:  # Plus de 5% du texte\n                analysis[\"repetitions\"][word] = {\n                    \"count\": count,\n                    \"ratio\": ratio\n                }\n                if ratio > 0.1:\n                    analysis[\"quality_issues\"].append(f\"Mot '{word}' r√©p√©t√© {count} fois ({ratio:.1%})\")\n        \n        # Analyser les segments\n        low_confidence = sum(1 for s in segments if s.get('confidence', 0) < -0.5)\n        high_no_speech = sum(1 for s in segments if s.get('no_speech_prob', 0) > 0.6)\n        \n        avg_conf = float(np.mean([s.get('confidence', 0) for s in segments])) if segments else 0.0\n        \n        analysis[\"statistics\"] = {\n            \"avg_confidence\": avg_conf,\n            \"low_confidence_segments\": low_confidence,\n            \"high_no_speech_segments\": high_no_speech,\n            \"words_per_segment\": float(total_words / len(segments)) if segments else 0.0\n        }\n        \n        # Identifier les probl√®mes\n        if low_confidence > len(segments) * 0.3:\n            analysis[\"quality_issues\"].append(f\"{low_confidence} segments avec faible confiance\")\n        \n        if high_no_speech > len(segments) * 0.2:\n            analysis[\"quality_issues\"].append(f\"{high_no_speech} segments d√©tect√©s comme silence\")\n        \n        # Score de qualit√© global\n        quality_score = 100\n        quality_score -= len(analysis[\"repetitions\"]) * 5\n        quality_score -= len(analysis[\"quality_issues\"]) * 10\n        quality_score = max(0, quality_score)\n        \n        analysis[\"quality_score\"] = quality_score\n        \n        return analysis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.363340Z","iopub.execute_input":"2025-09-24T05:32:57.363993Z","iopub.status.idle":"2025-09-24T05:32:57.378163Z","shell.execute_reply.started":"2025-09-24T05:32:57.363976Z","shell.execute_reply":"2025-09-24T05:32:57.377551Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# **Fallback AssemblyAI (si √©chec Whisper)**","metadata":{}},{"cell_type":"code","source":"class AssemblyAIFallback:\n    \"\"\"Service de fallback avec AssemblyAI\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        \n    def transcribe_with_assemblyai(self, audio_path: str) -> Dict:\n        \"\"\"\n        Transcription de secours via AssemblyAI\n        \n        Args:\n            audio_path: Chemin du fichier audio\n            \n        Returns:\n            Dict avec la transcription\n        \"\"\"\n        if not self.api_key:\n            return {\n                \"status\": \"error\",\n                \"error\": \"Cl√© API AssemblyAI non configur√©e\"\n            }\n        \n        try:\n            import assemblyai as aai\n            \n            print(\"üîÑ Utilisation du fallback AssemblyAI...\")\n            \n            aai.settings.api_key = self.api_key\n            transcriber = aai.Transcriber()\n            \n            # Upload et transcription\n            config_lang = aai.TranscriptionConfig(\n                language_code=\"fr\",\n                punctuate=True,\n                format_text=True,\n                disfluencies=True,\n                speaker_labels=True\n            )\n            transcript = transcriber.transcribe(audio_path, config=config_lang)\n            \n            if transcript.status == aai.TranscriptStatus.error:\n                raise Exception(f\"Erreur AssemblyAI: {transcript.error}\")\n            \n            # Attente de la transcription\n            while transcript.status not in [aai.TranscriptStatus.completed, aai.TranscriptStatus.error]:\n                time.sleep(5)\n                transcript = transcriber.get_transcript(transcript.id)\n            \n            return {\n                \"status\": \"success\",\n                \"method\": \"assemblyai\",\n                \"transcription\": transcript.text,\n                \"confidence\": transcript.confidence if hasattr(transcript, 'confidence') else 0.85,\n                \"words\": transcript.words if hasattr(transcript, 'words') else []\n            }\n            \n        except Exception as e:\n            print(f\"‚ùå Erreur AssemblyAI: {str(e)}\")\n            return {\n                \"status\": \"error\",\n                \"error\": str(e),\n                \"method\": \"assemblyai\"\n            }\n\n# Service de fallback\nfallback_service = AssemblyAIFallback(config.assemblyai_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.378900Z","iopub.execute_input":"2025-09-24T05:32:57.379108Z","iopub.status.idle":"2025-09-24T05:32:57.395498Z","shell.execute_reply.started":"2025-09-24T05:32:57.379093Z","shell.execute_reply":"2025-09-24T05:32:57.394912Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"1. Par d√©faut, la langue est auto. Pour ton cas, force fran√ßais :\n        config = aai.TranscriptionConfig(language_code=\"fr\")\n2. Diarisation (orateurs)\n        config = aai.TranscriptionConfig(speaker_labels=True)\n\nExemple :\n    config = aai.TranscriptionConfig(language_code=\"fr\", speaker_labels=True)\n    transcript = transcriber.transcribe(audio_path, config=config)\n\nAppel :\n    Si TranscriptionService.transcribe_audio renvoie status=\"error\" ou un real_time_factor >> 5 (trop lent) ou trop de segments sous ton confidence_threshold, alors :\n        > result = fallback_service.transcribe_with_assemblyai(audio_path)","metadata":{}},{"cell_type":"markdown","source":"**Pipeline de transcription avec gestion automatique du fallback**","metadata":{}},{"cell_type":"code","source":"def transcribe_audio_pipeline(\n    audio_path: str, \n    config: Config,\n    force_denoise: Optional[bool] = None,\n    analyze_quality: bool = True\n) -> Dict:\n    \"\"\"\n    Pipeline complet de transcription avec analyse de qualit√©\n    \n    Args:\n        audio_path: Chemin du fichier audio\n        config: Configuration\n        force_denoise: Forcer le d√©bruitage (None=auto)\n        analyze_quality: Analyser la qualit√© apr√®s transcription\n    \"\"\"\n    \n    print(\"=\" * 70)\n    print(\"üéØ PIPELINE DE TRANSCRIPTION INTELLIGENT V2\")\n    print(\"=\" * 70)\n    \n    # Pr√©parer le fichier\n    file_info = prepare_audio_file(audio_path)\n    print(f\"üìÅ Fichier: {os.path.basename(audio_path)}\")\n    print(f\"   Format: {file_info['format']}\")\n    print(f\"   Dur√©e: {format_timestamp(file_info['duration_seconds'])}\")\n    print(f\"   Taille: {file_info['size_mb']:.1f} MB\")\n    \n    # Service de transcription\n    transcription_service = TranscriptionService(config)\n    \n    # D√©terminer si d√©bruitage n√©cessaire\n    if force_denoise is None:\n        force_denoise = \"auto\"\n    \n    # Transcription\n    result = transcription_service.transcribe_with_preprocessing(\n        audio_path,\n        preprocess=force_denoise,\n        language=\"fr\"\n    )\n    \n    # Analyse de qualit√©\n    if analyze_quality and result[\"status\"] == \"success\":\n        print(\"\\nüìä Analyse de la qualit√©...\")\n        analyzer = QualityAnalyzer(config)\n        quality = analyzer.analyze_transcription(result)\n        result[\"quality_analysis\"] = quality\n        \n        print(f\"   Score de qualit√©: {quality['quality_score']}/100\")\n        \n        if quality[\"quality_issues\"]:\n            print(\"   ‚ö†Ô∏è Probl√®mes d√©tect√©s:\")\n            for issue in quality[\"quality_issues\"]:\n                print(f\"      - {issue}\")\n        \n        if quality[\"repetitions\"]:\n            print(\"   üîÑ R√©p√©titions excessives:\")\n            for word, data in list(quality[\"repetitions\"].items())[:3]:\n                print(f\"      - '{word}': {data['count']} fois ({data['ratio']:.1%})\")\n    \n    # Sauvegarder le r√©sultat\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_file = f\"{config.output_dir}/transcription_{timestamp}.json\"\n    \n    # Convertir les types NumPy en types Python natifs pour JSON\n    def convert_numpy_types(obj):\n        \"\"\"Convertit r√©cursivement les types NumPy en types Python natifs\"\"\"\n        import numpy as np\n        \n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, (np.bool_, bool)):\n            return bool(obj)\n        elif isinstance(obj, dict):\n            return {key: convert_numpy_types(value) for key, value in obj.items()}\n        elif isinstance(obj, list):\n            return [convert_numpy_types(item) for item in obj]\n        else:\n            return obj\n    \n    # Nettoyer le r√©sultat avant sauvegarde\n    result_clean = convert_numpy_types(result)\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(result_clean, f, ensure_ascii=False, indent=2)\n    \n    print(f\"\\nüíæ R√©sultat sauvegard√©: {output_file}\")\n    \n    # R√©sum√© final\n    if result[\"status\"] == \"success\":\n        print(\"\\n\" + \"=\" * 70)\n        print(\"‚úÖ TRANSCRIPTION R√âUSSIE\")\n        print(\"=\" * 70)\n        print(f\"üìù M√©thode: Whisper {config.whisper_model}\")\n        print(f\"üìä Confiance: {result.get('confidence', 0):.2%}\")\n        print(f\"üìë Segments: {len(result.get('segments', []))}\")\n        print(f\"üìÑ Longueur: {len(result.get('transcription', ''))} caract√®res\")\n        \n        if analyze_quality:\n            print(f\"‚≠ê Qualit√©: {result['quality_analysis']['quality_score']}/100\")\n        \n        # Aper√ßu\n        text = result.get('transcription', '')\n        if text:\n            print(f\"\\nüìñ Aper√ßu (300 premiers caract√®res):\")\n            print(f\"   {text[:300]}...\")\n    else:\n        print(f\"\\n‚ùå √âchec transcription: {result.get('error')}\")\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.397883Z","iopub.execute_input":"2025-09-24T05:32:57.398079Z","iopub.status.idle":"2025-09-24T05:32:57.416934Z","shell.execute_reply.started":"2025-09-24T05:32:57.398064Z","shell.execute_reply":"2025-09-24T05:32:57.416225Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Test avec votre fichier audio\n#audio_file = f\"{UPLOAD_PATH}atelier.mp3\"\n#audio_file = f\"{UPLOAD_PATH}test_1h.wav\"\naudio_file = f\"{UPLOAD_PATH}test_30mn.mp3\"\n#audio_info = prepare_audio_file(audio_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.417532Z","iopub.execute_input":"2025-09-24T05:32:57.417829Z","iopub.status.idle":"2025-09-24T05:32:57.434576Z","shell.execute_reply.started":"2025-09-24T05:32:57.417805Z","shell.execute_reply":"2025-09-24T05:32:57.433956Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# transcription_result = transcribe_audio_pipeline(\n#             audio_file, \n#             config,\n#             force_denoise=None  # Auto-d√©tection\n#         )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.435176Z","iopub.execute_input":"2025-09-24T05:32:57.435354Z","iopub.status.idle":"2025-09-24T05:32:57.449214Z","shell.execute_reply.started":"2025-09-24T05:32:57.435340Z","shell.execute_reply":"2025-09-24T05:32:57.448678Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# V√©rifier l'existence du fichier\nif os.path.exists(audio_file):\n    print(f\"‚úÖ Fichier trouv√©: {audio_file}\")\n    \n    # Lancer la transcription avec le pipeline am√©lior√©\n    transcription_result = transcribe_audio_pipeline(\n        audio_file, \n        config,\n        force_denoise=None,  # Auto-d√©tection\n        analyze_quality=True  # Analyse de qualit√© activ√©e\n    )\n    \n    # Afficher les statistiques finales\n    if transcription_result[\"status\"] == \"success\":\n        print(\"\\nüìà STATISTIQUES FINALES:\")\n        print(\"-\" * 40)\n        \n        if \"quality_analysis\" in transcription_result:\n            qa = transcription_result[\"quality_analysis\"]\n            print(f\"Score qualit√©: {qa['quality_score']}/100\")\n            print(f\"R√©p√©titions d√©tect√©es: {len(qa['repetitions'])}\")\n            print(f\"Probl√®mes identifi√©s: {len(qa['quality_issues'])}\")\n            \n            if qa['statistics']:\n                print(f\"Confiance moyenne: {qa['statistics']['avg_confidence']:.3f}\")\n                print(f\"Mots/segment: {qa['statistics']['words_per_segment']:.1f}\")\nelse:\n    print(f\"‚ùå Fichier non trouv√©: {audio_file}\")\n    print(\"Veuillez ajuster le chemin du fichier audio dans la cellule ci-dessus.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T05:32:57.449957Z","iopub.execute_input":"2025-09-24T05:32:57.450215Z","iopub.status.idle":"2025-09-24T05:35:50.703142Z","shell.execute_reply.started":"2025-09-24T05:32:57.450193Z","shell.execute_reply":"2025-09-24T05:35:50.702471Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Fichier trouv√©: /kaggle/input/meeting-audio/test_30mn.mp3\n======================================================================\nüéØ PIPELINE DE TRANSCRIPTION INTELLIGENT V2\n======================================================================\nüìÅ Fichier: test_30mn.mp3\n   Format: mp3\n   Dur√©e: 00:32:08\n   Taille: 73.5 MB\n============================================================\nüéØ TRANSCRIPTION AVEC PR√âPROCESSING INTELLIGENT\n============================================================\nüîç Analyse du niveau de bruit...\n  SNR: 41.1 dB\n  Silence: 20.0%\n  Impulsions: 0.00/1000 samples\n  ‚Üí Audio propre, pas de d√©bruitage n√©cessaire\n\nüìù Transcription de l'audio original...\n‚è≥ Chargement Whisper large-v3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6b862a05fe4d5da368e2385ac6216a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e6397a379d42e09c5b918ce194237c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocabulary.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f49e49cbe1564f20802352e1bdbf9ced"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59452f84111f4b7b95a5f9a4ce8b5a6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"587c3814bccd4af6ad7d31d3f57a49c4"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Mod√®le charg√© sur CUDA\nüéØ Transcription avec chunking intelligent...\n  Chunk 1/8: 00:00:00 - 00:05:00\n  Chunk 2/8: 00:04:30 - 00:09:30\n  Chunk 3/8: 00:09:00 - 00:14:00\n  Chunk 4/8: 00:13:30 - 00:18:30\n  Chunk 5/8: 00:18:00 - 00:23:00\n  Chunk 6/8: 00:22:30 - 00:27:30\n  Chunk 7/8: 00:27:00 - 00:32:00\n  Chunk 8/8: 00:31:30 - 00:32:08\n\n‚úÖ Transcription r√©ussie!\n  üìä Confiance moyenne: 0.00%\n  üìù Longueur: 15870 caract√®res\n  ‚è±Ô∏è Dur√©e audio: 1928.0s\n  üìë Segments: 232\n\nüìä Analyse de la qualit√©...\n   Score de qualit√©: 90/100\n   ‚ö†Ô∏è Probl√®mes d√©tect√©s:\n      - 71 segments d√©tect√©s comme silence\n\nüíæ R√©sultat sauvegard√©: /kaggle/working/transcription_20250924_053550.json\n\n======================================================================\n‚úÖ TRANSCRIPTION R√âUSSIE\n======================================================================\nüìù M√©thode: Whisper large-v3\nüìä Confiance: 0.00%\nüìë Segments: 232\nüìÑ Longueur: 15870 caract√®res\n‚≠ê Qualit√©: 90/100\n\nüìñ Aper√ßu (300 premiers caract√®res):\n    R√©alisation des documents n√©cessaires √† ce Conseil  ...  Merci.  Et la hausse du taux de base bancaire √† 12%, ceci nous donne un peu plus de r√©sultats √† fin juin 2025 et aussi √† l'estimation au 31 d√©cembre 2025.  Donc dans le tableau qui est pr√©sent√©, il y a dans le budget 2025, les co√ªts d'exploit...\n\nüìà STATISTIQUES FINALES:\n----------------------------------------\nScore qualit√©: 90/100\nR√©p√©titions d√©tect√©es: 0\nProbl√®mes identifi√©s: 1\nConfiance moyenne: 0.000\nMots/segment: 11.7\n","output_type":"stream"}],"execution_count":17}]}